{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0711-160232-dk376feg/driver-1145365187227182152\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0711-160232-dk376feg/driver-1145365187227182152\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Optimizing Skewness and Spillage\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.executor.cores\",4)\n",
    "        .config(\"spark.cores.max\",8)\n",
    "        .config(\"spark.executor.memory\", \"512M\")\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "        .config(\"spark.sql.adaptive.enabled\", False)\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2369cc9d-47cd-4918-8218-74adea67c5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable Adaptive Query Engine(AQE) and Broadcast Join\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "# spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "# spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928959d6-83ae-4c4b-b7e8-312d17b4758d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3ecdac-5721-47a0-b53a-d85f6d1232d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10 million records\n",
    "emp_schema = \"first_name string, last_name string, job_title string, dob date, email string, phone string, salary double, department string, department_id integer\"\n",
    "emp = spark.read.schema(emp_schema).option(\"header\",True).csv(\"/data/input/datasets/employee_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf48e4b6-3ead-4c68-b982-8a6bf6d13b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV file with 10 records\n",
    "dept_schema =\"department_id int, department_name string, description string, city string, state string, country string \"\n",
    "dept = spark.read.schema(dept_schema).option(\"header\",True).csv(\"/data/input/datasets/department_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d7a848-1093-445a-96d9-3a229e59e7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JOINING datasets\n",
    "dfjoined = emp.join(dept, on='department_id', how=\"left_outer\")\n",
    "dfjoined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7dd99e6-fc2d-4069-b896-d7291761aee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(2) Project [department_id#72, first_name#64, last_name#65, job_title#66, dob#67, email#68, phone#69, salary#70, department#71, department_name#83, description#84, city#85, state#86, country#87]\n+- *(2) SortMergeJoin [department_id#72], [department_id#82], LeftOuter\n   :- Sort [department_id#72 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(department_id#72, 200), ENSURE_REQUIREMENTS, [plan_id=98]\n   :     +- FileScan csv [first_name#64,last_name#65,job_title#66,dob#67,email#68,phone#69,salary#70,department#71,department_id#72] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n   +- Sort [department_id#82 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(department_id#82, 200), ENSURE_REQUIREMENTS, [plan_id=136]\n         +- *(1) Filter isnotnull(department_id#82)\n            +- FileScan csv [department_id#82,department_name#83,description#84,city#85,state#86,country#87] Batched: false, DataFilters: [isnotnull(department_id#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n\n\n"
     ]
    }
   ],
   "source": [
    "dfjoined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8703da4b-29ef-4479-95e5-a36c7f3f9e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In Spark UI, if you check '*Shuffle read size/records*' for 200 tasks being called for Stage3(check cell 6 output), onle few of them has processed the data. Rest all have done nothing.\n",
    "- So, the default shuffle partitions 200 is a waste for us. This is called skewness and might cause spillage in Memory and Disk in extreme cases and thus decreases performance.\n",
    "- Let's verify it below. If you see it shows 10 partitions but our Shuffle Partitions are 200 i.e 190 partitions has no data (Check '*Shuffle read size/records*' in Spark UI) \n",
    "- This is why we need to configure our '*Shuffle Partitions*' properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb575633-661a-4a8b-aa61-cac6cace7d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n|partitionnum|  count|\n+------------+-------+\n|         103|1000525|\n|         122| 999664|\n|          43| 999585|\n|         107| 999690|\n|          49| 998824|\n|          51|1000469|\n|         102|1000749|\n|          66|1001702|\n|         174| 999380|\n|          89| 999412|\n+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "dfpartitioned = dfjoined.withColumn(\"partitionnum\", spark_partition_id()).groupBy(\"partitionnum\").agg(count(lit(1)).alias(\"count\"))\n",
    "dfpartitioned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a0165c-c4d3-4299-9cac-395f5fa3b6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n|department_id|count(1)|\n+-------------+--------+\n|            1|  999585|\n|            6|  998824|\n|            3| 1000469|\n|            5| 1001702|\n|            9|  999412|\n|            4| 1000749|\n|            8| 1000525|\n|            7|  999690|\n|           10|  999664|\n|            2|  999380|\n+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verify Employee data based on department_id\n",
    "\n",
    "emp.groupBy(\"department_id\").agg(count(lit(1))).show()\n",
    "# We can see some minimal skewing because some department_id have data count slightly higher as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d542ffe-8a2f-40f8-98b7-3dc1a162f808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> Even though the skewness here is minimal, in extreme cases how do we fix skewness?\n",
    "- Since the 'joining column' itself is on department here, even if we repartition the data, the skewed departments will again come back to the same numbers and we will again have skewness for the data.\n",
    "- Here comes '*Salting*' to the rescue. It will have data evenly distributed across the partitions and thus all of the task will be procesing data evenly without any Spillage. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d14e079-96e9-4806-b630-ceece0b1c331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SALTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fd3599-dfab-49a2-a377-692023d6958a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set Shuffle Partitions to a lesser number - 16\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7669258d-5ab8-4907-85e9-649582b11926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n| 15|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to create a random number query every time and add to employee as Salt \n",
    "@udf\n",
    "def saltudf():\n",
    "    return random.randint(0,16)\n",
    "    \n",
    "# Salt dataframe to add to department\n",
    "saltdf = spark.range(0,16)\n",
    "saltdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d93cc964-b5cd-4a9e-b02d-dbfde5ffa55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+-----------------------+------------+---------+---------------+-------------+-------------+\n|first_name   |last_name   |job_title     |dob       |email                  |phone       |salary   |department     |department_id|salted_deptId|\n+-------------+------------+--------------+----------+-----------------------+------------+---------+---------------+-------------+-------------+\n|FirstName_280|LastName_280|Data Scientist|1982-02-27|user1249280@example.com|+13950857290|105255.84|Engineering    |3            |3_8          |\n|FirstName_281|LastName_281|Team Lead     |1996-06-07|user1249281@example.com|+15818087627|39471.77 |Marketing      |5            |5_16         |\n|FirstName_282|LastName_282|Data Analyst  |1996-09-02|user1249282@example.com|+18214812697|71801.39 |Data           |1            |1_5          |\n|FirstName_283|LastName_283|HR Coordinator|1968-03-25|user1249283@example.com|+13544214736|110637.46|Support        |6            |6_7          |\n|FirstName_284|LastName_284|HR Specialist |2000-12-13|user1249284@example.com|+16692419967|95749.78 |Engineering    |3            |3_10         |\n|FirstName_285|LastName_285|Team Lead     |1995-04-18|user1249285@example.com|+16847554796|122423.56|Finance        |4            |4_2          |\n|FirstName_286|LastName_286|HR Manager    |1968-06-06|user1249286@example.com|+14539442929|71813.24 |Engineering    |3            |3_10         |\n|FirstName_287|LastName_287|Data Scientist|1984-10-05|user1249287@example.com|+12528412425|115576.22|Operations     |9            |9_2          |\n|FirstName_288|LastName_288|Senior Manager|1985-05-26|user1249288@example.com|+14644924775|86367.71 |Data           |1            |1_7          |\n|FirstName_289|LastName_289|Team Lead     |2001-11-30|user1249289@example.com|+12589639198|120265.77|IT             |8            |8_1          |\n|FirstName_290|LastName_290|Data Engineer |1985-09-02|user1249290@example.com|+11245424565|85137.01 |Engineering    |3            |3_12         |\n|FirstName_291|LastName_291|Senior Manager|1963-09-25|user1249291@example.com|+19361570770|108959.51|Operations     |9            |9_15         |\n|FirstName_292|LastName_292|Manager       |1982-05-18|user1249292@example.com|+14458466302|110572.86|Engineering    |3            |3_9          |\n|FirstName_293|LastName_293|Manager       |1982-07-27|user1249293@example.com|+19295455657|122691.7 |Marketing      |5            |5_7          |\n|FirstName_294|LastName_294|Data Scientist|2003-12-27|user1249294@example.com|+11234720206|33682.37 |Legal          |7            |7_11         |\n|FirstName_295|LastName_295|HR Coordinator|1985-09-07|user1249295@example.com|+18578519040|81337.98 |Human Resources|2            |2_15         |\n|FirstName_296|LastName_296|Data Analyst  |1968-08-08|user1249296@example.com|+17978937881|52488.6  |Marketing      |5            |5_16         |\n|FirstName_297|LastName_297|Data Analyst  |1976-10-05|user1249297@example.com|+14608691318|44709.29 |Legal          |7            |7_7          |\n|FirstName_298|LastName_298|Data Analyst  |1969-09-05|user1249298@example.com|+12317186886|118018.96|Human Resources|2            |2_5          |\n|FirstName_299|LastName_299|HR Manager    |1999-12-11|user1249299@example.com|+11103526998|110664.91|IT             |8            |8_16         |\n+-------------+------------+--------------+----------+-----------------------+------------+---------+---------------+-------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# SALTED Employee\n",
    "\n",
    "from pyspark.sql.functions import lit, concat\n",
    "saltedemp = emp.withColumn(\"salted_deptId\", concat(\"department_id\",lit(\"_\"), saltudf()))\n",
    "saltedemp.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8df71ab-1cb4-4296-8868-1f8570312922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------------------+-------------+-----+-------+---+-------------+\n|department_id|department_name|         description|         city|state|country| id|salted_deptId|\n+-------------+---------------+--------------------+-------------+-----+-------+---+-------------+\n|            1|           Data|     Data Department|     New York|   NY|    USA|  0|          1_0|\n|            1|           Data|     Data Department|     New York|   NY|    USA|  1|          1_1|\n|            2|Human Resources|       HR Department|      Chicago|   IL|    USA|  0|          2_0|\n|            2|Human Resources|       HR Department|      Chicago|   IL|    USA|  1|          2_1|\n|            3|    Engineering|Engineering Depar...|San Francisco|   CA|    USA|  0|          3_0|\n|            3|    Engineering|Engineering Depar...|San Francisco|   CA|    USA|  1|          3_1|\n|            4|        Finance|  Finance Department|       Boston|   MA|    USA|  0|          4_0|\n|            4|        Finance|  Finance Department|       Boston|   MA|    USA|  1|          4_1|\n|            5|      Marketing|Marketing Department|  Los Angeles|   CA|    USA|  0|          5_0|\n|            5|      Marketing|Marketing Department|  Los Angeles|   CA|    USA|  1|          5_1|\n|            6|        Support|  Support Department|       Austin|   TX|    USA|  0|          6_0|\n|            6|        Support|  Support Department|       Austin|   TX|    USA|  1|          6_1|\n|            7|          Legal|    Legal Department|      Seattle|   WA|    USA|  0|          7_0|\n|            7|          Legal|    Legal Department|      Seattle|   WA|    USA|  1|          7_1|\n|            8|             IT|       IT Department|       Denver|   CO|    USA|  0|          8_0|\n|            8|             IT|       IT Department|       Denver|   CO|    USA|  1|          8_1|\n|            9|     Operations|Operations Depart...|        Miami|   FL|    USA|  0|          9_0|\n|            9|     Operations|Operations Depart...|        Miami|   FL|    USA|  1|          9_1|\n|           10|          Admin|Administration De...|      Houston|   TX|    USA|  0|         10_0|\n|           10|          Admin|Administration De...|      Houston|   TX|    USA|  1|         10_1|\n+-------------+---------------+--------------------+-------------+-----+-------+---+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "salteddept = dept.join(saltdf, how=\"cross\").withColumn(\"salted_deptId\", concat(\"department_id\", lit(\"_\"), \"id\"))\n",
    "salteddept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3eff1b9-654b-41ac-b14d-7bedf145bb5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------------------+-----+-----+-------+---+-------------+\n|department_id|department_name|         description| city|state|country| id|salted_deptId|\n+-------------+---------------+--------------------+-----+-----+-------+---+-------------+\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  0|          9_0|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  1|          9_1|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  2|          9_2|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  3|          9_3|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  4|          9_4|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  5|          9_5|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  6|          9_6|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  7|          9_7|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  8|          9_8|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA|  9|          9_9|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA| 10|         9_10|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA| 11|         9_11|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA| 12|         9_12|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA| 13|         9_13|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA| 14|         9_14|\n|            9|     Operations|Operations Depart...|Miami|   FL|    USA| 15|         9_15|\n+-------------+---------------+--------------------+-----+-----+-------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "salteddept.where(\"department_id=9\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18ef6637-7076-401d-9d06-df4fdd5d2a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "if you see for all department_id's till 15(0-15 i.e. 16 rows) we have added SAlTING above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b139ad78-21b6-4fac-b032-8b36b5901475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's make the SALTED join now\n",
    "saltedjoineddf = saltedemp.join(salteddept, on='salted_deptId', how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fef146-fe23-4d8d-8f7a-36de1cbfaf6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n|partitionnum| count|\n+------------+------+\n|          12|354557|\n|           5|705736|\n|          10|588190|\n|           1|352451|\n|           3|588690|\n|           2|824773|\n|          13|706439|\n|          14|471390|\n|           6|645863|\n|           9|824469|\n|           7|881842|\n|          11|939757|\n|          15|470043|\n|           4|469958|\n|           0|587533|\n|           8|588309|\n+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "dfpartitioned = saltedjoineddf.withColumn(\"partitionnum\", spark_partition_id()).groupBy(\"partitionnum\").agg(count(lit(1)).alias(\"count\"))\n",
    "dfpartitioned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07f8dd9-c0e8-40c1-90c1-9c9852a7cc39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Now, you can see above that the data has been distributed evenly across all the 16 partitions and there is no skewness.\n",
    "- It Took only 16 tasks (16 Shuffle Partitions we configured in cell 13) to process the data as opposed to 200 when we didnt use Salting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "996e6001-19de-4421-8353-7892fcda6059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> SUMMARY\n",
    "- In '*Summary Metrics*' tab SPARK UI Under '*Stages*' you will see the 'Duration' to process the job will be significantly reduced after using 'SALTING' technique."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20_skewness_and_spillage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
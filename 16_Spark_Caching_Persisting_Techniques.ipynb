{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0706-083214-7l388b8c/driver-535718982730242282\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0706-083214-7l388b8c/driver-535718982730242282\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Cache and Persist\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.executor.memory\", '512M')\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3ecdac-5721-47a0-b53a-d85f6d1232d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10 million records\n",
    "_schema = \"first_name string, last_name string, job_title string, dob date, email string, phone string, salary double, department string, department_id integer\"\n",
    "emp = spark.read.schema(_schema).option(\"header\",True).csv(\"/data/input/datasets/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9856670c-bb5b-40ca-9374-a3b1dc0c0a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\n|first_name|last_name|job_title        |dob       |email                         |phone          |salary            |department        |department_id|\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\n|Jennifer  |Williams |HR Specialist    |1951-01-21|Jennifer.Williams.@example.com|+1-845-311-804 |42951.90537045701 |Finance           |6            |\n|James     |Miller   |Sales Executive  |1939-09-25|James.Miller.@example.com     |+1-274-633-7306|50933.8591162336  |Data and Analytics|6            |\n|Linda     |Jones    |Data Scientist   |2023-05-26|Linda.Jones.@example.com      |+1-149-733-8924|66274.49226944339 |Data and Analytics|2            |\n|Srishti   |Smith    |Data Engineer    |2003-01-16|Srishti.Smith.@example.com    |+1-790-373-5222|43705.485219830625|Engineering       |4            |\n|Michael   |Brown    |Software Engineer|1973-08-28|Michael.Brown.@example.com    |+1-223-271-7921|97427.30108330262 |Data and Analytics|6            |\n|James     |Smith    |Product Manager  |1963-03-06|James.Smith.@example.com      |+1-578-398-5236|88657.24275976823 |Marketing         |4            |\n|Patricia  |Shetty   |Sales Executive  |1958-04-14|Patricia.Shetty.@example.com  |+1-130-463-1358|103219.03006790127|Engineering       |1            |\n|John      |Miller   |Software Engineer|1939-10-15|John.Miller.@example.com      |+1-470-839-1827|90518.82086271374 |Data and Analytics|6            |\n|John      |Davis    |Product Manager  |1991-01-22|John.Davis.@example.com       |+1-669-876-3759|90798.55591949391 |Data and Analytics|1            |\n|Robert    |Miller   |HR Specialist    |1966-03-29|Robert.Miller.@example.com    |+1-711-544-4088|97114.44013586667 |Marketing         |5            |\n|Srishti   |Davis    |Data Engineer    |1942-06-29|Srishti.Davis.@example.com    |+1-775-524-8908|57286.64649231718 |Data and Analytics|2            |\n|Robert    |Davis    |Data Engineer    |1967-04-03|Robert.Davis.@example.com     |+1-145-540-4540|81544.94970686873 |HR                |2            |\n|James     |Jones    |Data Engineer    |2012-08-11|James.Jones.@example.com      |+1-939-212-1221|77779.27197392733 |Engineering       |4            |\n|Srishti   |Jones    |HR Specialist    |2023-06-24|Srishti.Jones.@example.com    |+1-178-289-4668|72126.29782351096 |Marketing         |5            |\n|Mary      |Brown    |Software Engineer|2016-08-08|Mary.Brown.@example.com       |+1-488-169-8580|63513.55607727563 |Data and Analytics|5            |\n|Linda     |Jones    |Data Scientist   |1982-06-06|Linda.Jones.@example.com      |+1-424-265-1312|58757.91283169232 |HR                |4            |\n|Michael   |García   |Software Engineer|1993-12-31|Michael.García.@example.com   |+1-574-133-1864|96062.72254554348 |Finance           |6            |\n|Jennifer  |Jones    |HR Specialist    |1974-02-26|Jennifer.Jones.@example.com   |+1-582-87-4216 |92769.49068465423 |Marketing         |1            |\n|John      |Jones    |HR Specialist    |2001-10-30|John.Jones.@example.com       |+1-208-41-8842 |94323.25702758957 |Finance           |4            |\n|Srishti   |Davis    |Product Manager  |1978-09-15|Srishti.Davis.@example.com    |+1-425-792-4196|63939.698918206384|Data and Analytics|2            |\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "emp.where(\"salary>1000\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be984029-8751-47dc-bea9-dcd88d9f37b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[first_name: string, last_name: string, job_title: string, dob: date, email: string, phone: string, salary: double, department: string, department_id: int]"
     ]
    }
   ],
   "source": [
    "# CACHE dataframe (cache or persist)\n",
    "emp.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f350dbab-bd15-4212-a511-ea27e4c8a233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- For Cache, you have to trigger an action for it to actually do something.\n",
    "- That ACTION can be COUNT or WRITE.\n",
    "- COunt or Write will read the whole dataframe rather than reading a partial dataframe.\n",
    "- Let's see it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44febca-336d-4e39-b00d-6d5f79f1e066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 10000000"
     ]
    }
   ],
   "source": [
    "emp.cache().count() # MEMORY AND DISK # This will get you the count (10 million records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2105855f-520b-46fc-9313-f69440c37141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The above action will create CACHE in the '*Storage*' tab of '*Spark UI*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28d0224-e581-49e3-9153-c2bbc7c4d33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- *'MEMORY_AND_DISK'* is the default storage level, if you run CACHE for dataframe and dataset in PySpark.\n",
    "- But it is *'MEMORY_ONLY'*  for RDD's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee9e96e-f887-46c6-984e-5a06b6062b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\n|first_name|last_name|job_title        |dob       |email                         |phone          |salary            |department        |department_id|\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\n|Jennifer  |Williams |HR Specialist    |1951-01-21|Jennifer.Williams.@example.com|+1-845-311-804 |42951.90537045701 |Finance           |6            |\n|James     |Miller   |Sales Executive  |1939-09-25|James.Miller.@example.com     |+1-274-633-7306|50933.8591162336  |Data and Analytics|6            |\n|Linda     |Jones    |Data Scientist   |2023-05-26|Linda.Jones.@example.com      |+1-149-733-8924|66274.49226944339 |Data and Analytics|2            |\n|Srishti   |Smith    |Data Engineer    |2003-01-16|Srishti.Smith.@example.com    |+1-790-373-5222|43705.485219830625|Engineering       |4            |\n|Michael   |Brown    |Software Engineer|1973-08-28|Michael.Brown.@example.com    |+1-223-271-7921|97427.30108330262 |Data and Analytics|6            |\n|James     |Smith    |Product Manager  |1963-03-06|James.Smith.@example.com      |+1-578-398-5236|88657.24275976823 |Marketing         |4            |\n|Patricia  |Shetty   |Sales Executive  |1958-04-14|Patricia.Shetty.@example.com  |+1-130-463-1358|103219.03006790127|Engineering       |1            |\n|John      |Miller   |Software Engineer|1939-10-15|John.Miller.@example.com      |+1-470-839-1827|90518.82086271374 |Data and Analytics|6            |\n|John      |Davis    |Product Manager  |1991-01-22|John.Davis.@example.com       |+1-669-876-3759|90798.55591949391 |Data and Analytics|1            |\n|Robert    |Miller   |HR Specialist    |1966-03-29|Robert.Miller.@example.com    |+1-711-544-4088|97114.44013586667 |Marketing         |5            |\n|Srishti   |Davis    |Data Engineer    |1942-06-29|Srishti.Davis.@example.com    |+1-775-524-8908|57286.64649231718 |Data and Analytics|2            |\n|Robert    |Davis    |Data Engineer    |1967-04-03|Robert.Davis.@example.com     |+1-145-540-4540|81544.94970686873 |HR                |2            |\n|James     |Jones    |Data Engineer    |2012-08-11|James.Jones.@example.com      |+1-939-212-1221|77779.27197392733 |Engineering       |4            |\n|Srishti   |Jones    |HR Specialist    |2023-06-24|Srishti.Jones.@example.com    |+1-178-289-4668|72126.29782351096 |Marketing         |5            |\n|Mary      |Brown    |Software Engineer|2016-08-08|Mary.Brown.@example.com       |+1-488-169-8580|63513.55607727563 |Data and Analytics|5            |\n|Linda     |Jones    |Data Scientist   |1982-06-06|Linda.Jones.@example.com      |+1-424-265-1312|58757.91283169232 |HR                |4            |\n|Michael   |García   |Software Engineer|1993-12-31|Michael.García.@example.com   |+1-574-133-1864|96062.72254554348 |Finance           |6            |\n|Jennifer  |Jones    |HR Specialist    |1974-02-26|Jennifer.Jones.@example.com   |+1-582-87-4216 |92769.49068465423 |Marketing         |1            |\n|John      |Jones    |HR Specialist    |2001-10-30|John.Jones.@example.com       |+1-208-41-8842 |94323.25702758957 |Finance           |4            |\n|Srishti   |Davis    |Product Manager  |1978-09-15|Srishti.Davis.@example.com    |+1-425-792-4196|63939.698918206384|Data and Analytics|2            |\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "emp.where(\"salary>1000\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d7fd482-8eda-424d-8edf-2792ecb46583",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- Now because you used Cache before running the above cell, the above df with filter executed even faster(less than 1 second) as compared to before(check cell 3 above- It had taken 9 seconds to run without Cache).\n",
    "- Check Spark UI *SQL/DatFrame* Tab for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50c3ff2-0d8a-48b4-bf21-4c84f9455fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: DataFrame[first_name: string, last_name: string, job_title: string, dob: date, email: string, phone: string, salary: double, department: string, department_id: int]"
     ]
    }
   ],
   "source": [
    "# Remove CACHE\n",
    "emp.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26bbb949-f3b9-48bf-8848-4b17a5187604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The above action will REMOVE CACHE from the 'Storage' tab of 'Spark UI', which was created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b5f0de-4a7d-4aea-9ab7-74f51da1b162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 10000000"
     ]
    }
   ],
   "source": [
    "emp_cache = emp.cache()\n",
    "emp_cache.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3876a2cf-26b4-40bd-80b9-92d77386c0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\n|first_name|last_name|job_title        |dob       |email                         |phone          |salary            |department        |department_id|\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\n|Jennifer  |Williams |HR Specialist    |1951-01-21|Jennifer.Williams.@example.com|+1-845-311-804 |42951.90537045701 |Finance           |6            |\n|James     |Miller   |Sales Executive  |1939-09-25|James.Miller.@example.com     |+1-274-633-7306|50933.8591162336  |Data and Analytics|6            |\n|Linda     |Jones    |Data Scientist   |2023-05-26|Linda.Jones.@example.com      |+1-149-733-8924|66274.49226944339 |Data and Analytics|2            |\n|Srishti   |Smith    |Data Engineer    |2003-01-16|Srishti.Smith.@example.com    |+1-790-373-5222|43705.485219830625|Engineering       |4            |\n|Michael   |Brown    |Software Engineer|1973-08-28|Michael.Brown.@example.com    |+1-223-271-7921|97427.30108330262 |Data and Analytics|6            |\n|James     |Smith    |Product Manager  |1963-03-06|James.Smith.@example.com      |+1-578-398-5236|88657.24275976823 |Marketing         |4            |\n|Patricia  |Shetty   |Sales Executive  |1958-04-14|Patricia.Shetty.@example.com  |+1-130-463-1358|103219.03006790127|Engineering       |1            |\n|John      |Miller   |Software Engineer|1939-10-15|John.Miller.@example.com      |+1-470-839-1827|90518.82086271374 |Data and Analytics|6            |\n|John      |Davis    |Product Manager  |1991-01-22|John.Davis.@example.com       |+1-669-876-3759|90798.55591949391 |Data and Analytics|1            |\n|Robert    |Miller   |HR Specialist    |1966-03-29|Robert.Miller.@example.com    |+1-711-544-4088|97114.44013586667 |Marketing         |5            |\n|Srishti   |Davis    |Data Engineer    |1942-06-29|Srishti.Davis.@example.com    |+1-775-524-8908|57286.64649231718 |Data and Analytics|2            |\n|Robert    |Davis    |Data Engineer    |1967-04-03|Robert.Davis.@example.com     |+1-145-540-4540|81544.94970686873 |HR                |2            |\n|James     |Jones    |Data Engineer    |2012-08-11|James.Jones.@example.com      |+1-939-212-1221|77779.27197392733 |Engineering       |4            |\n|Srishti   |Jones    |HR Specialist    |2023-06-24|Srishti.Jones.@example.com    |+1-178-289-4668|72126.29782351096 |Marketing         |5            |\n|Mary      |Brown    |Software Engineer|2016-08-08|Mary.Brown.@example.com       |+1-488-169-8580|63513.55607727563 |Data and Analytics|5            |\n|Linda     |Jones    |Data Scientist   |1982-06-06|Linda.Jones.@example.com      |+1-424-265-1312|58757.91283169232 |HR                |4            |\n|Michael   |García   |Software Engineer|1993-12-31|Michael.García.@example.com   |+1-574-133-1864|96062.72254554348 |Finance           |6            |\n|Jennifer  |Jones    |HR Specialist    |1974-02-26|Jennifer.Jones.@example.com   |+1-582-87-4216 |92769.49068465423 |Marketing         |1            |\n|John      |Jones    |HR Specialist    |2001-10-30|John.Jones.@example.com       |+1-208-41-8842 |94323.25702758957 |Finance           |4            |\n|Srishti   |Davis    |Product Manager  |1978-09-15|Srishti.Davis.@example.com    |+1-425-792-4196|63939.698918206384|Data and Analytics|2            |\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "emp.where(\"salary>1000\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad11453e-e4a3-4c20-a554-8bc0cc056e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the above two cells we can understand that:\n",
    "- It took less then 1 second to run because-\n",
    "- SPark keeps a lineage and whenever we cache a data and refer the original dataframe ('emp' in this case). it tried to read the data from cache, rather than from the orginal data source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93f8331-6005-4fce-8518-cf855279e040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: DataFrame[first_name: string, last_name: string, job_title: string, dob: date, email: string, phone: string, salary: double, department: string, department_id: int]"
     ]
    }
   ],
   "source": [
    "emp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac261a6-8fe2-4ea0-8b19-745e15eeba37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: 1428007"
     ]
    }
   ],
   "source": [
    "emp_cache_fil = emp.where(\"salary<50000\").cache()\n",
    "emp_cache_fil.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e3eaf68-8ac8-485c-82a8-21060e9e3294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The below code will not read from a CACHE because the filter had triggered a partial cache.\n",
    "- Now if you point the original data frame, the Cache will be invalidated and Spark will again read the data from the original source\n",
    "- This is why, partial CACHE is very dangerous if not done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c31b51d-e616-463a-99c9-0b867c98e108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: DataFrame[first_name: string, last_name: string, job_title: string, dob: date, email: string, phone: string, salary: double, department: string, department_id: int]"
     ]
    }
   ],
   "source": [
    "emp.where(\"salary<60000\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664d36db-de29-40e6-a181-72cc2c7cc34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To REMOVE all the Cache\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a28c872-8b30-443c-a3ab-06e7ca2d5bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Different Storage levels involved in CACHE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36207c35-56a6-434d-b068-4eb816b9228a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER\n",
    "import pyspark\n",
    "emp_persist= emp.persist(pyspark.StorageLevel.MEMORY_ONLY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2106b55-88cd-417b-907d-2264ae0e7c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- This time the Cache will show 'Storage Level:Memory Serialized 1x Replicated'.\n",
    "- Cache puts the data as unserialized,but if we put it with persist, the data gets serialized in the memory only.\n",
    "Let's do that below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d142135d-48a0-48d3-891d-c3e234e944da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_persist.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41d6d72d-f55c-4e75-b386-28076d0125d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> NOTE: \n",
    "- If the size of your data is greater than the MEMORY you have specified(will throw out of memory error), ALWAYS remember to use *'MEMORY_AND DISK'*.\n",
    "- For PySpark, whenever we do '*MEMORY_ONLY*', by default the data is serialized in *'MEMORY_AND_DISK'*. So, you cannot use *'MEMORY_ONLY_SER'* and *'MEMORY_AND_DISK_SER'* in PySpark.\n",
    "- *'MEMORY_ONLY_SER'* is used for Scala and Java jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "070a3a7b-2123-4408-a7ac-6bb8594e72a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d30288-837d-4984-916d-f16dd4f3a3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2\n",
    "emp_persist= emp.persist(pyspark.StorageLevel.MEMORY_ONLY_2)\n",
    "emp_persist.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2762c04b-91a5-42b9-a219-42791294dd65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- This time the Cache will show 'Storage Level:Memory Serialized 2x Replicated'.\n",
    "- It implies that it is replicated twice in all the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ebd2e12-ea8f-4ab7-86d9-16d75e5b2dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> IMPORTANT DIFFERENCE between CACHE and PERSIST method:\n",
    "- PERSIST: You need to define the storage level you need to use\n",
    "- CACHE: The default storage level is '*MEMORY_AND_DISK*' and the data is de-serialized.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "16_Spark_Caching_Persisting_Techniques",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
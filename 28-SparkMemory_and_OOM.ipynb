{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfda0eb-81a4-4bfc-9bb3-28106b8c7ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0721-082313-5ztn6db/driver-2219262838439062708\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0721-082313-5ztn6db/driver-2219262838439062708\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Spark Memory Management\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.executor.cores\",4)\n",
    "        .config(\"spark.cores.max\",8)\n",
    "        .config(\"spark.executor.memory\", \"512M\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c791a474-14c4-4f81-8e28-e6999e419950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Memory Calculation per Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5b5464-80a0-48ed-af86-ad7081af848d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The On-Heap Usuable Memory is 455.68 MB\n"
     ]
    }
   ],
   "source": [
    "# JVM On-Heap Usable Memory (89% of executor memory)\n",
    "print(f\"The On-Heap Usuable Memory is {512*0.89} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d740280-e907-478b-a5b8-f9dd0de2de33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The On-Heap Non-Reserved Memory is 155.68 MB\n"
     ]
    }
   ],
   "source": [
    "# Subtracting the Reserved Memory (300MB)\n",
    "print(f\"The On-Heap Non-Reserved Memory is {455.68-300} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbcb7c3c-eb5a-4fed-b009-e829e128523a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Spark Memory is 93.408 MB\n"
     ]
    }
   ],
   "source": [
    "# Total Spark Memory (Unified Memory - Storage + Execution Memory)(60% Default) spark.memory.fraction = 0.6\n",
    "print(f\"Total Spark Memory is {155.68*0.6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f339fd9d-7c74-46d1-9fd6-ba7e5396e0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Undefined Memory is 62.272000000000006 MB\n"
     ]
    }
   ],
   "source": [
    "# User /Undefined Memory (Not controlled by Spark) (remaining 40% default)\n",
    "print(f\"Total Undefined Memory is {155.68*0.4} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5f092c-f78c-4af9-8ee9-a077d92078c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Storage Memory is 46.704 MB\n"
     ]
    }
   ],
   "source": [
    "# Storage Memory (spark.memory.storageFraction = 0.5)\n",
    "print(f\"Total Storage Memory is {93.408*0.5} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e752b97-64ca-47ee-9d54-db4f3a32c57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each Core Memory is 11.676 MB\n"
     ]
    }
   ],
   "source": [
    "# Executor Memory per Core -- We have 8 executors and 4 cores each\n",
    "print(f\"Each Core Memory is {46.704/4} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ed21bf-e1bc-4279-94ec-cd6187efb8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Out of Memory(OOM) Errors on Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31109a57-5a8e-4b70-b288-66e4656e08a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable Adaptive Query Engine(AQE) and Broadcast Join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42229af5-a17a-4b95-aa4b-01f1a08d0ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\nfalse\n-1\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.adaptive.autoBroadcastJoinThreshold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7b9f7e-b538-434c-8ae8-c8def19d66d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 118M\n-rw-r--r-- 1 root root 12M Jul 21 09:00 file_xs.txt\n-rw-r--r-- 1 root root 12M Jul 21 09:00 file_singleline_xs.txt\n-rw-r--r-- 1 root root 22M Jul 21 09:00 file_s.txt\n-rw-r--r-- 1 root root 75M Jul 21 09:00 cities.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -ltrh /data/datasets/oom_example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22fc355-3199-40cd-b9f2-e99537f4bed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ files\n",
    "# df = spark.read.format(\"text\").load(\"/data/datasets/oom_example/file_singleline_xs.txt\")  -- size 12MB\n",
    "df = spark.read.format(\"text\").load(\"file:///data/datasets/oom_example/file_singleline_xs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2f99dd-1312-4f08-b696-0d25f8e5f570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 1"
     ]
    }
   ],
   "source": [
    "# Cache data\n",
    "# Data gets stored into the Storage Memory after Caching.\n",
    "df.cache().count()  # it is a single line file so the output is '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5395a5c7-adf2-4ac5-b828-b70347b9a18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the above execution, we read a file of size 12MB(from DISK-SER) and after doing 'count'(action) it got Deserialised and expanded to 300 MB in Memory(Check Spark UI executors tab for more info and there is skewing of data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d30abf-bd08-4a40-b124-27886e55ed0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- value: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73fc67c-7659-4d22-95c7-9897b501338f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode data to Count words in the file(s)\n",
    "from pyspark.sql.functions import lower, split, explode, count, lit\n",
    "dffinal = (\n",
    "    df.withColumn(\"value\", lower(\"value\"))\n",
    "    .withColumn(\"splittedval\", split(\"value\", \" \"))\n",
    "    .withColumn(\"explodedval\", explode(\"splittedval\"))\n",
    "    .drop(\"splittedval\", \"value\")\n",
    "    .groupBy(\"explodedval\").agg(count(lit(1)).alias(\"cnt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65fbe826-db84-495c-8730-f7842e52573a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The above 'explode' operation would cause multiplication of records which will cause the memory to overflow!\n",
    "- The Executor will start crashing because of this and you will see 'Out Of Memory' Error in the Spark UI Job tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac87ccc-914b-4d22-9931-55e269bb3caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|explodedval|   cnt|\n+-----------+------+\n| keyboards.| 44193|\n|       used| 44193|\n|       lazy| 44193|\n| landscape.| 44193|\n|      green| 44193|\n|        for| 44193|\n|      jumps| 44193|\n|  sparkling| 44193|\n|   contains| 44193|\n|     letter| 44193|\n|    pangram| 44193|\n|      water| 44193|\n|        fox| 44193|\n|    testing| 44193|\n|   sentence| 44193|\n|         it| 44193|\n|      flows| 44193|\n|        the|176772|\n|       dog.| 44193|\n|      clear| 44193|\n+-----------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dffinal.show() # This should fail if there is 'Out Of Memory' error in the job executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883afff5-4c46-4d94-a550-02c4ccff2f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|               value|\n+--------------------+\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Lets read the same singleline text in multiline format\n",
    "df1 = spark.read.format(\"text\").load(\"file:///data/datasets/oom_example/file_xs.txt\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323a5d31-350a-4482-bb73-a00bd73cff02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- value: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cd3ae0-e640-4c86-973f-f4e866c8342e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, split, explode, count, lit\n",
    "dffinal1 = (\n",
    "    df1.withColumn(\"value\", lower(\"value\"))\n",
    "    .withColumn(\"splittedval\", split(\"value\", \" \"))\n",
    "    .withColumn(\"explodedval\", explode(\"splittedval\"))\n",
    "    .drop(\"splittedval\", \"value\")\n",
    "    .groupBy(\"explodedval\").agg(count(lit(1)).alias(\"cnt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f8eb05-ccd5-459a-96ea-5fe8f39365e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|explodedval|   cnt|\n+-----------+------+\n| keyboards.| 44193|\n|       used| 44193|\n|       lazy| 44193|\n| landscape.| 44193|\n|      green| 44193|\n|        for| 44193|\n|      jumps| 44193|\n|  sparkling| 44193|\n|   contains| 44193|\n|     letter| 44193|\n|    pangram| 44193|\n|      water| 44193|\n|        fox| 44193|\n|    testing| 44193|\n|   sentence| 44193|\n|         it| 44193|\n|      flows| 44193|\n|        the|176772|\n|       dog.| 44193|\n|      clear| 44193|\n+-----------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dffinal1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5f701d-a11f-4789-b28f-7de055ef2975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|               value|\n+--------------------+\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n|The quick brown f...|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format(\"text\").load(\"file:///data/datasets/oom_example/file_s.txt\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536ba6cc-0522-4062-a726-6e5f13e7ed1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, split, explode, count, lit\n",
    "dffinal2 = (\n",
    "    df2.withColumn(\"value\", lower(\"value\"))\n",
    "    .withColumn(\"splittedval\", split(\"value\", \" \"))\n",
    "    .withColumn(\"explodedval\", explode(\"splittedval\"))\n",
    "    .drop(\"splittedval\", \"value\")\n",
    "    .groupBy(\"explodedval\").agg(count(lit(1)).alias(\"cnt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08431671-d1e9-4e4e-a6c3-a6610e3ca53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|explodedval|   cnt|\n+-----------+------+\n| keyboards.| 84369|\n|       used| 84369|\n|       lazy| 84369|\n| landscape.| 84369|\n|      green| 84369|\n|        for| 84369|\n|      jumps| 84369|\n|  sparkling| 84369|\n|   contains| 84369|\n|     letter| 84369|\n|    pangram| 84369|\n|      water| 84369|\n|        fox| 84369|\n|    testing| 84369|\n|   sentence| 84369|\n|         it| 84369|\n|      flows| 84369|\n|        the|337476|\n|       dog.| 84369|\n|      clear| 84369|\n+-----------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dffinal2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed39cbc9-bbc7-45bf-9dcf-23462554e412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write in 'noop' format for simulation\n",
    "dffinal2.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5417e2fb-a55a-4141-b3dd-21c8ea4fc62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The 'option': wholtext=True will write the contents in a single record show below.\n",
    "- It wrote the multiline file in a single record! So, now when it run it with explode, it will again fail with OOM erros!!!\n",
    "- So, you have to be careful while using options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05112a05-a3a6-43e6-91e5-b993bac0dda2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|               value|\n+--------------------+\n|The quick brown f...|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.format(\"text\").option(\"wholetext\",True).load(\"file:///data/datasets/oom_example/file_xs.txt\")\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27880f21-e7fb-4605-aa42-f82f901f5733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "NOTE:\n",
    "- Sometimes a '**Bad Core**' can also cause '**OOM(Out Of Memory)**' error.\n",
    "- So, it is not always that you have to expand your memory to work it out.\n",
    "- Also, OOM errors are still bound to be encountered even if the 'AQE' is enabled (Because the logic still remains same in the background)\n",
    "- It is also very important to understand how memory works and how a BAD CODE can lead to OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0f24fa-99b3-4f1e-b3c7-0fa4c4ef89cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "28-SparkMemory_and_OOM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
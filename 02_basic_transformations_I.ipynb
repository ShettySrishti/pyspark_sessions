{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c03f034-eca9-43dc-85a1-f0a6182bc547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark Introduction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c49b857-0814-40d1-8d07-6f2b0c0ad06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0601-110416-rcre241o/driver-9027803605301150018\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0601-110416-rcre241o/driver-9027803605301150018\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "939005c3-f863-4354-8978-7288d5bd31ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Employee Data & Schema  --> list of lists\n",
    "data = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60f3ee7-eef3-4511-9e38-2acefd7e89db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create emp DataFrame\n",
    "emp = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ad9051-4438-4833-8d98-b4585e854600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 8"
     ]
    }
   ],
   "source": [
    "#Check number of partitions\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224d926a-6855-4b75-8bad-fecc80587eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emp.show()  #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327ecd4f-1bce-4b6a-9b1c-8fcd37c430d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: string (nullable = true)\n |-- department_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- hire_date: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Schema for emp\n",
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87ce8e1-0b99-4857-96d2-9c7574add70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: StructType([StructField('employee_id', StringType(), True), StructField('department_id', StringType(), True), StructField('name', StringType(), True), StructField('age', StringType(), True), StructField('gender', StringType(), True), StructField('salary', StringType(), True), StructField('hire_date', StringType(), True)])"
     ]
    }
   ],
   "source": [
    "#Alternate way of schema \n",
    "emp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c3c56f-4798-4cf4-b02f-077b76b0edc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Basic example for schema\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "#To write this in spark's native datatype we need to import the above libs\n",
    "schema_string = \"name string, age int\" \n",
    "\n",
    "#Converting the above schema string to spark's native string\n",
    "schema_string = StructType([\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"age\", IntegerType(), nullable=True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "558560d8-9a22-49eb-9af3-fc5cae56f311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: Column<'name'>"
     ]
    }
   ],
   "source": [
    "#Columns and expressions\n",
    "\n",
    "from pyspark.sql.functions import col, expr\n",
    "expr(\"name\")\n",
    "col(\"name\")\n",
    "\n",
    "#Both will give same result \"Column<'name'>\" because any manipulation done on a column is considered as an expression!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fdf1f963-c737-4826-95eb-eea6ca570a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: Column<'salary'>"
     ]
    }
   ],
   "source": [
    "#Call a column from a dataframe\n",
    "\n",
    "from pyspark.sql.functions import col, expr\n",
    "emp.salary\n",
    "emp[\"salary\"]\n",
    "\n",
    "#Both will give same result --> Column<'salary'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b16a1814-5817-4dfe-b496-3bb0509fbe72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+\n|employee_id|         name|age|salary|\n+-----------+-------------+---+------+\n|        001|     John Doe| 30| 50000|\n|        002|   Jane Smith| 25| 45000|\n|        003|    Bob Brown| 35| 55000|\n|        004|    Alice Lee| 28| 48000|\n|        005|    Jack Chan| 40| 60000|\n|        006|    Jill Wong| 32| 52000|\n|        007|James Johnson| 42| 70000|\n|        008|     Kate Kim| 29| 51000|\n|        009|      Tom Tan| 33| 58000|\n|        010|     Lisa Lee| 27| 47000|\n|        011|   David Park| 38| 65000|\n|        012|   Susan Chen| 31| 54000|\n|        013|    Brian Kim| 45| 75000|\n|        014|    Emily Lee| 26| 46000|\n|        015|  Michael Lee| 37| 63000|\n|        016|  Kelly Zhang| 30| 49000|\n|        017|  George Wang| 34| 57000|\n|        018|    Nancy Liu| 29| 50000|\n|        019|  Steven Chen| 36| 62000|\n|        020|    Grace Kim| 32| 53000|\n+-----------+-------------+---+------+\n\n+-----------+-------------+---+------+\n|employee_id|         name|age|salary|\n+-----------+-------------+---+------+\n|        001|     John Doe| 30| 50000|\n|        002|   Jane Smith| 25| 45000|\n|        003|    Bob Brown| 35| 55000|\n|        004|    Alice Lee| 28| 48000|\n|        005|    Jack Chan| 40| 60000|\n|        006|    Jill Wong| 32| 52000|\n|        007|James Johnson| 42| 70000|\n|        008|     Kate Kim| 29| 51000|\n|        009|      Tom Tan| 33| 58000|\n|        010|     Lisa Lee| 27| 47000|\n|        011|   David Park| 38| 65000|\n|        012|   Susan Chen| 31| 54000|\n|        013|    Brian Kim| 45| 75000|\n|        014|    Emily Lee| 26| 46000|\n|        015|  Michael Lee| 37| 63000|\n|        016|  Kelly Zhang| 30| 49000|\n|        017|  George Wang| 34| 57000|\n|        018|    Nancy Liu| 29| 50000|\n|        019|  Steven Chen| 36| 62000|\n|        020|    Grace Kim| 32| 53000|\n+-----------+-------------+---+------+\n\n+-----------+-------------+---+------+\n|employee_id|         name|age|salary|\n+-----------+-------------+---+------+\n|        001|     John Doe| 30| 50000|\n|        002|   Jane Smith| 25| 45000|\n|        003|    Bob Brown| 35| 55000|\n|        004|    Alice Lee| 28| 48000|\n|        005|    Jack Chan| 40| 60000|\n|        006|    Jill Wong| 32| 52000|\n|        007|James Johnson| 42| 70000|\n|        008|     Kate Kim| 29| 51000|\n|        009|      Tom Tan| 33| 58000|\n|        010|     Lisa Lee| 27| 47000|\n|        011|   David Park| 38| 65000|\n|        012|   Susan Chen| 31| 54000|\n|        013|    Brian Kim| 45| 75000|\n|        014|    Emily Lee| 26| 46000|\n|        015|  Michael Lee| 37| 63000|\n|        016|  Kelly Zhang| 30| 49000|\n|        017|  George Wang| 34| 57000|\n|        018|    Nancy Liu| 29| 50000|\n|        019|  Steven Chen| 36| 62000|\n|        020|    Grace Kim| 32| 53000|\n+-----------+-------------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#SELECT employee_id, name, age, salary from emp\n",
    "\n",
    "emp_filtered = emp.select(col(\"employee_id\"), col(\"name\"), col(\"age\"), col(\"salary\"))\n",
    "emp_filtered.show()\n",
    "\n",
    "emp_filtered1 = emp.select(expr(\"employee_id\"), expr(\"name\"), expr(\"age\"), expr(\"salary\"))\n",
    "emp_filtered1.show()\n",
    "\n",
    "emp_filtered2 = emp.select(emp.employee_id, emp.name, emp.age, emp.salary)\n",
    "emp_filtered2.show()\n",
    "\n",
    "# All the above methods will give same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "95428917-6d75-4c4d-8e65-7cc11414a695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#SELECT employee_id as emp_id, name, cast(age as int) as age, salary from emp_cols using 'expr'\n",
    "\n",
    "emp_casted = emp_filtered.select(expr(\"employee_id as emp_id\"), emp.name, expr(\"cast(age as int) as age\"), emp.salary)\n",
    "\n",
    "#OR \n",
    "emp_casted1 = emp_filtered.selectExpr(\"employee_id as emp_id\", \"name\", \"cast(age as int) as age\", \"salary\")\n",
    "\n",
    "\n",
    "#Both the above methods will give same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7dfe78ca-c0e0-4b64-97b0-cbdb898efb83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n|emp_id|         name|age|salary|\n+------+-------------+---+------+\n|   001|     John Doe| 30| 50000|\n|   002|   Jane Smith| 25| 45000|\n|   003|    Bob Brown| 35| 55000|\n|   004|    Alice Lee| 28| 48000|\n|   005|    Jack Chan| 40| 60000|\n|   006|    Jill Wong| 32| 52000|\n|   007|James Johnson| 42| 70000|\n|   008|     Kate Kim| 29| 51000|\n|   009|      Tom Tan| 33| 58000|\n|   010|     Lisa Lee| 27| 47000|\n|   011|   David Park| 38| 65000|\n|   012|   Susan Chen| 31| 54000|\n|   013|    Brian Kim| 45| 75000|\n|   014|    Emily Lee| 26| 46000|\n|   015|  Michael Lee| 37| 63000|\n|   016|  Kelly Zhang| 30| 49000|\n|   017|  George Wang| 34| 57000|\n|   018|    Nancy Liu| 29| 50000|\n|   019|  Steven Chen| 36| 62000|\n|   020|    Grace Kim| 32| 53000|\n+------+-------------+---+------+\n\n+------+-------------+---+------+\n|emp_id|         name|age|salary|\n+------+-------------+---+------+\n|   001|     John Doe| 30| 50000|\n|   002|   Jane Smith| 25| 45000|\n|   003|    Bob Brown| 35| 55000|\n|   004|    Alice Lee| 28| 48000|\n|   005|    Jack Chan| 40| 60000|\n|   006|    Jill Wong| 32| 52000|\n|   007|James Johnson| 42| 70000|\n|   008|     Kate Kim| 29| 51000|\n|   009|      Tom Tan| 33| 58000|\n|   010|     Lisa Lee| 27| 47000|\n|   011|   David Park| 38| 65000|\n|   012|   Susan Chen| 31| 54000|\n|   013|    Brian Kim| 45| 75000|\n|   014|    Emily Lee| 26| 46000|\n|   015|  Michael Lee| 37| 63000|\n|   016|  Kelly Zhang| 30| 49000|\n|   017|  George Wang| 34| 57000|\n|   018|    Nancy Liu| 29| 50000|\n|   019|  Steven Chen| 36| 62000|\n|   020|    Grace Kim| 32| 53000|\n+------+-------------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#SHOW the above dataframe #ACTION\n",
    "emp_casted.show()\n",
    "emp_casted1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "25673512-d84c-4bdf-942c-939e3c032cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Verifying whether 'age' is casted to int\n",
    "emp_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65de3ab-5331-4cea-95bd-a4ed9ee931ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FILTER employee based on age\n",
    "#SELECT emp_id, name, age salary from emp_casted where age > 30\n",
    "\n",
    "emp_final = emp_casted.selectExpr(\"emp_id\", \"name\", \"age\", \"salary\"). where(\"age > 30\")  \n",
    "\n",
    "#just '.select' will also work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0224531-21a7-40ef-9727-ca213a4286e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n|emp_id|         name|age|salary|\n+------+-------------+---+------+\n|   003|    Bob Brown| 35| 55000|\n|   005|    Jack Chan| 40| 60000|\n|   006|    Jill Wong| 32| 52000|\n|   007|James Johnson| 42| 70000|\n|   009|      Tom Tan| 33| 58000|\n|   011|   David Park| 38| 65000|\n|   012|   Susan Chen| 31| 54000|\n|   013|    Brian Kim| 45| 75000|\n|   015|  Michael Lee| 37| 63000|\n|   017|  George Wang| 34| 57000|\n|   019|  Steven Chen| 36| 62000|\n|   020|    Grace Kim| 32| 53000|\n+------+-------------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#SHOW the above dataframe (ACTION)\n",
    "\n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43cdade-6d39-4d80-842c-f2fc056cb89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#WRITE the data back as CSV (ACTION)\n",
    "\n",
    "emp_final.write.format(\"csv\").save(\"data/output/2/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678e7ce1-fb41-415d-beeb-f73faa2c4bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[52]: StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True)])"
     ]
    }
   ],
   "source": [
    "# IMPORTANT\n",
    "\n",
    "#We can use 'schema_string' directly rather than writing the spark native datatype. \n",
    "schema_string = \"name string, age int\"\n",
    "\n",
    "#Shown below is how spark converts the 'schema_string' implicitly to its native datatype\n",
    "from pyspark.sql.types import _parse_datatype_string\n",
    "\n",
    "#Passing 'schema_string' into the method above which will fetch the spark native datatype\n",
    "native_spark_schema = _parse_datatype_string(schema_string)\n",
    "\n",
    "#Calling \n",
    "native_spark_schema"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_basic_transformations_I",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
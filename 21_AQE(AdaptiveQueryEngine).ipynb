{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0718-154715-llug4xtu/driver-1930990475653713018\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0718-154715-llug4xtu/driver-1930990475653713018\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"AQE in Spark\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.executor.cores\",4)\n",
    "        .config(\"spark.cores.max\",8)\n",
    "        .config(\"spark.executor.memory\", \"512M\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", True) # Enbale Adaptive Query Engine(AQE) (NOTE:It is enabled by default in 3.0, So True is not necessarily needed)\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ec5628-87a7-4ee8-b632-7fd7fd0861b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c19a0d3d-315b-45ce-88c7-dd047ec09920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Coalescing post-shuffle partitions --> removes unnecessary shuffle partitions\n",
    "# Skewed Join optimization (balanaces partition size) --> joins smaller pertitions OR/AND splits bigger partitions\n",
    "\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "# spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc19c16-bd20-4d38-9361-02c062c5f838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: '2MB'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix partition size to avoid skew\n",
    "spark.conf.get(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\",\"1MB\") # DEfault Value : 64MB\n",
    "spark.conf.get(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\",\"2MB\") # Default value 256 MB  -- Means if szie execeed 2MB we consider it as skewed partition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd12430-cb7d-465c-9ba0-a4501d4cd8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> NOTE: In many cases we dont need to modify skewedPartitionThresholdInBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8ccaf1-bfb3-4445-aecb-cc8811f1783b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1MB\n2MB\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\",\"1MB\"))\n",
    "print(spark.conf.get(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\",\"2MB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3ecdac-5721-47a0-b53a-d85f6d1232d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10 million records\n",
    "emp_schema = \"first_name string, last_name string, job_title string, dob date, email string, phone string, salary double, department string, department_id integer\"\n",
    "emp = spark.read.schema(emp_schema).option(\"header\",True).csv(\"/data/input/datasets/employee_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf48e4b6-3ead-4c68-b982-8a6bf6d13b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV file with 10 records\n",
    "dept_schema =\"department_id int, department_name string, description string, city string, state string, country string \"\n",
    "dept = spark.read.schema(dept_schema).option(\"header\",True).csv(\"/data/input/datasets/department_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d7a848-1093-445a-96d9-3a229e59e7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JOINING datasets\n",
    "dfjoined = emp.join(dept, on='department_id', how=\"left_outer\")\n",
    "dfjoined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7dd99e6-fc2d-4069-b896-d7291761aee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [department_id#132, first_name#124, last_name#125, job_title#126, dob#127, email#128, phone#129, salary#130, department#131, department_name#143, description#144, city#145, state#146, country#147]\n   +- BroadcastHashJoin [department_id#132], [department_id#142], LeftOuter, BuildRight, false, true\n      :- FileScan csv [first_name#124,last_name#125,job_title#126,dob#127,email#128,phone#129,salary#130,department#131,department_id#132] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=260]\n         +- Filter isnotnull(department_id#142)\n            +- FileScan csv [department_id#142,department_name#143,description#144,city#145,state#146,country#147] Batched: false, DataFilters: [isnotnull(department_id#142)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n\n\n"
     ]
    }
   ],
   "source": [
    "dfjoined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8703da4b-29ef-4479-95e5-a36c7f3f9e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In Spark UI, if you check '*Total succeeded*' in Jobs tab, you will see it has taken significantly lesser shuffle partitions because Spark has coalesced unnecessary Shuffle Partitions post Shuffle\n",
    "- Spark has automatically taken care of Skewness using AQE and created post shuffle partitions (No spillage happening)\n",
    "- In Spark 3.0, if AQE is enabled and partition size is set up properly, Spark will automtically take careof the Spillage of the data into Memory/Disk and hence fixes Skewness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c6b265-e4d0-471f-985c-d91dd4a8eb53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10MB\n"
     ]
    }
   ],
   "source": [
    "# Converting JOIN to Broadcast Join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "print(spark.conf.get(\"spark.sql.adaptive.autoBroadcastJoinThreshold\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24749b1-c3f4-4199-95a7-32b85ccd238c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> IMPORTANT: With AQE enabled you dont need to specify broadcast on a smaller dataframe. The AQE auto takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3225bd84-4c29-4c18-b22c-13101bcde887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JOINING datasets\n",
    "dfjoined = emp.join(dept, on='department_id', how=\"left_outer\")\n",
    "dfjoined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7fb5553-d81e-4284-873c-7398563744b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   *(2) Project [department_id#132, first_name#124, last_name#125, job_title#126, dob#127, email#128, phone#129, salary#130, department#131, department_name#143, description#144, city#145, state#146, country#147]\n   +- *(2) BroadcastHashJoin [department_id#132], [department_id#142], LeftOuter, BuildRight, false, true\n      :- FileScan csv [first_name#124,last_name#125,job_title#126,dob#127,email#128,phone#129,salary#130,department#131,department_id#132] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n      +- ShuffleQueryStage 0, Statistics(sizeInBytes=1144.0 B, rowCount=10, isRuntime=true)\n         +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=505]\n            +- *(1) Filter isnotnull(department_id#142)\n               +- FileScan csv [department_id#142,department_name#143,description#144,city#145,state#146,country#147] Batched: false, DataFilters: [isnotnull(department_id#142)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n+- == Initial Plan ==\n   Project [department_id#132, first_name#124, last_name#125, job_title#126, dob#127, email#128, phone#129, salary#130, department#131, department_name#143, description#144, city#145, state#146, country#147]\n   +- BroadcastHashJoin [department_id#132], [department_id#142], LeftOuter, BuildRight, false, true\n      :- FileScan csv [first_name#124,last_name#125,job_title#126,dob#127,email#128,phone#129,salary#130,department#131,department_id#132] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=452]\n         +- Filter isnotnull(department_id#142)\n            +- FileScan csv [department_id#142,department_name#143,description#144,city#145,state#146,country#147] Batched: false, DataFilters: [isnotnull(department_id#142)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n\n\n"
     ]
    }
   ],
   "source": [
    "dfjoined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d3e04e5-25a0-47fc-a707-3786764ab054",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "> IMPORTANT:\n",
    "- **Dynamic Optimization**: AQE allows Spark to optimize query plans dynamically at runtime based on actual data statistics.\n",
    "- **Improves Performance**: It helps in optimizing joins, skew handling, and partitioning, leading to faster query execution.\n",
    "- **Automatic Adjustments**: Adjusts execution strategies such as join types and shuffle partitions without manual intervention.\n",
    "- **Enables Better Resource Utilization**: Enhances resource efficiency by adapting to data characteristics during execution.\n",
    "- In the latest versions of Spark (from Spark 3.0 onwards), Adaptive Query Execution (AQE) is enabled by default. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "21_AQE(AdaptiveQueryEngine)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
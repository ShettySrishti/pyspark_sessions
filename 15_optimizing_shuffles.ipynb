{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0706-083214-7l388b8c/driver-535718982730242282\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0706-083214-7l388b8c/driver-535718982730242282\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Optimizing Shuffles\")\n",
    "        .config(\"spark.executors.cores\",4)\n",
    "        .config(\"spark.cores.max\", 16)\n",
    "        .config(\"spark.executor.memory\", '512M')\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42dac558-806e-4e72-a334-9ade50ca74ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> IMP: In a Spark Standalone master, the defaut parallelism will be shown as 16 as per the configs we have specified in 'spark.config' above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d627ef-6c18-4cbb-9799-fabdd3d522a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to a Spark standalone master.\n"
     ]
    }
   ],
   "source": [
    "master_url = spark.sparkContext.master\n",
    "if master_url.startswith(\"spark://\"):\n",
    "    print(\"Spark Master URL:\", master_url)\n",
    "else:\n",
    "    print(\"Not connected to a Spark standalone master.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a556a2d1-3784-4dc5-abd2-c911cb3b968b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "- We are connected to a local master (Since we are not connected to a Spark standalone master): \n",
    "Master\n",
    "local[8]\n",
    "- If you are running Spark locally (e.g., in local[*] mode), the default parallelism is typically set to the number of threads/cores available.\n",
    "- If your environment has 8 cores, Spark will set default parallelism to 8 as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33c4113-aa0f-4e43-bb45-d9f3e280ac1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local[8]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716e7fd3-b68c-4435-b756-2414881ab53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 8"
     ]
    }
   ],
   "source": [
    "# Check default parallelism\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0dcea3-b326-4232-baf0-34c77fb6a87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable Adaptive Query Engine(AQE) and Broadcast Join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3ecdac-5721-47a0-b53a-d85f6d1232d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10 million records\n",
    "_schema = \"first_name string, last_name string, job_title string, dob date, email string, phone string, salary double, department string, department_id integer\"\n",
    "emp = spark.read.option(\"header\",True).schema(_schema).csv(\"/data/input/datasets/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "196b5d7b-626d-4050-b0ed-ca9400c17927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "IMPORTANT NOTE:\n",
    "- Option values: In Spark, options typically expect string values, so it's more correct to pass \"true\" (string) instead of True (boolean).\n",
    "- Use --> option(\"header\", \"true\"), instead of this --> option(\"header\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7b3536-985c-48fa-84ab-96613e86130d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find out AVG Salary as per Department\n",
    "from pyspark.sql.functions import avg\n",
    "emp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e51b6f-7b3b-4139-adff-d16bdc4d13a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data for Performance Benchmarking\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d71dcc3-1e08-4a20-91fa-f2b8dac5d52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+------------+\n|first_name|last_name|job_title        |dob       |email                         |phone          |salary            |department        |department_id|partition_id|\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+------------+\n|Jennifer  |Williams |HR Specialist    |1951-01-21|Jennifer.Williams.@example.com|+1-845-311-804 |42951.90537045701 |Finance           |6            |0           |\n|James     |Miller   |Sales Executive  |1939-09-25|James.Miller.@example.com     |+1-274-633-7306|50933.8591162336  |Data and Analytics|6            |0           |\n|Linda     |Jones    |Data Scientist   |2023-05-26|Linda.Jones.@example.com      |+1-149-733-8924|66274.49226944339 |Data and Analytics|2            |0           |\n|Srishti   |Smith    |Data Engineer    |2003-01-16|Srishti.Smith.@example.com    |+1-790-373-5222|43705.485219830625|Engineering       |4            |0           |\n|Michael   |Brown    |Software Engineer|1973-08-28|Michael.Brown.@example.com    |+1-223-271-7921|97427.30108330262 |Data and Analytics|6            |0           |\n|James     |Smith    |Product Manager  |1963-03-06|James.Smith.@example.com      |+1-578-398-5236|88657.24275976823 |Marketing         |4            |0           |\n|Patricia  |Shetty   |Sales Executive  |1958-04-14|Patricia.Shetty.@example.com  |+1-130-463-1358|103219.03006790127|Engineering       |1            |0           |\n|John      |Miller   |Software Engineer|1939-10-15|John.Miller.@example.com      |+1-470-839-1827|90518.82086271374 |Data and Analytics|6            |0           |\n|John      |Davis    |Product Manager  |1991-01-22|John.Davis.@example.com       |+1-669-876-3759|90798.55591949391 |Data and Analytics|1            |0           |\n|Robert    |Miller   |HR Specialist    |1966-03-29|Robert.Miller.@example.com    |+1-711-544-4088|97114.44013586667 |Marketing         |5            |0           |\n|Srishti   |Davis    |Data Engineer    |1942-06-29|Srishti.Davis.@example.com    |+1-775-524-8908|57286.64649231718 |Data and Analytics|2            |0           |\n|Robert    |Davis    |Data Engineer    |1967-04-03|Robert.Davis.@example.com     |+1-145-540-4540|81544.94970686873 |HR                |2            |0           |\n|James     |Jones    |Data Engineer    |2012-08-11|James.Jones.@example.com      |+1-939-212-1221|77779.27197392733 |Engineering       |4            |0           |\n|Srishti   |Jones    |HR Specialist    |2023-06-24|Srishti.Jones.@example.com    |+1-178-289-4668|72126.29782351096 |Marketing         |5            |0           |\n|Mary      |Brown    |Software Engineer|2016-08-08|Mary.Brown.@example.com       |+1-488-169-8580|63513.55607727563 |Data and Analytics|5            |0           |\n|Linda     |Jones    |Data Scientist   |1982-06-06|Linda.Jones.@example.com      |+1-424-265-1312|58757.91283169232 |HR                |4            |0           |\n|Michael   |García   |Software Engineer|1993-12-31|Michael.García.@example.com   |+1-574-133-1864|96062.72254554348 |Finance           |6            |0           |\n|Jennifer  |Jones    |HR Specialist    |1974-02-26|Jennifer.Jones.@example.com   |+1-582-87-4216 |92769.49068465423 |Marketing         |1            |0           |\n|John      |Jones    |HR Specialist    |2001-10-30|John.Jones.@example.com       |+1-208-41-8842 |94323.25702758957 |Finance           |4            |0           |\n|Srishti   |Davis    |Product Manager  |1978-09-15|Srishti.Davis.@example.com    |+1-425-792-4196|63939.698918206384|Data and Analytics|2            |0           |\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp.withColumn(\"partition_id\", spark_partition_id()).where(\"partition_id=0\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e6c58a-896d-45e7-a3a2-c5214d187781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: '200'"
     ]
    }
   ],
   "source": [
    "# Check Spark Shuffle Partition setting\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e71933-ad81-419b-b1db-563d65ff6f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Let's decrease the Shuffle Partitions into an approprite number to optimize it.\n",
    "- For default '200' Shuffle Partitions, it is taking a lot time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "911a0acf-4b0a-4cdb-bc9f-ae9061079f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: '16'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fadf3f-b166-4ac1-a79f-ddf5027f3f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-Write data for Performance Benchmarking\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36ac249e-5747-4f1f-b3bb-b6012b461f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> Now, Let's see if Reading a Partitioned Data will improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d602f3c2-2ef4-46a7-ab51-2a94822eb807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame partitioned by department_id\n",
    "emp.write.mode(\"overwrite\").partitionBy(\"department_id\").csv(\"/data/input/emp_partitioned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb0f76b-4213-4fb1-86ec-06649e40d574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Partitioned Data\n",
    "emp_partitioned = spark.read.schema(_schema).option(\"header\",'true').csv(\"/data/input/emp_partitioned.csv\")\n",
    "emp_partitioned_avg = emp_partitioned.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))\n",
    "\n",
    "# This will take even less time in the Spark UI to proces data since it is partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d5bf69-8880-4567-a623-f9bf92ed2446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data for Performance Benchmarking\n",
    "emp_partitioned_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a69bdf-d600-4176-bc40-091172c368c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> There will always be Performance benefit during Shuffle when we read a partitioned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7267fa30-f5a0-42cc-83de-7935edb981c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMORTANT NOTE for OPTIMIZATION**\n",
    "- **Good Shuffle**: Avoid unnecessary Shuffles wherever possible because Shuffle is a costly operation. Do not go for Shuffle operations without any need.\n",
    "- **Repartitioning**: Ensure your data is properly partitioned as per the requirement\n",
    "- **Filter**: Apply filters on your data as early as possible. Filter your data before doing aggregation or shuffle operations. It will avoid shuffling of unnecessary data which in turn will reduce the shuffling amount.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "15_optimizing_shuffles",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
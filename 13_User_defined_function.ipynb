{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0705-151100-j50rz52y/driver-4398755585524187558\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0705-151100-j50rz52y/driver-4398755585524187558\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"UserDefinedFunction\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1924b39f-68b9-4898-85de-130d82914758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to a Spark standalone master.\n"
     ]
    }
   ],
   "source": [
    "master_url = spark.sparkContext.master\n",
    "if master_url.startswith(\"spark://\"):\n",
    "    print(\"Spark Master URL:\", master_url)\n",
    "else:\n",
    "    print(\"Not connected to a Spark standalone master.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c40c7b-dc89-4a7c-9f43-8aafc390e986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 7"
     ]
    }
   ],
   "source": [
    "# Read Employee data\n",
    "schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\"\n",
    "emp = spark.read.option(\"header\",True).schema(schema).csv(\"/data/output/1/emp.csv\")\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812f9033-776e-4c29-bd2c-c560f9c6eb15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate 10% of the Salary as Bonus\n",
    "def bonus(salary):\n",
    "    return int(salary)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e59818b8-4ad6-405b-b2d4-95334c603c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reguster an UDF\n",
    "from pyspark.sql.functions import udf\n",
    "bonus_udf=udf(bonus) # This is only available for the dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab19a2ee-653c-438b-9c3f-13598716a567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+------+\n|employee_id|department_id|       name|age|gender|salary| hire_date| bonus|\n+-----------+-------------+-----------+---+------+------+----------+------+\n|        019|          103|Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n|        008|          102|   Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n|        009|          103|    Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n|        006|          103|  Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n+-----------+-------------+-----------+---+------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create new column as bonus UDF\n",
    "from pyspark.sql.functions import expr\n",
    "emp.withColumn(\"bonus\", bonus_udf(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "457d7691-6b85-40b3-875c-b18617a22d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- What if we need to use UDF with Spark SQL or Spark SQL expressions? \n",
    "- For that, we need to register it in Scala as shown below.\n",
    "- Once that UDF is registered it is available to be used in any language. You can write it in expressions to get the work done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca07e345-c744-44b1-97f5-afbc46106c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: <function __main__.bonus(salary)>"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"bonus_sql_udf\", bonus, \"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7edfa61c-82c6-40fa-b72a-38e8ed92a0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+------+\n|employee_id|department_id|       name|age|gender|salary| hire_date| bonus|\n+-----------+-------------+-----------+---+------+------+----------+------+\n|        019|          103|Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n|        008|          102|   Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n|        009|          103|    Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n|        006|          103|  Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n+-----------+-------------+-----------+---+------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create new column as bonus UDF\n",
    "from pyspark.sql.functions import expr\n",
    "emp.withColumn(\"bonus\", expr(\"bonus_sql_udf(salary)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10ba433b-81b4-4e50-ad56-33d6af6865d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> NOTE: It is only recommended to use python UDF only when it is utmost necessary, since it is an expensive process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da6536f-6ae8-4219-baba-1ad32cce3b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+----------+------+\n|employee_id|department_id|       name|age|gender|salary| hire_date| bonus|\n+-----------+-------------+-----------+---+------+------+----------+------+\n|        019|          103|Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n|        008|          102|   Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n|        009|          103|    Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n|        006|          103|  Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n+-----------+-------------+-----------+---+------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create new column as bonus without UDF\n",
    "emp.withColumn(\"bonus\", expr(\"salary * 0.1 \")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "543a53bb-c0df-4253-90e7-970914fa22cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- You will see same result above without UDF. \n",
    "- But for this you will not have python process created.\n",
    "- It is happening within the JVM itself which is the executor."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13_User_defined_function",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
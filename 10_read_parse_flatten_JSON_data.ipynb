{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378b0791-5969-4b4e-a779-110ebd9fac6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Reading And Parsing JSON files/data\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c49b857-0814-40d1-8d07-6f2b0c0ad06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0629-111837-gf06xvg7/driver-3963052025575130306\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0629-111837-gf06xvg7/driver-3963052025575130306\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a243f8bc-32d6-4da8-b023-74b4144dffdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Single-line JSON (compact)\n",
    "s1 = '{\"order\":{\"id\": \"ORD1001\", \"details\": {\"customer\": {\"id\": \"CUST123\", \"name\": \"John Doe\"}, \"items\": {\"product\": {\"item_id\": \"ITM001\", \"name\": \"Product 1\"}, \"quantity\": 2}}, \"contact\": {\"email\": \"john.doe@example.com\", \"phone\": \"5551234567\"}}}}'\n",
    "\n",
    "# Multi-line JSON (pretty printed)\n",
    "m1 = '''\n",
    "{\n",
    "  \"order\": {\n",
    "    \"id\": \"ORD1002\",\n",
    "    \"details\": {\n",
    "      \"customer\": {\n",
    "        \"id\": \"CUST456\",\n",
    "        \"name\": \"Jane Smith\"\n",
    "      },\n",
    "      \"items\": {\n",
    "        \"product\": {\n",
    "          \"item_id\": \"ITM003\",\n",
    "          \"name\": \"Product 3\"\n",
    "        },\n",
    "        \"quantity\": 5\n",
    "      }\n",
    "    },\n",
    "    \"contact\": {\n",
    "      \"email\": \"jane.smith@example.com\",\n",
    "      \"phone\": \"5559876543\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13f547a-5f05-4852-b2d4-e96dda2205b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create RDDs from these JSON strings\n",
    "rdd_s1 = spark.sparkContext.parallelize([s1])\n",
    "rdd_m1 = spark.sparkContext.parallelize([m1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37ca30c-a4cc-434f-9f14-e7f9fec421e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse JSON into separate DataFrames\n",
    "df_s1 = spark.read.json(rdd_s1)\n",
    "df_m1 = spark.read.json(rdd_m1)\n",
    "\n",
    "# for file\n",
    "# spark.read.option(\"multiline\", True).json(\"file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31032926-8ae1-4395-b554-dc5315cc7283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+\n|order                                                                                         |\n+----------------------------------------------------------------------------------------------+\n|{{john.doe@example.com, 5551234567}, {{CUST123, John Doe}, {{ITM001, Product 1}, 2}}, ORD1001}|\n+----------------------------------------------------------------------------------------------+\n\n+--------------------------------------------------------------------------------------------------+\n|order                                                                                             |\n+--------------------------------------------------------------------------------------------------+\n|{{jane.smith@example.com, 5559876543}, {{CUST456, Jane Smith}, {{ITM003, Product 3}, 5}}, ORD1002}|\n+--------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_s1.show(truncate=False)\n",
    "df_m1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1038c77c-2448-4101-9b7a-1a37d4eb3aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- When you read JSON directly with spark.read.json(), Spark infers the schema based on the JSON structure, but sometimes nested fields are kept as nested structures, resulting in a single column with a complex type.\n",
    "- To get multiple top-level columns like contact, customer_id, order_id, order_line_items, you need to flatten the nested JSON structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67cd387b-97c9-44c5-a5b5-f291488f517f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+----------+------------+--------+----------------------+-------------+\n|order_id|customer_id|customer_name|product_id|product_name|quantity|contact_email         |contact_phone|\n+--------+-----------+-------------+----------+------------+--------+----------------------+-------------+\n|ORD1002 |CUST456    |Jane Smith   |ITM003    |Product 3   |5       |jane.smith@example.com|5559876543   |\n+--------+-----------+-------------+----------+------------+--------+----------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# For df_m1\n",
    "df_flat_m1 = df_m1.select(\n",
    "    col(\"order.id\").alias(\"order_id\"),\n",
    "    col(\"order.details.customer.id\").alias(\"customer_id\"),\n",
    "    col(\"order.details.customer.name\").alias(\"customer_name\"),\n",
    "    col(\"order.details.items.product.item_id\").alias(\"product_id\"),\n",
    "    col(\"order.details.items.product.name\").alias(\"product_name\"),\n",
    "    col(\"order.details.items.quantity\").alias(\"quantity\"),\n",
    "    col(\"order.contact.email\").alias(\"contact_email\"),\n",
    "    col(\"order.contact.phone\").alias(\"contact_phone\")\n",
    ")\n",
    "df_flat_m1.show(truncate=False)\n",
    "\n",
    "# The same will wokf for single line if the schema and nested levels are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abbe34de-0692-4919-bdf2-91c5b3b2fe33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With Schema (Enforcing a schema and returning only required columns)\n",
    "_schema = \"order_id string, customer_id string, contact_phone string\"\n",
    "df_schema = spark.read.schema(_schema).json(\"Json file path\")  --this cannot be done on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79dc2c81-fa3d-4f35-8860-b4ea5076223c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- order: struct (nullable = true)\n |    |-- contact: struct (nullable = true)\n |    |    |-- email: string (nullable = true)\n |    |    |-- phone: string (nullable = true)\n |    |-- details: struct (nullable = true)\n |    |    |-- customer: struct (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |-- items: struct (nullable = true)\n |    |    |    |-- product: struct (nullable = true)\n |    |    |    |    |-- item_id: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- quantity: long (nullable = true)\n |    |-- id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_m1.printSchema()\n",
    "\n",
    "# _schema = \"order struct<contact struct<email string, phone string>, details struct<customer struct<id string, name string>, items struct<product struct<item_id string, name string>, quantity long>>>, id string>\"  # for schema enforcing/changing on a json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1191a1d6-0fd8-4e70-b74e-6a09cdfd38a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "IMPORTANT NOTE \n",
    "- Use 'from_json' and 'to_json' and 'explode' from pyspark.sql.functions \n",
    "- from_json: Parses a JSON string column into a StructType (i.e., a structured Spark DataFrame column). It's useful when you have JSON data stored as strings and want to extract its fields into separate columns\n",
    "- --> String JSON → StructType\n",
    "- to_json: Converts a complex column (like a StructType or ArrayType) into a JSON string. This is useful for serializing structured data back into JSON format.\n",
    "- --> StructType → JSON string\n",
    "- Use 'explode' to flatten/Expand the JSon files\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_read_parse_flatten_JSON_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
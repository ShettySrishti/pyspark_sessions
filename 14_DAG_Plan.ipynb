{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0705-151100-j50rz52y/driver-4398755585524187558\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0705-151100-j50rz52y/driver-4398755585524187558\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"DAG plan understanding\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380e3cab-eb25-4637-9766-e738809d369a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable Adaptive Query Engine(AQE) and Broadcast Join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e49c68-2419-457f-8226-09dffb13d6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 8"
     ]
    }
   ],
   "source": [
    "# Check the default Parallelism\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adfa56b-2b5c-4a86-9cef-2fd8708ebd51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: 8"
     ]
    }
   ],
   "source": [
    "# Create dataframes\n",
    "df1 = spark.range(4, 200, 2)\n",
    "df1.rdd.getNumPartitions() \n",
    "df2 = spark.range(2, 200, 4)\n",
    "df2.rdd.getNumPartitions()\n",
    "\n",
    "# Default partition is '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f1dc6d-5205-4ed5-bc0b-19715d40018b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-partition data\n",
    "df3 = df1.repartition(5)\n",
    "df4 = df2.repartition(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a81a78-f21a-4134-a57f-6ed53946a53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 5"
     ]
    }
   ],
   "source": [
    "df3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7ae4df-d90f-462a-b6d9-33178bfc2154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: 7"
     ]
    }
   ],
   "source": [
    "df4.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebaab1cc-e9dd-4eac-be4a-ac4cfd471453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join the dataframes\n",
    "df_joined = df3.join(df4, on='id')\n",
    "\n",
    "# Here by default 200 shuffle partitions are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03bad5f-d785-4a63-9f23-f18e4cd1d648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|total_sum|\n+---------+\n|     4998|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Get the sum of id's\n",
    "from pyspark.sql.functions import expr\n",
    "df_sum = df_joined.selectExpr(\"sum(id) AS total_sum\")\n",
    "df_sum.show()\n",
    "\n",
    "# Here those 200 partitions are being pushed into 1 partition by default in order to do the SUM (last stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7896fe09-87be-4828-9474-e3f27682578f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(4) HashAggregate(keys=[], functions=[finalmerge_sum(merge sum#424L) AS sum(id#408L)#418L])\n+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=586]\n   +- *(3) HashAggregate(keys=[], functions=[partial_sum(id#408L) AS sum#424L])\n      +- *(3) Project [id#408L]\n         +- *(3) BroadcastHashJoin [id#408L], [id#410L], Inner, BuildRight, false, false\n            :- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=576]\n            :  +- *(1) Range (4, 200, step=2, splits=8)\n            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=580]\n               +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=579]\n                  +- *(2) Range (2, 200, step=4, splits=8)\n\n\n"
     ]
    }
   ],
   "source": [
    "# Explain plan\n",
    "df_sum.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec3fbb0-39c9-4fc7-8ac2-fb0ba6373017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UNION the data again to see SKipped Stages\n",
    "df_union = df_sum.union(df4)\n",
    "# df_union.show() # After doing show() the skipped stages will be displayed in the Spark UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ef1c9e-051a-4b3e-8d41-276062e5fbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In the above command, the Spark will skip the stages for 'df_sum' since it was already processed before.\n",
    "- It will just create stages to perform a UNION here.\n",
    "- Check 'Spark UI' for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47011796-094f-44d9-b978-38e522b9c079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nUnion\n:- *(4) HashAggregate(keys=[], functions=[finalmerge_sum(merge sum#424L) AS sum(id#408L)#418L])\n:  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=780]\n:     +- *(3) HashAggregate(keys=[], functions=[partial_sum(id#408L) AS sum#424L])\n:        +- *(3) Project [id#408L]\n:           +- *(3) BroadcastHashJoin [id#408L], [id#410L], Inner, BuildRight, false, false\n:              :- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=770]\n:              :  +- *(1) Range (4, 200, step=2, splits=8)\n:              +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=774]\n:                 +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=773]\n:                    +- *(2) Range (2, 200, step=4, splits=8)\n+- ReusedExchange [id#486L], Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=773]\n\n\n"
     ]
    }
   ],
   "source": [
    "df_union.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d7715e-919d-4ff5-9cfa-ed2c67501593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: MapPartitionsRDD[193] at javaToPython at <unknown>:0"
     ]
    }
   ],
   "source": [
    "# Dataframe to RDD\n",
    "df1.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c92dc9c-4452-44df-862a-e29afe7d51f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- > NOTE: RDD's are recommended to use ONLY when we require the data to be distributed physically with the help of the code or we have to work extensively with Spark Core API's. \n",
    "- In other cases, we ALWAYS go with Dataframes"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "14_DAG_Plan",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
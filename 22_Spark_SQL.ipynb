{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0719-080645-69bb8cez/driver-5476063862036625659\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0719-080645-69bb8cez/driver-5476063862036625659\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Spark SQL\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec5a00df-d890-454b-aa7b-e409db61e5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What is Catalog? Catalog stores metadata and metadata (Columns, datatype, comments and other metadata related to the tables/views created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba88c0c-90ef-490c-8ccf-f9791ba4b546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-544566587118084>:5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Spark Catalog (Metadata) - in-memory/hive\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# spark.conf.get(\"spark.sql.catalogImplementation\") # hive is the default here\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# for in-memory we have to mention it explicity like below:\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.catalogImplementation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min-memory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.catalogImplementation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/conf.py:40\u001B[0m, in \u001B[0;36mRuntimeConfig.set\u001B[0;34m(self, key, value)\u001B[0m\n",
       "\u001B[1;32m     37\u001B[0m \u001B[38;5;129m@since\u001B[39m(\u001B[38;5;241m2.0\u001B[39m)\n",
       "\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mset\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mstr\u001B[39m, value: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     39\u001B[0m     \u001B[38;5;124;03m\"\"\"Sets the given Spark runtime configuration property.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot modify the value of a static config: spark.sql.catalogImplementation."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-544566587118084>:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Spark Catalog (Metadata) - in-memory/hive\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# spark.conf.get(\"spark.sql.catalogImplementation\") # hive is the default here\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# for in-memory we have to mention it explicity like below:\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.catalogImplementation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min-memory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.catalogImplementation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/conf.py:40\u001B[0m, in \u001B[0;36mRuntimeConfig.set\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;129m@since\u001B[39m(\u001B[38;5;241m2.0\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mset\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mstr\u001B[39m, value: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;124;03m\"\"\"Sets the given Spark runtime configuration property.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot modify the value of a static config: spark.sql.catalogImplementation.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot modify the value of a static config: spark.sql.catalogImplementation.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Catalog (Metadata) - in-memory/hive\n",
    "spark.conf.get(\"spark.sql.catalogImplementation\") # hive is the default here\n",
    "\n",
    "# for in-memory we have to mention it explicity like below:\n",
    "spark.conf.set(\"spark.sql.catalogImplementation\",\"in-memory\")\n",
    "spark.conf.get(\"spark.sql.catalogImplementation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c2824e-f7bf-426b-ad6c-83e48a77eb30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**You see the above error because:**\n",
    "- Spark configurations, especially those related to catalog implementation, are immutable after SparkSession has been initialized. \n",
    "- spark.sql.catalogImplementation is a static configuration, meaning it must be set before SparkSession is created.\n",
    "- Trying to change it after the SparkSession is initialized (via spark.conf.set()) will have no effect or may throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968f1106-d357-4197-9ac8-1df162d99782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 'hive'"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1555363-b095-4a0f-a77e-6405f9b964ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> IMPORTANT:\n",
    "- Using '*in-memory catalog implementation*' is not safe because if the Spark Session is lost or restarted, all the previous tables (views will be lost in any session because it is temporary) created will be lost!\n",
    "- To avoid this, we should use '*hive catalog implementation*'\n",
    "- Use '*enableHiveSupport()*' while creating Spark session itself, if it is not enabled by default \n",
    "- If a hive catalog is implemented, the metastore/datastore is persisted in a particular location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9104d92-4fb5-4802-b2d1-ba16168b6200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|databaseName|\n+------------+\n|     default|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Show databases\n",
    "db = spark.sql(\"SHOW databases\")\n",
    "db.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbfac63-b98a-4cdd-8725-c15c48c2cac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3ecdac-5721-47a0-b53a-d85f6d1232d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10 million records\n",
    "emp_schema = \"first_name string, last_name string, job_title string, dob date, email string, phone string, salary double, department string, department_id integer\"\n",
    "emp = spark.read.schema(emp_schema).option(\"header\",True).csv(\"/data/input/datasets/employee_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf48e4b6-3ead-4c68-b982-8a6bf6d13b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV file with 10 records\n",
    "dept_schema =\"department_id int, department_name string, description string, city string, state string, country string \"\n",
    "dept = spark.read.schema(dept_schema).option(\"header\",True).csv(\"/data/input/datasets/department_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c349dc-af80-4147-bca1-8fb0f389576d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register dataframes are temp views\n",
    "emp.createOrReplaceTempView(\"empview\")\n",
    "dept.createOrReplaceTempView(\"deptview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25dc96eb-297a-4b3d-86c7-9513f038bcdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n|        | deptview|       true|\n|        |  empview|       true|\n+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Show tables in default\").show()  # Now you will see two views here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ce30d7-e950-4e8b-a773-dc73889a13f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> NOTE:\n",
    "- '*isTemporary= true'* --> means it is a view not a table\n",
    "- That also implies that once that particular active session is lost those views will also be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c851fce6-7b00-4020-9465-9463558535f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+-----------------------+------------+---------+----------+-------------+\n|first_name   |last_name   |job_title     |dob       |email                  |phone       |salary   |department|department_id|\n+-------------+------------+--------------+----------+-----------------------+------------+---------+----------+-------------+\n|FirstName_282|LastName_282|Data Analyst  |1996-09-02|user1249282@example.com|+18214812697|71801.39 |Data      |1            |\n|FirstName_288|LastName_288|Senior Manager|1985-05-26|user1249288@example.com|+14644924775|86367.71 |Data      |1            |\n|FirstName_306|LastName_306|Data Analyst  |1988-03-14|user1249306@example.com|+12338438014|95641.32 |Data      |1            |\n|FirstName_346|LastName_346|HR Manager    |1963-11-03|user1249346@example.com|+14531761440|110552.52|Data      |1            |\n|FirstName_375|LastName_375|HR Specialist |1976-09-03|user1249375@example.com|+14896803254|48903.01 |Data      |1            |\n+-------------+------------+--------------+----------+-----------------------+------------+---------+----------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# View data from Table -- triple quotes for multi-line\n",
    "\n",
    "empdf= spark.sql(\"\"\"\n",
    "          SELECT * \n",
    "          FROM empview\n",
    "          WHERE department_id=1\n",
    "\"\"\")\n",
    "\n",
    "empdf.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d42e825-035f-4a60-8826-76552c3afe7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+\n|   first_name|   last_name|     job_title|       dob|               email|       phone|   salary|department|department_id|dob_year|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+\n|FirstName_282|LastName_282|  Data Analyst|1996-09-02|user1249282@examp...|+18214812697| 71801.39|      Data|            1|    1996|\n|FirstName_288|LastName_288|Senior Manager|1985-05-26|user1249288@examp...|+14644924775| 86367.71|      Data|            1|    1985|\n|FirstName_306|LastName_306|  Data Analyst|1988-03-14|user1249306@examp...|+12338438014| 95641.32|      Data|            1|    1988|\n|FirstName_346|LastName_346|    HR Manager|1963-11-03|user1249346@examp...|+14531761440|110552.52|      Data|            1|    1963|\n|FirstName_375|LastName_375| HR Specialist|1976-09-03|user1249375@examp...|+14896803254| 48903.01|      Data|            1|    1976|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Create a new column dob_year and register s temp view\n",
    "emptemp = spark.sql(\"\"\"\n",
    "          SELECT e.*, date_format(dob, 'yyyy') AS dob_year\n",
    "          FROM empview e\n",
    "          WHERE department_id=1\n",
    "\"\"\")\n",
    "emptemp.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62344929-3bc8-4ba7-b6f4-42bae740570e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n|database|  tableName|isTemporary|\n+--------+-----------+-----------+\n|        |   deptview|       true|\n|        |emptempview|       true|\n|        |    empview|       true|\n+--------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emptemp.createOrReplaceTempView(\"emptempview\")\n",
    "spark.sql(\"Show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e2f79f-ed88-456b-ae43-9f54b5b8d36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+\n|   first_name|   last_name|     job_title|       dob|               email|       phone|   salary|department|department_id|dob_year|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+\n|FirstName_282|LastName_282|  Data Analyst|1996-09-02|user1249282@examp...|+18214812697| 71801.39|      Data|            1|    1996|\n|FirstName_288|LastName_288|Senior Manager|1985-05-26|user1249288@examp...|+14644924775| 86367.71|      Data|            1|    1985|\n|FirstName_306|LastName_306|  Data Analyst|1988-03-14|user1249306@examp...|+12338438014| 95641.32|      Data|            1|    1988|\n|FirstName_346|LastName_346|    HR Manager|1963-11-03|user1249346@examp...|+14531761440|110552.52|      Data|            1|    1963|\n|FirstName_375|LastName_375| HR Specialist|1976-09-03|user1249375@examp...|+14896803254| 48903.01|      Data|            1|    1976|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM emptempview\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce224ef-5682-4977-96c9-6fce633fd72e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|   first_name|   last_name|     job_title|       dob|               email|       phone|   salary|department|department_id|dob_year|department_name|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|FirstName_282|LastName_282|  Data Analyst|1996-09-02|user1249282@examp...|+18214812697| 71801.39|      Data|            1|    1996|           Data|\n|FirstName_288|LastName_288|Senior Manager|1985-05-26|user1249288@examp...|+14644924775| 86367.71|      Data|            1|    1985|           Data|\n|FirstName_306|LastName_306|  Data Analyst|1988-03-14|user1249306@examp...|+12338438014| 95641.32|      Data|            1|    1988|           Data|\n|FirstName_346|LastName_346|    HR Manager|1963-11-03|user1249346@examp...|+14531761440|110552.52|      Data|            1|    1963|           Data|\n|FirstName_375|LastName_375| HR Specialist|1976-09-03|user1249375@examp...|+14896803254| 48903.01|      Data|            1|    1976|           Data|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# JOIN emp and dept - HINTS\n",
    "dfjoined = spark.sql(\"\"\"\n",
    "          SELECT e.*,d.department_name\n",
    "          FROM emptempview e LEFT OUTER JOIN deptview d\n",
    "          ON e.department_id=d.department_id\"\"\")\n",
    "dfjoined.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b97735b-22a2-4e78-b597-0066faa8397b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you see below, by default it is using Broadcasting(BroadcastHashJoin) on department view(smaller view), because the AQE is taking care of the Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2b07be-ff2f-4862-9ac3-cc917a0bec48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [first_name#104, last_name#105, job_title#106, dob#107, email#108, phone#109, salary#110, department#111, department_id#112, dob_year#833, department_name#123]\n   +- BroadcastHashJoin [department_id#112], [department_id#122], LeftOuter, BuildRight, false, true\n      :- Project [first_name#104, last_name#105, job_title#106, dob#107, email#108, phone#109, salary#110, department#111, department_id#112, date_format(cast(dob#107 as timestamp), yyyy, Some(Etc/UTC)) AS dob_year#833]\n      :  +- Filter (isnotnull(department_id#112) AND (department_id#112 = 1))\n      :     +- FileScan csv [first_name#104,last_name#105,job_title#106,dob#107,email#108,phone#109,salary#110,department#111,department_id#112] Batched: false, DataFilters: [isnotnull(department_id#112), (department_id#112 = 1)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id), EqualTo(department_id,1)], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=1152]\n         +- Filter (isnotnull(department_id#122) AND (department_id#122 = 1))\n            +- FileScan csv [department_id#122,department_name#123] Batched: false, DataFilters: [isnotnull(department_id#122), (department_id#122 = 1)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id), EqualTo(department_id,1)], ReadSchema: struct<department_id:int,department_name:string>\n\n\n"
     ]
    }
   ],
   "source": [
    "dfjoined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb774b6-60a8-4eaf-a434-7c2d852e6ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> Now consider, if you dont want to use a BROADCAST JOIN but you want to use a SHUFFLE JOIN, then we need to put a HINT! See it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75e4688-44e6-453f-9446-2db1e75a4dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|   first_name|   last_name|     job_title|       dob|               email|       phone|   salary|department|department_id|dob_year|department_name|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|FirstName_282|LastName_282|  Data Analyst|1996-09-02|user1249282@examp...|+18214812697| 71801.39|      Data|            1|    1996|           Data|\n|FirstName_288|LastName_288|Senior Manager|1985-05-26|user1249288@examp...|+14644924775| 86367.71|      Data|            1|    1985|           Data|\n|FirstName_306|LastName_306|  Data Analyst|1988-03-14|user1249306@examp...|+12338438014| 95641.32|      Data|            1|    1988|           Data|\n|FirstName_346|LastName_346|    HR Manager|1963-11-03|user1249346@examp...|+14531761440|110552.52|      Data|            1|    1963|           Data|\n|FirstName_375|LastName_375| HR Specialist|1976-09-03|user1249375@examp...|+14896803254| 48903.01|      Data|            1|    1976|           Data|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "dfjoined = spark.sql(\"\"\"\n",
    "          SELECT /*+ SHUFFLE_MERGE(e) */    -- HINT to use SortMergeJoin(Shuffle Join)\n",
    "          e.*,d.department_name\n",
    "          FROM emptempview e LEFT OUTER JOIN deptview d\n",
    "          ON e.department_id=d.department_id\"\"\")\n",
    "dfjoined.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dee35c-05a4-460a-b7e2-38a4254eb131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|   first_name|   last_name|     job_title|       dob|               email|       phone|   salary|department|department_id|dob_year|department_name|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|FirstName_282|LastName_282|  Data Analyst|1996-09-02|user1249282@examp...|+18214812697| 71801.39|      Data|            1|    1996|           Data|\n|FirstName_288|LastName_288|Senior Manager|1985-05-26|user1249288@examp...|+14644924775| 86367.71|      Data|            1|    1985|           Data|\n|FirstName_306|LastName_306|  Data Analyst|1988-03-14|user1249306@examp...|+12338438014| 95641.32|      Data|            1|    1988|           Data|\n|FirstName_346|LastName_346|    HR Manager|1963-11-03|user1249346@examp...|+14531761440|110552.52|      Data|            1|    1963|           Data|\n|FirstName_375|LastName_375| HR Specialist|1976-09-03|user1249375@examp...|+14896803254| 48903.01|      Data|            1|    1976|           Data|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "dfjoined = spark.sql(\"\"\"\n",
    "          SELECT /*+ BROADCAST(d) */    -- HINT to use Broadcast, it will use BroadcastHashJoin now\n",
    "          e.*,d.department_name\n",
    "          FROM emptempview e LEFT OUTER JOIN deptview d\n",
    "          ON e.department_id=d.department_id\"\"\")\n",
    "dfjoined.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a6362d6-4673-4620-a223-05cfb2291a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the Data as table\n",
    "\n",
    "dfjoined.write.format(\"parquet\").saveAsTable(\"empfinaltable\")\n",
    "\n",
    "# Check Database Tables --> default --> empfinaltable created there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d798a7b5-6274-46ae-abc3-8ea004a1c4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n|database|    tableName|isTemporary|\n+--------+-------------+-----------+\n| default|empfinaltable|      false|\n|        |     deptview|       true|\n|        |  emptempview|       true|\n|        |      empview|       true|\n+--------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Show tables in default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a47ff012-4758-418c-8ee0-36a2188cfe79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By default, for the above table (empfinaltable) created:\n",
    "- Spark saves tables in Hive warehouse directory, typically at: /user/hive/warehouse/empfinaltable/ (You will find this folder there)\n",
    "- You can also see that '*isTemporary=false*', which means it is a Tabl now and not a View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826f51d3-5a5d-416c-968d-ac6171425fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|   first_name|   last_name|     job_title|       dob|               email|       phone|   salary|department|department_id|dob_year|department_name|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|FirstName_778|LastName_778|Senior Manager|1971-12-04|user7499778@examp...|+13461914433|137993.09|      Data|            1|    1971|           Data|\n|FirstName_783|LastName_783|     Team Lead|2013-03-01|user7499783@examp...|+14660611852| 89633.42|      Data|            1|    2013|           Data|\n|FirstName_787|LastName_787| HR Specialist|1974-01-16|user7499787@examp...|+14538342783| 90809.83|      Data|            1|    1974|           Data|\n|FirstName_790|LastName_790|  Data Analyst|1993-02-26|user7499790@examp...|+13024777044| 62901.98|      Data|            1|    1993|           Data|\n|FirstName_801|LastName_801| HR Specialist|1989-07-07|user7499801@examp...|+14509905365|104401.21|      Data|            1|    1989|           Data|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# READ the data from the table\n",
    "empnew = spark.read.table(\"empfinaltable\")\n",
    "empnew.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1dadc36-acc4-4787-8b64-806bec6f9a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20487125-a436-4829-9376-79d71bdbd121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|   first_name|   last_name|     job_title|       dob|               email|       phone|   salary|department|department_id|dob_year|department_name|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\n|FirstName_778|LastName_778|Senior Manager|1971-12-04|user7499778@examp...|+13461914433|137993.09|      Data|            1|    1971|           Data|\n|FirstName_783|LastName_783|     Team Lead|2013-03-01|user7499783@examp...|+14660611852| 89633.42|      Data|            1|    2013|           Data|\n|FirstName_787|LastName_787| HR Specialist|1974-01-16|user7499787@examp...|+14538342783| 90809.83|      Data|            1|    1974|           Data|\n|FirstName_790|LastName_790|  Data Analyst|1993-02-26|user7499790@examp...|+13024777044| 62901.98|      Data|            1|    1993|           Data|\n|FirstName_801|LastName_801| HR Specialist|1989-07-07|user7499801@examp...|+14509905365|104401.21|      Data|            1|    1989|           Data|\n+-------------+------------+--------------+----------+--------------------+------------+---------+----------+-------------+--------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "empnew = spark.sql(\"SELECT * FROM empfinaltable\")\n",
    "empnew.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960a20ac-a4ef-45cb-9549-424d7a2ec1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|     first_name|   string|   null|\n|      last_name|   string|   null|\n|      job_title|   string|   null|\n|            dob|     date|   null|\n|          email|   string|   null|\n|          phone|   string|   null|\n|         salary|   double|   null|\n|     department|   string|   null|\n|  department_id|      int|   null|\n|       dob_year|   string|   null|\n|department_name|   string|   null|\n+---------------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# SHOW details of Metadata\n",
    "spark.sql(\"DESCRIBE empfinaltable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49479cb-57fa-4326-99c4-dffa0cb2e520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------+-------+\n|col_name                    |data_type                                                |comment|\n+----------------------------+---------------------------------------------------------+-------+\n|first_name                  |string                                                   |null   |\n|last_name                   |string                                                   |null   |\n|job_title                   |string                                                   |null   |\n|dob                         |date                                                     |null   |\n|email                       |string                                                   |null   |\n|phone                       |string                                                   |null   |\n|salary                      |double                                                   |null   |\n|department                  |string                                                   |null   |\n|department_id               |int                                                      |null   |\n|dob_year                    |string                                                   |null   |\n|department_name             |string                                                   |null   |\n|                            |                                                         |       |\n|# Detailed Table Information|                                                         |       |\n|Catalog                     |spark_catalog                                            |       |\n|Database                    |default                                                  |       |\n|Table                       |empfinaltable                                            |       |\n|Owner                       |root                                                     |       |\n|Created Time                |Sat Jul 19 09:17:19 UTC 2025                             |       |\n|Last Access                 |UNKNOWN                                                  |       |\n|Created By                  |Spark 3.3.2                                              |       |\n|Type                        |MANAGED                                                  |       |\n|Provider                    |parquet                                                  |       |\n|Statistics                  |28871809 bytes                                           |       |\n|Location                    |dbfs:/user/hive/warehouse/empfinaltable                  |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       |       |\n|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat         |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat|       |\n+----------------------------+---------------------------------------------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# See all metadats information using 'extended'\n",
    "spark.sql(\"DESCRIBE extended empfinaltable\").show(truncate=False, n=50)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "22_Spark_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0706-083214-7l388b8c/driver-535718982730242282\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0706-083214-7l388b8c/driver-535718982730242282\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Distributed Shared Variables\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.executor.cores\",4)\n",
    "        .config(\"spark.cores.max\",16)\n",
    "        .config(\"spark.executor.memory\", \"512M\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3ecdac-5721-47a0-b53a-d85f6d1232d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10 million records\n",
    "_schema = \"first_name string, last_name string, job_title string, dob date, email string, phone string, salary double, department string, department_id integer\"\n",
    "emp = spark.read.schema(_schema).option(\"header\",True).csv(\"/data/input/datasets/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff7f1003-7ab3-4a38-ae01-2aecfa3b9ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Broadcast Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62412e9f-3d0f-41a4-9554-b3b889b5f5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: pyspark.broadcast.Broadcast"
     ]
    }
   ],
   "source": [
    "# Variable (lookup)\n",
    "dept_names = {1: 'Department 1',\n",
    "              2: 'Department 2',\n",
    "              3: 'Department 3',\n",
    "              4: 'Department 4',\n",
    "              5: 'Department 5'\n",
    "}\n",
    "# Broadcast the variable\n",
    "broadcast_dept_names = spark.sparkContext.broadcast(dept_names)\n",
    "# Check the type of the variable\n",
    "type(broadcast_dept_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16df9c6c-50a3-4c41-8363-755c2b185223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: {1: 'Department 1',\n 2: 'Department 2',\n 3: 'Department 3',\n 4: 'Department 4',\n 5: 'Department 5'}"
     ]
    }
   ],
   "source": [
    "# Check the value of the variable\n",
    "broadcast_dept_names.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ace41d8-d4d3-4767-b4d1-09d717b988c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The below action (.show()) will be Processed in a Single Stage (No Shuffle involved) because we have distributed broadcast variable in each of the executors.\n",
    "- It is being used below with the help of an UDF. We are not using any operation that is leading to Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e811d887-ffc1-4d5e-b3d3-3046aaf413ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+------------+\n|first_name|last_name|job_title        |dob       |email                         |phone          |salary            |department        |department_id|dept_name   |\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+------------+\n|Jennifer  |Williams |HR Specialist    |1951-01-21|Jennifer.Williams.@example.com|+1-845-311-804 |42951.90537045701 |Finance           |6            |null        |\n|James     |Miller   |Sales Executive  |1939-09-25|James.Miller.@example.com     |+1-274-633-7306|50933.8591162336  |Data and Analytics|6            |null        |\n|Linda     |Jones    |Data Scientist   |2023-05-26|Linda.Jones.@example.com      |+1-149-733-8924|66274.49226944339 |Data and Analytics|2            |Department 2|\n|Srishti   |Smith    |Data Engineer    |2003-01-16|Srishti.Smith.@example.com    |+1-790-373-5222|43705.485219830625|Engineering       |4            |Department 4|\n|Michael   |Brown    |Software Engineer|1973-08-28|Michael.Brown.@example.com    |+1-223-271-7921|97427.30108330262 |Data and Analytics|6            |null        |\n+----------+---------+-----------------+----------+------------------------------+---------------+------------------+------------------+-------------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Create UDF to return Department Name\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "@udf\n",
    "def getDeptNames(dept_id):\n",
    "    return broadcast_dept_names.value.get(dept_id)\n",
    "\n",
    "emp_final = emp.withColumn(\"dept_name\", getDeptNames(col(\"department_id\")) )\n",
    "emp_final.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "661cd45a-aa15-4ba1-a6dd-93572c486a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c1a0f2-9cb6-43e3-bc2e-675ff39cdb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n|department_id|         sum(salary)|\n+-------------+--------------------+\n|            6|1.248861142452698E11|\n+-------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate total salary of Department 5\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "emp.where(\"department_id=6\").groupby(\"department_id\").agg(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7f7dbb7-a3fa-4e88-ae8b-6e73a92eb046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The cell above has salary in exponential format, so we will cast it to long as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98dcc2ef-2f8f-4131-a326-6003084da6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n|CAST(sum(salary) AS BIGINT)|\n+---------------------------+\n|               124886114245|\n+---------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.where(\"department_id=6\").agg(sum(\"salary\").cast(\"long\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bdc695a-e779-43ff-9143-cb972aea1ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In the above action, Exchange and Shuffling happens.(Check '*Spark UI'* for more info)\n",
    "- So to eliminate Shuffling, we will now use another method: distributed variable *'Accumulators'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6bd1b3-2366-4ff7-bc10-315517fe75d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: 124886114245.26982"
     ]
    }
   ],
   "source": [
    "# Accumulators\n",
    "dept_sal = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# Use foreach\n",
    "def calculateSalary(department_id, salary):\n",
    "    if department_id == 6:\n",
    "        dept_sal.add(salary)\n",
    "\n",
    "emp.foreach(lambda row: calculateSalary(row.department_id, row.salary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c98319e-4d41-4074-92c3-c8ccac7de6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: 124886114245.26982"
     ]
    }
   ],
   "source": [
    "# View total value\n",
    "dept_sal.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5be48ecf-d294-41ba-a6c3-ccd9c18572cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Now, if you check Spark UI for the above accumulator task, it will be processed in a single stage (No Shuffling involved).\n",
    "- ALso, the total value is same as the one we ran without Accumulators (check cell 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01b930b3-339b-44f1-ae9f-c4f6e37e9b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> This was just a simple use case. You can use these two variables (Broadcast and Accumulators) for many other long list of use cases"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17_distributed_shared_variables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
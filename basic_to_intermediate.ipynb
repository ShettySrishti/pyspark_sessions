{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8bbefa2-28f4-4627-a1d4-55ab9040626f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---+------+------+----------+\n|employee_id|department_id|      name|age|gender|salary| hire_date|\n+-----------+-------------+----------+---+------+------+----------+\n|        002|          101|Jane Smith| 25|Female| 45000|2016-02-15|\n|        004|          102| Alice Lee| 28|Female| 48000|2017-09-30|\n|        003|          102| Bob Brown| 35|  Male| 55000|2014-05-01|\n|        005|          103| Jack Chan| 40|  Male| 60000|2013-04-01|\n|        001|          101|  John Doe| 30|  Male| 50000|2015-01-01|\n+-----------+-------------+----------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/data/input/employee.csv\", header= True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce59440-dbfe-4821-a063-6da556b6a76a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Selecting only some columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n|employee_id|      name|salary|\n+-----------+----------+------+\n|        002|Jane Smith| 45000|\n|        004| Alice Lee| 48000|\n|        003| Bob Brown| 55000|\n|        005| Jack Chan| 60000|\n|        001|  John Doe| 50000|\n+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1=df.select(df.employee_id, df.name, df.salary)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb2b26e-5d7a-4f21-a304-0f6041db3d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n|employee_id|      name|salary|\n+-----------+----------+------+\n|        001|  John Doe| 50000|\n|        002|Jane Smith| 45000|\n|        003| Bob Brown| 55000|\n|        004| Alice Lee| 48000|\n|        005| Jack Chan| 60000|\n+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1=df.select(\"employee_id\",\"name\",\"salary\").orderBy(df.employee_id)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee74ea1-58cc-409c-b112-0408384ef6b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---+------+------+----------+\n|employee_id|department_id|      name|age|gender|salary| hire_date|\n+-----------+-------------+----------+---+------+------+----------+\n|        002|          101|Jane Smith| 25|Female| 45000|2016-02-15|\n|        004|          102| Alice Lee| 28|Female| 48000|2017-09-30|\n|        003|          102| Bob Brown| 35|  Male| 55000|2014-05-01|\n|        005|          103| Jack Chan| 40|  Male| 60000|2013-04-01|\n|        001|          101|  John Doe| 30|  Male| 50000|2015-01-01|\n+-----------+-------------+----------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88408ff-5a97-421c-a69f-2fed0263727d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Selecting range of columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---+------+\n|department_id|      name|age|gender|\n+-------------+----------+---+------+\n|          101|Jane Smith| 25|Female|\n|          102| Alice Lee| 28|Female|\n|          102| Bob Brown| 35|  Male|\n|          103| Jack Chan| 40|  Male|\n|          101|  John Doe| 30|  Male|\n+-------------+----------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1=df.select(df.columns[1:5])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402b24d5-1488-46ce-8f59-4cf9f8e1a55d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add new col based on existing col"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---+------+------+----------+--------+\n|employee_id|department_id|      name|age|gender|salary| hire_date| salary1|\n+-----------+-------------+----------+---+------+------+----------+--------+\n|        002|          101|Jane Smith| 25|Female| 45000|2016-02-15|450000.0|\n|        004|          102| Alice Lee| 28|Female| 48000|2017-09-30|480000.0|\n|        003|          102| Bob Brown| 35|  Male| 55000|2014-05-01|550000.0|\n|        005|          103| Jack Chan| 40|  Male| 60000|2013-04-01|600000.0|\n|        001|          101|  John Doe| 30|  Male| 50000|2015-01-01|500000.0|\n+-----------+-------------+----------+---+------+------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sal1=df.withColumn(\"salary1\",df.salary*10)\n",
    "sal1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f8200a-d5ed-457b-bad6-d0edc35ee2d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add new col based on non existing col and constant value"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---+------+------+----------+\n|employee_id|department_id|      name|age|gender|salary| hire_date|\n+-----------+-------------+----------+---+------+------+----------+\n|        002|          101|Jane Smith| 25|Unisex| 45000|2016-02-15|\n|        004|          102| Alice Lee| 28|Unisex| 48000|2017-09-30|\n|        003|          102| Bob Brown| 35|Unisex| 55000|2014-05-01|\n|        005|          103| Jack Chan| 40|Unisex| 60000|2013-04-01|\n|        001|          101|  John Doe| 30|Unisex| 50000|2015-01-01|\n+-----------+-------------+----------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "gdf=df.withColumn(\"gender\",lit(\"Unisex\"))\n",
    "gdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67cb951-6952-47bb-831f-1dc1037d028d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update existing Schema"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: string (nullable = true)\n |-- department_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- hire_date: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c35f13-b40e-4a8b-994d-a2b1ea9e03a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Casting single and multiple columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df=df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))  # Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2508034d-5580-45c3-a47a-2626397a0cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df=df.withColumn(\"employee_id\",col(\"employee_id\").cast(\"Integer\")) \\\n",
    "        .withColumn(\"salary\",col(\"salary\").cast(\"Float\"))   # Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4778e1f-6c1d-4df3-bab3-dbfc0b3ceef8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename a column"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|\n|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|\n|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumnRenamed(\"name\",\"employee_name\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8878242d-94c7-4dc0-9810-c3a2b520b297",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "gender=df.filter(df.gender==\"Female\") # Single condition\n",
    "gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60971139-7622-4bad-bccd-45671bcd865e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "f=df.filter(col(\"gender\")==\"Female\")\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91706941-f126-4154-88af-fa69990cab60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|\n|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|\n|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "m=df.filter(df.gender!='Female')\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52d4601-0ce1-49df-8616-c1ba264a2e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "fil=df.filter((df.employee_name==\"Jane Smith\") & (df.salary>=\"40000.0\")) # Multiple condition\n",
    "fil.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b432cdf8-d7c3-4a88-a0b7-304e7493093c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "strts=df.filter(df.employee_name.startswith(\"Al\")) #case-sensitive\n",
    "strts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8999dfa4-61aa-4649-bca6-d0d9fdaea9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|\n|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "ends=df.filter(df.employee_name.endswith(\"n\"))  # case-sensitive\n",
    "ends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e59a66be-58b4-48be-8713-c834061a9caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|employee_name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "lke = df.filter(df.employee_name.like(\"%D%\"))\n",
    "lke.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa51d6b-99d2-4032-a094-11bddb7d8256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          7|          101|James Johnson| 42|  Male|   LOW|2012-03-15|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/data/input/employee_dup.csv\", inferSchema=True, header= True)\n",
    "df.orderBy(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5719030b-9be0-43af-ac52-fe42cd098a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ditinct/Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c945d8-7bbe-4c33-bc40-cf448f071ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          7|          101|James Johnson| 42|  Male|   LOW|2012-03-15|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.distinct().orderBy(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a408eb61-a7bd-4800-bee5-d070ca77c328",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DROP DUPLICATES"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          7|          101|James Johnson| 42|  Male|   LOW|2012-03-15|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates([\"employee_id\",\"salary\"]).orderBy(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "712e9931-e731-46de-9bc5-df24481ffb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          7|          101|James Johnson| 42|  Male|   LOW|2012-03-15|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.dropDuplicates().orderBy(\"employee_id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2922a0e8-877f-4f8b-a936-42b0b172997a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sort/OrderBy"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          7|          101|James Johnson| 42|  Male|   LOW|2012-03-15|\n+-----------+-------------+-------------+---+------+------+----------+\n\n+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          7|          101|James Johnson| 42|  Male|   LOW|2012-03-15|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(col(\"salary\").asc(),col(\"name\").desc()).show()\n",
    "df1=df.sort(df.salary.asc(),df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3e583c-7796-42d1-ae71-16c03e38a668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          7|          101|James Johnson| 42|  Male|   LOW|2012-03-15|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"salary\").asc(),col(\"name\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f98d25d-0787-4a3e-ab3d-80319980c43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Aggreation and GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a6f462-9401-4458-9aad-e0ca5a02872f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GroupBy"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|gender|sum(salary)|\n+------+-----------+\n|Female|   243000.0|\n|  Male|   223000.0|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"salary\",col(\"salary\").cast(\"Float\"))\n",
    "df.groupBy(\"gender\").sum(\"salary\").show() # Single "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8fc4d8-ff19-48dd-b89c-a1579605f873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+\n|gender| salary|max(salary)|\n+------+-------+-----------+\n|  Male|55000.0|    55000.0|\n|Female|45000.0|    45000.0|\n|  Male|60000.0|    60000.0|\n|Female|52000.0|    52000.0|\n|Female|47000.0|    47000.0|\n|Female|51000.0|    51000.0|\n|  Male|58000.0|    58000.0|\n|Female|48000.0|    48000.0|\n|  Male|50000.0|    50000.0|\n+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"salary\").isNotNull()).groupBy(\"gender\", \"salary\").max(\"salary\").show() # Multiple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53789b1-e3a1-4fae-ab57-84ecc632328e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|gender|count|\n+------+-----+\n|Female|    5|\n|  Male|    5|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f34051d3-20d6-4069-bb28-8f9efbf82707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e302d14d-292c-41fd-ae69-73c4a8808e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/data/input/datasets/employee_recs.csv\n",
    "/data/input/datasets/department_recs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23ee60f-c1a7-4415-8cb0-2c6c25f21b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf=spark.read.csv(\"/data/input/emp.csv\", header=True, inferSchema=True)   # .orderBy(\"department_id\")\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d9ffd2-8f5f-40b4-a3e3-a7602456fc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+----------------------+\n|department_id|department_code|department_name       |\n+-------------+---------------+----------------------+\n|105          |Operations     |Operations Department |\n|106          |DNA            |Data and Analytics    |\n|110          |Admin          |Administration        |\n|111          |IT             |Information Technology|\n|104          |Marketing      |Marketing & Sales     |\n|109          |Finance        |Finance Department    |\n|102          |BI             |Business Intelligence |\n|101          |DNA            |Data and Analytics    |\n|107          |QA             |Quality Assurance     |\n|108          |HR             |Human Resources       |\n+-------------+---------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "deptdf=spark.read.csv(\"/data/input/dept.csv\", header=True, inferSchema=True)\n",
    "deptdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b501da-63b0-4190-9e48-5257bf626c77",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inner Join"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\n|department_id|employee_id|name         |age|gender|salary|hire_date |department_code|department_name      |\n+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\n|105          |17         |George Wang  |34 |Male  |57000 |2016-03-15|Operations     |Operations Department|\n|104          |18         |Nancy Liu    |29 |Female|50000 |2017-06-01|Marketing      |Marketing & Sales    |\n|102          |20         |Grace Kim    |32 |Female|53000 |2018-11-01|BI             |Business Intelligence|\n|101          |7          |James Johnson|42 |Male  |70000 |2012-03-15|DNA            |Data and Analytics   |\n|102          |8          |Kate Kim     |29 |Female|51000 |2019-10-01|BI             |Business Intelligence|\n+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"inner\").show(n=5,truncate=False) # for same named column\n",
    "# empdf.join(deptdf,empdf.col1==deptdf.col1, \"inner\").show() # for different named join column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10b63d1-7c02-478d-b246-37c41ec13865",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Join/Left Outer"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\n|department_id|employee_id|name         |age|gender|salary|hire_date |department_code|department_name      |\n+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\n|105          |17         |George Wang  |34 |Male  |57000 |2016-03-15|Operations     |Operations Department|\n|104          |18         |Nancy Liu    |29 |Female|50000 |2017-06-01|Marketing      |Marketing & Sales    |\n|103          |19         |Steven Chen  |36 |Male  |62000 |2015-08-01|null           |null                 |\n|102          |20         |Grace Kim    |32 |Female|53000 |2018-11-01|BI             |Business Intelligence|\n|101          |7          |James Johnson|42 |Male  |70000 |2012-03-15|DNA            |Data and Analytics   |\n+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"left\").show(n=5, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30215eed-930c-4a11-9600-a0792657078b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Right Join/Right Outer"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+---------------------+\n|department_id|employee_id|name       |age |gender|salary|hire_date |department_code|department_name      |\n+-------------+-----------+-----------+----+------+------+----------+---------------+---------------------+\n|105          |12         |Susan Chen |31  |Female|54000 |2017-02-15|Operations     |Operations Department|\n|105          |17         |George Wang|34  |Male  |57000 |2016-03-15|Operations     |Operations Department|\n|106          |13         |Brian Kim  |45  |Male  |75000 |2011-07-01|DNA            |Data and Analytics   |\n|106          |15         |Michael Lee|37  |Male  |63000 |2014-09-30|DNA            |Data and Analytics   |\n|110          |null       |null       |null|null  |null  |null      |Admin          |Administration       |\n+-------------+-----------+-----------+----+------+------+----------+---------------+---------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"right\").show(n=5, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06725b49-7101-4af7-b899-f6cefad67ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+----------------------+\n|department_id|employee_id|name       |age |gender|salary|hire_date |department_code|department_name       |\n+-------------+-----------+-----------+----+------+------+----------+---------------+----------------------+\n|111          |null       |null       |null|null  |null  |null      |IT             |Information Technology|\n|110          |null       |null       |null|null  |null  |null      |Admin          |Administration        |\n|109          |null       |null       |null|null  |null  |null      |Finance        |Finance Department    |\n|108          |null       |null       |null|null  |null  |null      |HR             |Human Resources       |\n|107          |16         |Kelly Zhang|30  |Female|49000 |2018-04-01|QA             |Quality Assurance     |\n|107          |14         |Emily Lee  |26  |Female|46000 |2019-01-01|QA             |Quality Assurance     |\n|106          |15         |Michael Lee|37  |Male  |63000 |2014-09-30|DNA            |Data and Analytics    |\n|106          |13         |Brian Kim  |45  |Male  |75000 |2011-07-01|DNA            |Data and Analytics    |\n|105          |17         |George Wang|34  |Male  |57000 |2016-03-15|Operations     |Operations Department |\n|105          |12         |Susan Chen |31  |Female|54000 |2017-02-15|Operations     |Operations Department |\n|104          |18         |Nancy Liu  |29  |Female|50000 |2017-06-01|Marketing      |Marketing & Sales     |\n|104          |10         |Lisa Lee   |27  |Female|47000 |2018-08-01|Marketing      |Marketing & Sales     |\n|104          |11         |David Park |38  |Male  |65000 |2015-11-01|Marketing      |Marketing & Sales     |\n|103          |19         |Steven Chen|36  |Male  |62000 |2015-08-01|null           |null                  |\n|103          |9          |Tom Tan    |33  |Male  |58000 |2016-06-01|null           |null                  |\n+-------------+-----------+-----------+----+------+------+----------+---------------+----------------------+\nonly showing top 15 rows\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"fullouter\").orderBy(col(\"department_id\").desc()).show(n=15, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8354a975-a775-43eb-a534-2e55d8191006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+---+------+------+----------+---------------+-----------------+\n|department_id|employee_id|name       |age|gender|salary|hire_date |department_code|department_name  |\n+-------------+-----------+-----------+---+------+------+----------+---------------+-----------------+\n|103          |19         |Steven Chen|36 |Male  |62000 |2015-08-01|null           |null             |\n|103          |9          |Tom Tan    |33 |Male  |58000 |2016-06-01|null           |null             |\n|107          |16         |Kelly Zhang|30 |Female|49000 |2018-04-01|QA             |Quality Assurance|\n|103          |5          |Jack Chan  |40 |Male  |60000 |2013-04-01|null           |null             |\n|103          |6          |Jill Wong  |32 |Female|52000 |2018-07-01|null           |null             |\n|107          |14         |Emily Lee  |26 |Female|46000 |2019-01-01|QA             |Quality Assurance|\n+-------------+-----------+-----------+---+------+------+----------+---------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"left\").filter(col(\"department_id\").isin('107','103')).show(n=6, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e487b92-6146-4c93-975b-0a05945ad6aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Semi Join"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+---+------+------+----------+\n|department_id|employee_id|name       |age|gender|salary|hire_date |\n+-------------+-----------+-----------+---+------+------+----------+\n|107          |16         |Kelly Zhang|30 |Female|49000 |2018-04-01|\n|107          |14         |Emily Lee  |26 |Female|46000 |2019-01-01|\n+-------------+-----------+-----------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"leftsemi\").filter(col(\"department_id\").isin('107','103')).show(n=5, truncate=False) # Only matching frm left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c15264-5383-4ce0-a296-90f12abe7bd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Anti"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+---+------+------+----------+\n|department_id|employee_id|name       |age|gender|salary|hire_date |\n+-------------+-----------+-----------+---+------+------+----------+\n|103          |19         |Steven Chen|36 |Male  |62000 |2015-08-01|\n|103          |9          |Tom Tan    |33 |Male  |58000 |2016-06-01|\n|103          |5          |Jack Chan  |40 |Male  |60000 |2013-04-01|\n|103          |6          |Jill Wong  |32 |Female|52000 |2018-07-01|\n+-------------+-----------+-----------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"leftanti\").filter(col(\"department_id\").isin('107','103')).show(n=50, truncate=False) #Only non-matching frm left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6cf079-37ca-4cd4-a1e4-2e6333c144be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cross (Cartesian Product"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\n|department_id|employee_id|name         |age|gender|salary|hire_date |department_code|department_name      |\n+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\n|101          |1          |John Doe     |30 |Male  |50000 |2015-01-01|DNA            |Data and Analytics   |\n|101          |7          |James Johnson|42 |Male  |70000 |2012-03-15|DNA            |Data and Analytics   |\n|101          |2          |Jane Smith   |25 |Female|45000 |2016-02-15|DNA            |Data and Analytics   |\n|102          |8          |Kate Kim     |29 |Female|51000 |2019-10-01|BI             |Business Intelligence|\n|102          |20         |Grace Kim    |32 |Female|53000 |2018-11-01|BI             |Business Intelligence|\n|102          |3          |Bob Brown    |35 |Male  |55000 |2014-05-01|BI             |Business Intelligence|\n|102          |4          |Alice Lee    |28 |Female|48000 |2017-09-30|BI             |Business Intelligence|\n|104          |10         |Lisa Lee     |27 |Female|47000 |2018-08-01|Marketing      |Marketing & Sales    |\n|104          |11         |David Park   |38 |Male  |65000 |2015-11-01|Marketing      |Marketing & Sales    |\n|104          |18         |Nancy Liu    |29 |Female|50000 |2017-06-01|Marketing      |Marketing & Sales    |\n|105          |12         |Susan Chen   |31 |Female|54000 |2017-02-15|Operations     |Operations Department|\n|105          |17         |George Wang  |34 |Male  |57000 |2016-03-15|Operations     |Operations Department|\n|106          |13         |Brian Kim    |45 |Male  |75000 |2011-07-01|DNA            |Data and Analytics   |\n|106          |15         |Michael Lee  |37 |Male  |63000 |2014-09-30|DNA            |Data and Analytics   |\n|107          |14         |Emily Lee    |26 |Female|46000 |2019-01-01|QA             |Quality Assurance    |\n|107          |16         |Kelly Zhang  |30 |Female|49000 |2018-04-01|QA             |Quality Assurance    |\n+-------------+-----------+-------------+---+------+------+----------+---------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf,\"department_id\", \"cross\").orderBy(col(\"department_id\")).show(n=50, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d50833-2de3-494e-ae8d-5747b3ffda0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Union/Union All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c703e4-3034-460f-964b-61ae8c11d334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|     Sales| 50000|\n|  2|    Bob|        IT| 60000|\n|  3|Charlie|   Finance| 56000|\n|  4|  David|        HR| 52000|\n|  5|    Eva| Marketing| 58000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "emp1_data = [\n",
    "    [1, 'Alice', 'Sales', 50000],\n",
    "    [2, 'Bob', 'IT', 60000],\n",
    "    [3, 'Charlie', 'Finance', 56000],\n",
    "    [4, 'David', 'HR', 52000],\n",
    "    [5, 'Eva', 'Marketing', 58000]\n",
    "]\n",
    "col1 = ['id', 'name', 'department', 'salary']\n",
    "emp1 = spark.createDataFrame(emp1_data,col1)\n",
    "emp1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "712e0b54-7956-4688-a33a-5dcab3c6b8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  6|  Frank| Marketing| 51000|\n|  7|  Grace|        IT| 62000|\n|  3|Charlie|   Finance| 56000|\n|  9|    Ian|     Sales| 53000|\n| 10|   Judy|        HR| 59000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp2_data = [\n",
    "    [6, 'Frank', 'Marketing', 51000],\n",
    "    [7, 'Grace', 'IT', 62000],\n",
    "    [3, 'Charlie', 'Finance', 56000],\n",
    "    [9, 'Ian', 'Sales', 53000],\n",
    "    [10, 'Judy', 'HR', 59000]\n",
    "]\n",
    "cols2 = ['id', 'name', 'department', 'salary']\n",
    "\n",
    "emp2 = spark.createDataFrame(data=emp2_data,schema=cols2)\n",
    "emp2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8251a7f-2d5f-4cb4-9cb1-52aa070b3324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|     Sales| 50000|\n|  2|    Bob|        IT| 60000|\n|  3|Charlie|   Finance| 56000|\n|  4|  David|        HR| 52000|\n|  5|    Eva| Marketing| 58000|\n|  6|  Frank| Marketing| 51000|\n|  7|  Grace|        IT| 62000|\n|  3|Charlie|   Finance| 56000|\n|  9|    Ian|     Sales| 53000|\n| 10|   Judy|        HR| 59000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp1.union(emp2).show()  # We are getting duplicates for 'Charlie' with UNION as well here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4507ad-d3a5-4d1b-b62d-f60f3cdbff8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|     Sales| 50000|\n|  2|    Bob|        IT| 60000|\n|  3|Charlie|   Finance| 56000|\n|  4|  David|        HR| 52000|\n|  5|    Eva| Marketing| 58000|\n|  6|  Frank| Marketing| 51000|\n|  7|  Grace|        IT| 62000|\n|  9|    Ian|     Sales| 53000|\n| 10|   Judy|        HR| 59000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp1.union(emp2).distinct().show()  # So use DISTINCT to remove duplicates here with UNION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f32bbfd9-0b41-4d53-b503-d510736150ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "IMPORTANT: \n",
    "- In Apache Spark, the union() method does not remove duplicates by default. \n",
    "- It simply concatenates the datasets or DataFrames, similar to the SQL UNION ALL operation, which includes all records from both inputs, including duplicates\n",
    "- So use '**DISTINCT**' with '**UNION**' to not see duplicates!\n",
    "- 'unionAll' has been deprecated from from Spark 2.0 and later versions. It is recommended to use 'union'(without duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982ae421-d7e4-4899-9ffa-81086862b0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|     Sales| 50000|\n|  2|    Bob|        IT| 60000|\n|  3|Charlie|   Finance| 55000|\n|  4|  David|        HR| 52000|\n|  5|    Eva| Marketing| 58000|\n|  6|  Frank| Marketing| 51000|\n|  7|  Grace|        IT| 62000|\n|  8|  Helen|   Finance| 56000|\n|  9|    Ian|     Sales| 53000|\n| 10|   Judy|        HR| 59000|\n+---+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp1.unionAll(emp2).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72dcc67-6d9f-4335-9051-ead1a4f8a2c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### fill and fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a99f13c-9b19-4c56-a604-b5b30b7a4233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name| age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen|  36|  Male| 62000|2015-08-01|           null|              null|\n|          103|          9|    Tom Tan|  33|  Male| 58000|2016-06-01|           null|              null|\n|          103|          5|  Jack Chan|  40|  Male| 60000|2013-04-01|           null|              null|\n|          103|          6|  Jill Wong|  32|Female| 52000|2018-07-01|           null|              null|\n|          104|         18|  Nancy Liu|  29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee|  27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park|  38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|       null|null|  null|  null|      null|             HR|   Human Resources|\n|          109|       null|       null|null|  null|  null|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "filldf = empdf.join(deptdf,\"department_id\", \"fullouter\").filter(col(\"department_id\").isin('103','104','108','109')).orderBy(col(\"department_id\"))\n",
    "filldf.show(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36080a0-457f-4e9d-a93a-2b71cc9e989d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name| age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen|  36|  Male| 62000|2015-08-01|               |                  |\n|          103|          9|    Tom Tan|  33|  Male| 58000|2016-06-01|               |                  |\n|          103|          5|  Jack Chan|  40|  Male| 60000|2013-04-01|               |                  |\n|          103|          6|  Jill Wong|  32|Female| 52000|2018-07-01|               |                  |\n|          104|         18|  Nancy Liu|  29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee|  27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park|  38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|           |null|      |  null|      null|             HR|   Human Resources|\n|          109|       null|           |null|      |  null|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filldf.na.fill(\"\").show(n=50)  # this works only for string columns, it wont work for age, salary, id, hire_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e31a68bd-dd9d-4824-af93-e8e355524e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name| age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen|  36|  Male| 62000|2015-08-01|               |                  |\n|          103|          9|    Tom Tan|  33|  Male| 58000|2016-06-01|               |                  |\n|          103|          5|  Jack Chan|  40|  Male| 60000|2013-04-01|               |                  |\n|          103|          6|  Jill Wong|  32|Female| 52000|2018-07-01|               |                  |\n|          104|         18|  Nancy Liu|  29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee|  27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park|  38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|           |null|      |  null|      null|             HR|   Human Resources|\n|          109|       null|           |null|      |  null|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filldf.fillna(\"\").show() # fillna also works same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8849a4fd-8342-46e1-8348-6275c0c80c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+---+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name|age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+---+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen| 36|  Male| 62000|2015-08-01|           null|              null|\n|          103|          9|    Tom Tan| 33|  Male| 58000|2016-06-01|           null|              null|\n|          103|          5|  Jack Chan| 40|  Male| 60000|2013-04-01|           null|              null|\n|          103|          6|  Jill Wong| 32|Female| 52000|2018-07-01|           null|              null|\n|          104|         18|  Nancy Liu| 29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee| 27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park| 38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|          0|       null|  0|  null|     0|      null|             HR|   Human Resources|\n|          109|          0|       null|  0|  null|     0|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+---+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filldf.fillna(0).show() # For numeric columns like age, id, salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3aa902-97c3-4496-86de-f5f25702490f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+---+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name|age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+---+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen| 36|  Male| 62000|2015-08-01|           null|              null|\n|          103|          9|    Tom Tan| 33|  Male| 58000|2016-06-01|           null|              null|\n|          103|          5|  Jack Chan| 40|  Male| 60000|2013-04-01|           null|              null|\n|          103|          6|  Jill Wong| 32|Female| 52000|2018-07-01|           null|              null|\n|          104|         18|  Nancy Liu| 29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee| 27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park| 38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|       null|  0|  null|     0|      null|             HR|   Human Resources|\n|          109|       null|       null|  0|  null|     0|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+---+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filldf.fillna({\"age\": 0, \"salary\": 0}).show() # for only speccific numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd47264-3ff2-4459-9013-a49e1e5b38bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name| age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen|  36|  Male| 62000|2015-08-01|           null|              null|\n|          103|          9|    Tom Tan|  33|  Male| 58000|2016-06-01|           null|              null|\n|          103|          5|  Jack Chan|  40|  Male| 60000|2013-04-01|           null|              null|\n|          103|          6|  Jill Wong|  32|Female| 52000|2018-07-01|           null|              null|\n|          104|         18|  Nancy Liu|  29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee|  27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park|  38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|       null|null|  null|  null|2025-01-01|             HR|   Human Resources|\n|          109|       null|       null|null|  null|  null|2025-01-01|        Finance|Finance Department|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import date\n",
    "filldf.fillna({\"hire_date\": \"2025\"}).show() # for date columns  -it implicity converted 2025 into YYYY-MM-DD format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc6b5a9-ef93-4552-8208-b2293f46a942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name| age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen|  36|  Male| 62000|2015-08-01|         UKNOWN|            UKNOWN|\n|          103|          9|    Tom Tan|  33|  Male| 58000|2016-06-01|         UKNOWN|            UKNOWN|\n|          103|          5|  Jack Chan|  40|  Male| 60000|2013-04-01|         UKNOWN|            UKNOWN|\n|          103|          6|  Jill Wong|  32|Female| 52000|2018-07-01|         UKNOWN|            UKNOWN|\n|          104|         18|  Nancy Liu|  29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee|  27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park|  38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|     UKNOWN|null|UKNOWN|  null|      null|             HR|   Human Resources|\n|          109|       null|     UKNOWN|null|UKNOWN|  null|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filldf.na.fill(\"UKNOWN\").show(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f641a8f1-e1cc-41ad-8a98-af6405d76c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name| age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen|  36|  Male| 62000|2015-08-01|               |              null|\n|          103|          9|    Tom Tan|  33|  Male| 58000|2016-06-01|               |              null|\n|          103|          5|  Jack Chan|  40|  Male| 60000|2013-04-01|               |              null|\n|          103|          6|  Jill Wong|  32|Female| 52000|2018-07-01|               |              null|\n|          104|         18|  Nancy Liu|  29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee|  27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park|  38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|       null|null|  null|  null|      null|             HR|   Human Resources|\n|          109|       null|       null|null|  null|  null|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filldf.na.fill(\"\",[\"department_code\"]).show(n=50) # Single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5bbd61-7376-4dd9-aa73-b2a8914d3cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|department_id|employee_id|       name| age|gender|salary| hire_date|department_code|   department_name|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n|          103|         19|Steven Chen|  36|  Male| 62000|2015-08-01|               |              null|\n|          103|          9|    Tom Tan|  33|  Male| 58000|2016-06-01|               |              null|\n|          103|          5|  Jack Chan|  40|  Male| 60000|2013-04-01|               |              null|\n|          103|          6|  Jill Wong|  32|Female| 52000|2018-07-01|               |              null|\n|          104|         18|  Nancy Liu|  29|Female| 50000|2017-06-01|      Marketing| Marketing & Sales|\n|          104|         10|   Lisa Lee|  27|Female| 47000|2018-08-01|      Marketing| Marketing & Sales|\n|          104|         11| David Park|  38|  Male| 65000|2015-11-01|      Marketing| Marketing & Sales|\n|          108|       null|       null|null|      |  null|      null|             HR|   Human Resources|\n|          109|       null|       null|null|      |  null|      null|        Finance|Finance Department|\n+-------------+-----------+-----------+----+------+------+----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filldf.na.fill(\"\",[\"department_code\"]).na.fill(\"\",[\"gender\"]).show(n=50) # Multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a94f9fbc-714d-4d59-8228-a9dca48c9dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Collect()\n",
    "- PySpark RDD/DataFrame Collect(): Action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node\n",
    "- Use collect() on a smaller dataset usually after filter(), groupBy() etc.\n",
    "- Using collect() on large datasets results in OutOfMemory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ef684b-5d31-4048-8137-5aedfe80827b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [Row(department_id=103, employee_id=19, name='Steven Chen', age=36, gender='Male', salary=62000, hire_date=datetime.date(2015, 8, 1), department_code=None, department_name=None),\n Row(department_id=103, employee_id=9, name='Tom Tan', age=33, gender='Male', salary=58000, hire_date=datetime.date(2016, 6, 1), department_code=None, department_name=None),\n Row(department_id=103, employee_id=5, name='Jack Chan', age=40, gender='Male', salary=60000, hire_date=datetime.date(2013, 4, 1), department_code=None, department_name=None),\n Row(department_id=103, employee_id=6, name='Jill Wong', age=32, gender='Female', salary=52000, hire_date=datetime.date(2018, 7, 1), department_code=None, department_name=None),\n Row(department_id=104, employee_id=18, name='Nancy Liu', age=29, gender='Female', salary=50000, hire_date=datetime.date(2017, 6, 1), department_code='Marketing', department_name='Marketing & Sales'),\n Row(department_id=104, employee_id=10, name='Lisa Lee', age=27, gender='Female', salary=47000, hire_date=datetime.date(2018, 8, 1), department_code='Marketing', department_name='Marketing & Sales'),\n Row(department_id=104, employee_id=11, name='David Park', age=38, gender='Male', salary=65000, hire_date=datetime.date(2015, 11, 1), department_code='Marketing', department_name='Marketing & Sales'),\n Row(department_id=108, employee_id=None, name=None, age=None, gender=None, salary=None, hire_date=None, department_code='HR', department_name='Human Resources'),\n Row(department_id=109, employee_id=None, name=None, age=None, gender=None, salary=None, hire_date=None, department_code='Finance', department_name='Finance Department')]"
     ]
    }
   ],
   "source": [
    "filldf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1aae518-623b-465c-8984-f8198a6578b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### StructType and StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bce46a4-946e-45b8-8066-4240f4257d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n| id|   name|country|\n+---+-------+-------+\n|  1|Srishti|  India|\n|  2|  Srish|  India|\n|  3|  Rohan|  India|\n+---+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'Srishti', 'India'),(2,'Srish', 'India'),(3,'Rohan', 'India')]\n",
    "col=['id','name','country']\n",
    "ppldf=spark.createDataFrame(data=data,schema=col)\n",
    "ppldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f30e7a5-0b84-4051-86a7-e85da73ca5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "ppldf.printSchema()  # It has taken default schema as per the column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9c0b95-b658-4b8e-ac65-5869eb06d02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how to define proper schema below using StructType and StrucField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b16a94-686b-49a0-bbc6-0b4c86b9cc52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n| ID|   Name|Country|\n+---+-------+-------+\n|  1|Srishti|  India|\n|  2|  Srish|  India|\n|  3|  Rohan|  India|\n+---+-------+-------+\n\nroot\n |-- ID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType \n",
    "\n",
    "data = [(1,'Srishti', 'India'),(2,'Srish', 'India'),(3,'Rohan', 'India')]\n",
    "\n",
    "# schema = StructType([StructField(name='ID', dataType=IntegerType()),\n",
    "#                      StructField(name='Name', dataType=StringType()),\n",
    "#                      StructField(name='Country', dataType=StringType())])\n",
    "# OR\n",
    "schema = StructType([StructField('ID', IntegerType()),\n",
    "                     StructField('Name', StringType()),\n",
    "                     StructField('Country', StringType())])\n",
    "ppldf=spark.createDataFrame(data,schema)\n",
    "ppldf.show()\n",
    "ppldf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "954b729d-72c3-40d9-8067-88465ad033aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pivot/Unpivot\n",
    "- Pivot : Row values (Unique) to Column\n",
    "- Unpivot: Column values (Unique) to Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41216133-5b3d-491a-8f88-95101e03928c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n|      Item|Amount|   Country|\n+----------+------+----------+\n|Strawberry|  1000|     India|\n|     Mango|   500|       USA|\n|Watermelon|   750|    Mexico|\n| Pineapple|  1200|     India|\n|     Mango|   900|     India|\n| Pineapple|  1100|Costa_Rica|\n|Strawberry|   650|Costa_Rica|\n|Watermelon|   800|    Mexico|\n|Watermelon|   950|    Mexico|\n|Strawberry|   700|     India|\n+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "items = [\n",
    "    ('Strawberry', 1000, 'India'), ('Mango', 500, 'USA'), ('Watermelon', 750, 'Mexico'), ('Pineapple', 1200, 'India'), ('Mango', 900, 'India'),\n",
    "    ('Pineapple', 1100, 'Costa_Rica'), ('Strawberry', 650, 'Costa_Rica'), ('Watermelon', 800, 'Mexico'), ('Watermelon', 950, 'Mexico'), ('Strawberry', 700, 'India')\n",
    "]\n",
    "columns = [\"Item\",\"Amount\",\"Country\"]\n",
    "idf = spark.createDataFrame(items,columns)\n",
    "idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401a0a57-95a2-46fc-9881-9198d371a4ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pivot"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3167937683094062>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43midf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpivot\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCountry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   2935\u001B[0m \n",
       "\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n",
       "\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n",
       "\u001B[1;32m   2966\u001B[0m     )\n",
       "\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'pivot'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3167937683094062>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43midf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpivot\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCountry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   2935\u001B[0m \n\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n\u001B[1;32m   2966\u001B[0m     )\n\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'pivot'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'pivot'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idf.pivot(\"Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15648000-f33c-48ef-bb99-7b1b5417cc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Important\n",
    "- pivot() must be used after groupBy().\n",
    "- You need to specify an aggregation function (like sum(), avg(), etc.).\n",
    "- The syntax idf.pivot(\"Country\").show() is incomplete; it should include groupBy() and agg().\n",
    "- idf.groupBy(\"some_column\").pivot(\"Country\").agg(F.sum(\"value_column\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2c3abe-c02b-46e3-a98c-fa6bd7b47ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+------+----+\n|      Item|Costa_Rica|India|Mexico| USA|\n+----------+----------+-----+------+----+\n| Pineapple|      1100| 1200|  null|null|\n|Strawberry|       650| 1700|  null|null|\n|     Mango|      null|  900|  null| 500|\n|Watermelon|      null| null|  2500|null|\n+----------+----------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "idf.groupBy(\"Item\").pivot(\"Country\").sum(\"Amount\").show() # Under the hood, sum performs like .agg(F.sum(\"Amount\"))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd0b0f9-9283-4d97-aafc-30a8da6df667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+------+----+\n|      Item|Costa_Rica|India|Mexico| USA|\n+----------+----------+-----+------+----+\n| Pineapple|      1100| 1200|  null|null|\n|Strawberry|       650| 1700|  null|null|\n|     Mango|      null|  900|  null| 500|\n|Watermelon|      null| null|  2500|null|\n+----------+----------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "idf2=idf.groupBy(\"Item\").pivot(\"Country\").sum(\"Amount\")\n",
    "idf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be13f0aa-a7be-4406-b48a-ee3ce0b438cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unpivot"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+\n|      Item|Country|Amount|\n+----------+-------+------+\n| Pineapple|  India|  1200|\n|Strawberry|  India|  1700|\n|     Mango|  India|   900|\n|Watermelon|  India|  null|\n+----------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "idf2.select(\"Item\",expr(\"stack(1,'India',India) AS (Country, Amount)\")).show() # Unpivoting for one column and its total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99826f70-ba5e-45b3-a460-55cc628946e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n|      Item|   Country|Amount|\n+----------+----------+------+\n| Pineapple|     India|  1200|\n| Pineapple|Costa_Rica|  1100|\n| Pineapple|       USA|  null|\n| Pineapple|    Mexico|  null|\n|Strawberry|     India|  1700|\n|Strawberry|Costa_Rica|   650|\n|Strawberry|       USA|  null|\n|Strawberry|    Mexico|  null|\n|     Mango|     India|   900|\n|     Mango|Costa_Rica|  null|\n|     Mango|       USA|   500|\n|     Mango|    Mexico|  null|\n|Watermelon|     India|  null|\n|Watermelon|Costa_Rica|  null|\n|Watermelon|       USA|  null|\n|Watermelon|    Mexico|  2500|\n+----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Unpivoting multiple columns into row and its total with 'Item' column as it is\n",
    "idf2.select(\"Item\",expr(\"stack(4,'India',India,'Costa_Rica',Costa_Rica,'USA',USA,'Mexico',Mexico) AS (Country, Amount)\")).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77997a17-46e9-47d6-bc2d-bea307a350dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### UDF's (User Defined Functions)\n",
    "- PySpark UDF's are the most expensiveo operations from computation perspective.\n",
    "- So, it is not recommended to use UDF unless it is extremely essential for you to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e27fd0-9040-4f49-932f-6c82dea0da34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Finance\",10),\n",
    "       (\"Marketing\",20),\n",
    "       (\"Sales\",30),\n",
    "       (\"IT\",40)]\n",
    "cols = [\"dept_name\",\"dept_id\"] \n",
    "depdf = spark.createDataFrame(data, cols)\n",
    "depdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb00b561-c853-4656-8f36-f68915e5861a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def addone(a):\n",
    "    return a+1\n",
    "addonedf = udf(addone, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe0d852-ff0a-4c2a-981c-eaf290ca12a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------------+\n|dept_name|dept_id|addone(dept_id)|\n+---------+-------+---------------+\n|  Finance|     10|             11|\n|Marketing|     20|             21|\n|    Sales|     30|             31|\n|       IT|     40|             41|\n+---------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "depdf.select(\"dept_name\",\"dept_id\",addonedf(\"dept_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b09d178b-d5aa-473e-99cd-36f621036f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800bf89a-5482-4543-9ebc-c9f30a772c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1,'Srishti',60000),(2,'Shetty',70000),(3,'Rohan',40000)]\n",
    "columns = ['ID','Name','Salary']\n",
    "\n",
    "trdf = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9145a8d-264d-4530-86c3-fc6e80e4b329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "def uppercase(trdf):\n",
    "    return trdf.withColumn(\"Name\", upper(trdf.Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe38805-ca6f-4ca3-a7d2-e1b659429bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| ID|   Name|Salary|\n+---+-------+------+\n|  1|SRISHTI| 60000|\n|  2| SHETTY| 70000|\n|  3|  ROHAN| 40000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "trdf.transform(uppercase).show() # Transform on the uppercase function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e66767e-8f98-4d06-9393-7ea6f545c301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e7046c-b6b6-4f98-8292-f59a5d6bb17d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trdf.createOrReplaceTempView(\"employeetempview\")  # Create a table on df. This table will be lost after its running session is lost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca9eb45-1f54-47e2-9834-6b9b5b5d5fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>maxsal</th></tr></thead><tbody><tr><td>1</td><td>Srishti</td><td>60000</td></tr><tr><td>2</td><td>Shetty</td><td>70000</td></tr><tr><td>3</td><td>Rohan</td><td>40000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Srishti",
         60000
        ],
        [
         2,
         "Shetty",
         70000
        ],
        [
         3,
         "Rohan",
         40000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "maxsal",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT id, name,MAX(Salary) AS maxsal\n",
    "FROM employeetempview\n",
    "GROUP BY id, name  -- You can use all sql operations on this temp view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50263f59-2f21-49a2-ab98-f927134dba91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Window Functions\n",
    "- ROW_NUMBER()\n",
    "- RANK()\n",
    "- DENSE_RANK()\n",
    "- LEAD()\n",
    "- LAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe6f997-5962-4e54-bbc7-989ffb50f12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+\n| ID|   Name|Salary|Country|\n+---+-------+------+-------+\n|  1|  Alice| 75000|    USA|\n|  2|    Bob| 62000|    USA|\n|  3|Charlie| 82000| Canada|\n|  4|  David| 82000| Canada|\n|  5|    Eve| 55000|    USA|\n|  6|  Frank| 47000| Canada|\n|  7|  Grace| 55000|    USA|\n+---+-------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "e = data = [\n",
    "    (1, 'Alice', 75000, 'USA'),\n",
    "    (2, 'Bob', 62000, 'USA'),\n",
    "    (3, 'Charlie', 82000, 'Canada'),\n",
    "    (4, 'David', 82000, 'Canada'),  # Same salary as Charlie in Canada\n",
    "    (5, 'Eve', 55000, 'USA'),\n",
    "    (6, 'Frank', 47000, 'Canada'),\n",
    "    (7, 'Grace', 55000, 'USA')    # Same salary as Eve in USA\n",
    "]\n",
    "c = ['ID','Name','Salary', 'Country']\n",
    "edf = spark.createDataFrame(e,c)\n",
    "edf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f76b042-5534-48d5-bdcd-b76bfe0deef5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Row_Number()"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+---+\n| ID|   Name|Salary|Country| rn|\n+---+-------+------+-------+---+\n|  6|  Frank| 47000| Canada|  1|\n|  3|Charlie| 82000| Canada|  2|\n|  4|  David| 82000| Canada|  3|\n|  5|    Eve| 55000|    USA|  1|\n|  7|  Grace| 55000|    USA|  2|\n|  2|    Bob| 62000|    USA|  3|\n|  1|  Alice| 75000|    USA|  4|\n+---+-------+------+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "rnwindow = Window.partitionBy(\"Country\").orderBy(\"Salary\")\n",
    "edf.withColumn(\"rn\",row_number().over(rnwindow)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90bb4979-a67a-4091-8c22-be66dcd3cc5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rank"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+---+\n| ID|   Name|Salary|Country| rn|\n+---+-------+------+-------+---+\n|  6|  Frank| 47000| Canada|  1|\n|  3|Charlie| 82000| Canada|  2|\n|  4|  David| 82000| Canada|  2|\n|  5|    Eve| 55000|    USA|  1|\n|  7|  Grace| 55000|    USA|  1|\n|  2|    Bob| 62000|    USA|  3|\n|  1|  Alice| 75000|    USA|  4|\n+---+-------+------+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "edf.withColumn(\"rn\",rank().over(rnwindow)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7b4e5b-0215-4d3c-96f8-02f72271cb48",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DENSE RANK"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+---+\n| ID|   Name|Salary|Country| rn|\n+---+-------+------+-------+---+\n|  6|  Frank| 47000| Canada|  1|\n|  3|Charlie| 82000| Canada|  2|\n|  4|  David| 82000| Canada|  2|\n|  5|    Eve| 55000|    USA|  1|\n|  7|  Grace| 55000|    USA|  1|\n|  2|    Bob| 62000|    USA|  2|\n|  1|  Alice| 75000|    USA|  3|\n+---+-------+------+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "edf.withColumn(\"rn\",dense_rank().over(rnwindow)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39a745b4-4245-4209-8e8f-41339bfeb7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Date functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652b279d-0a1e-44c3-bf9e-3d431c565989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n|date_col  |timestamp_col          |\n+----------+-----------------------+\n|2025-07-23|2025-07-23 10:30:00.789|\n|2025-08-15|2025-08-15 14:45:30.264|\n+----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "time = [\n",
    "    ('2025-07-23', '2025-07-23 10:30:00.789'),\n",
    "    ('2025-08-15', '2025-08-15 14:45:30.264')\n",
    "]\n",
    "timecols = ['date_col','timestamp_col']\n",
    "timedf = spark.createDataFrame(time,timecols)\n",
    "timedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea0ce9a-7d57-4f69-8302-216df39cb4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, date_add, date_sub, datediff, date_trunc, year, month, dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4499f80-468b-499a-9042-f45a1c2f665f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "date_format"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+----------------------------------+----------+\n|  date_col|date_format(date_col, dd-MMMM-yy)|date_format(date_col, dd/MMM/yyyy)|      date|\n+----------+---------------------------------+----------------------------------+----------+\n|2025-07-23|                       23-July-25|                       23/Jul/2025|2025/07/23|\n|2025-08-15|                     15-August-25|                       15/Aug/2025|2025/08/15|\n+----------+---------------------------------+----------------------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "timedf.select(\"date_col\",date_format(\"date_col\",\"dd-MMMM-yy\"),date_format(\"date_col\",\"dd/MMM/yyyy\"),date_format(\"date_col\",\"yyyy/MM/dd\").alias(\"date\")).show()  # 'M' should be capital or else it will yield 00 as month!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac76de0a-51f8-44eb-8a45-3e220b8b851d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "date_add"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n|  date_col|date_add(date_col, 10)|\n+----------+----------------------+\n|2025-07-23|            2025-08-02|\n|2025-08-15|            2025-08-25|\n+----------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "timedf.select(\"date_col\", date_add(\"date_col\",10)).show()  # Will add 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33254e71-e698-4ab8-a719-69f2fb7d1e26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "date_sub"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|  date_col|   newdate|\n+----------+----------+\n|2025-07-23|2025-07-13|\n|2025-08-15|2025-08-05|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "timedf.select(\"date_col\", date_sub(\"date_col\",10).alias(\"newdate\")).show() # Will remove 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e021a7-0192-47f0-847b-763b94eefd3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "datediff"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n|  date_col|   newdate|difference|\n+----------+----------+----------+\n|2025-07-23|2025-07-13|        10|\n|2025-08-15|2025-08-05|        10|\n+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "dddf= timedf.select(\"date_col\", date_sub(\"date_col\",10).alias(\"newdate\"))\n",
    "dddf.select(\"date_col\",\"newdate\",datediff(\"date_col\",\"newdate\").alias(\"difference\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58fd0c6-93e4-4220-a820-58e32e30d0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n|  date_col|year(date_col)|\n+----------+--------------+\n|2025-07-23|          2025|\n|2025-08-15|          2025|\n+----------+--------------+\n\n+----------+-------------------+\n|  date_col|dayofyear(date_col)|\n+----------+-------------------+\n|2025-07-23|                204|\n|2025-08-15|                227|\n+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "dddf.select(\"date_col\",year(\"date_col\")).show()\n",
    "dddf.select(\"date_col\",dayofyear(\"date_col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1535ca0-caaf-4cdf-ae0d-71f5d3040559",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "date_trunc"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n|  date_col|  Start Of The Year|\n+----------+-------------------+\n|2025-07-23|2025-01-01 00:00:00|\n|2025-08-15|2025-01-01 00:00:00|\n+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "dddf.select(\"date_col\", date_trunc(\"year\",\"date_col\").alias(\"Start Of The Year\")).show() # Truncates each date to the start of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2367f6e-c238-4b41-a6ad-5be9f88e98fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are multiple 'Date functions' provided by PySpark. Check Spark documentation for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d46f85b8-b424-4e84-9d9e-6849d8eb50c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a83f40d3-157b-4bc3-aa61-7c24d0bddbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Partitioning\n",
    "- **partitionBy()** : Partition in Memory (DataFrame)\n",
    "- **repartition() or coalesce()**: Partition on Disk (File System)\n",
    "\n",
    "Examples:\n",
    "- df.write.option(\"header\",True).partitionBy(\"state).mode(\"overwrite).csv(\"/tmp/zipcodes-state\")\n",
    "- df.repartition(10).write.option(\"header\",True).mode(\"overwrite).csv(\"/tmp/zipcodes-state\")  \n",
    "- df.coalesce(2).write.option(\"header\",True).mode(\"overwrite).csv(\"/tmp/zipcodes-state\")\n",
    "\n",
    "\n",
    "\n",
    "> NOTE: \n",
    "- **Repartition** increase or decrease the number of partitions, and it shuffles all the data around. It's useful when you need a specific number of partitions or want the data evenly distributed, but performance is slower.\n",
    "- **Coalesce** only decreases the partition with minimal or no shuffling, so it's faster and uses less resources. It's best when you already have more partitions than needed and want to combine them efficiently, especially after filtering or summarizing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6754fa80-6e41-4459-9866-ab18314d4fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explode() function\n",
    "- In PySpark, the explode() function is used to transform a column containing arrays or maps into multiple rows, one for each element in the array or each key-value pair in the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefeca5d-eff1-4a18-a8a5-0928133c023c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+-----+\n|customer|               items|order_id|total|\n+--------+--------------------+--------+-----+\n|   Alice|[{1200, Laptop, 1...|       1| 1250|\n|     Bob|   [{800, Phone, 1}]|       2|  800|\n+--------+--------------------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "jsondf = spark.read.json(\"file:///data/input/order_singleline.json\")  # Use 'file:///' for files which are physically not visible in the UI of Databricks path\n",
    "jsondf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3425350b-3772-45c2-a285-59dfb192bd43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer</th><th>items</th><th>order_id</th><th>total</th></tr></thead><tbody><tr><td>Alice</td><td>List(List(1200, Laptop, 1), List(25, Mouse, 2))</td><td>1</td><td>1250</td></tr><tr><td>Bob</td><td>List(List(800, Phone, 1))</td><td>2</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         [
          [
           1200,
           "Laptop",
           1
          ],
          [
           25,
           "Mouse",
           2
          ]
         ],
         1,
         1250
        ],
        [
         "Bob",
         [
          [
           800,
           "Phone",
           1
          ]
         ],
         2,
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "items",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"price\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"product\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"quantity\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(jsondf.collect()) # display is databricks function since show() isnt showing the output for this im using display for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8b4c77-b8cb-45f0-a154-a30b6705d087",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Single flattening using explode"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer</th><th>col</th><th>order_id</th><th>total</th></tr></thead><tbody><tr><td>Alice</td><td>List(1200, Laptop, 1)</td><td>1</td><td>1250</td></tr><tr><td>Alice</td><td>List(25, Mouse, 2)</td><td>1</td><td>1250</td></tr><tr><td>Bob</td><td>List(800, Phone, 1)</td><td>2</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         [
          1200,
          "Laptop",
          1
         ],
         1,
         1250
        ],
        [
         "Alice",
         [
          25,
          "Mouse",
          2
         ],
         1,
         1250
        ],
        [
         "Bob",
         [
          800,
          "Phone",
          1
         ],
         2,
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"price\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"product\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"quantity\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "display(jsondf.select(\"customer\",explode(col(\"items\")), \"order_id\", \"total\"))  # First_Level explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c8dfb1d-d191-49b0-9d20-458c8c875c46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Recursive flattening/Deeep flattening"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer</th><th>total</th><th>product</th><th>quantity</th><th>price</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>1250</td><td>Laptop</td><td>1</td><td>1200</td></tr><tr><td>1</td><td>Alice</td><td>1250</td><td>Mouse</td><td>2</td><td>25</td></tr><tr><td>2</td><td>Bob</td><td>800</td><td>Phone</td><td>1</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         1250,
         "Laptop",
         1,
         1200
        ],
        [
         1,
         "Alice",
         1250,
         "Mouse",
         2,
         25
        ],
        [
         2,
         "Bob",
         800,
         "Phone",
         1,
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explodeddf = jsondf.select(\"customer\",explode(col(\"items\")).alias(\"item\"), \"order_id\", \"total\") \n",
    "\n",
    "# Full/Recursive Explosion\n",
    "result_df = explodeddf.select(                                  \n",
    "    \"order_id\",\n",
    "    \"customer\",\n",
    "    \"total\",\n",
    "    col(\"item.product\").alias(\"product\"),\n",
    "    col(\"item.quantity\").alias(\"quantity\"),\n",
    "    col(\"item.price\").alias(\"price\")\n",
    ")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73166255-7531-4388-a345-e2102b65d29f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jsondf.createOrReplaceTempView(\"jsonview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264c4e09-c797-4158-ab63-fa96969ff48c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer</th><th>items</th><th>order_id</th><th>total</th></tr></thead><tbody><tr><td>Alice</td><td>List(List(1200, Laptop, 1), List(25, Mouse, 2))</td><td>1</td><td>1250</td></tr><tr><td>Bob</td><td>List(List(800, Phone, 1))</td><td>2</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         [
          [
           1200,
           "Laptop",
           1
          ],
          [
           25,
           "Mouse",
           2
          ]
         ],
         1,
         1250
        ],
        [
         "Bob",
         [
          [
           800,
           "Phone",
           1
          ]
         ],
         2,
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "items",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"price\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"product\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"quantity\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM jsonview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a346cf-9fac-4895-b608-4802236984df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "First level explosion in Spark SQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer</th><th>item</th><th>order_id</th><th>total</th></tr></thead><tbody><tr><td>Alice</td><td>List(1200, Laptop, 1)</td><td>1</td><td>1250</td></tr><tr><td>Alice</td><td>List(25, Mouse, 2)</td><td>1</td><td>1250</td></tr><tr><td>Bob</td><td>List(800, Phone, 1)</td><td>2</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         [
          1200,
          "Laptop",
          1
         ],
         1,
         1250
        ],
        [
         "Alice",
         [
          25,
          "Mouse",
          2
         ],
         1,
         1250
        ],
        [
         "Bob",
         [
          800,
          "Phone",
          1
         ],
         2,
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "item",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"price\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"product\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"quantity\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT customer ,explode(items) as item, order_id, total\n",
    "FROM jsonview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fc6080d-fce4-4e4d-bbd4-35b03b16e59d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**What does LATERAL VIEW do?**\n",
    "- It allows you to \"explode\" or \"flatten\" nested collections (like arrays or maps) into multiple rows.\n",
    "- When combined with functions like explode(), it generates a separate row for each element in the array or collection.\n",
    "- It works like a join, but specifically for expanding nested data into multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad30385-cd88-48b5-8c40-3d3908918b74",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full explosion in Spark SQL with LATERAL VIEW"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer</th><th>order_id</th><th>total</th><th>product</th><th>quantity</th><th>price</th></tr></thead><tbody><tr><td>Alice</td><td>1</td><td>1250</td><td>Laptop</td><td>1</td><td>1200</td></tr><tr><td>Alice</td><td>1</td><td>1250</td><td>Mouse</td><td>2</td><td>25</td></tr><tr><td>Bob</td><td>2</td><td>800</td><td>Phone</td><td>1</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         1,
         1250,
         "Laptop",
         1,
         1200
        ],
        [
         "Alice",
         1,
         1250,
         "Mouse",
         2,
         25
        ],
        [
         "Bob",
         2,
         800,
         "Phone",
         1,
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT customer, order_id, total, item.product, item.quantity, item.price\n",
    "FROM jsonview\n",
    "LATERAL VIEW explode(items) AS item  -- alias is complusory for a LATERAL VIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50022e34-97f5-463d-89ba-20b62bd09e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cache VS Persist (Storing intermediate results of a DataFrame)\n",
    "\n",
    "**`cache()`**:\n",
    "  - Caches the DataFrame or RDD in memory with default storage level (`MEMORY_AND_DISK` in Spark 2.0+) and `MEMORY_ONLY` in RDD\n",
    "  - Stores the dataset in memory (RAM) primarily, with fallback to disk if needed.\n",
    "  - Used for quick, simple caching without needing to specify storage levels.\n",
    "  - Less flexible as it doesn't allow choosing different storage options.\n",
    "  - Syntax: _dataframe.cache()_\n",
    "\n",
    "**`persist()`**:\n",
    "  - Persists the DataFrame or RDD with a specified storage level\n",
    "  - Allows specifying various storage levels- `MEMORY_ONLY`, `DISK_ONLY`, `MEMORY_AND_DISK`,`MEMORY_ONLY_SER`,`MEMORY_AND_DISK_SER`,`OFF_HEAP (available in Spark 3.0+)`\n",
    "  - Provides more control over how data is stored and managed in memory/disk\n",
    "  - Suitable for advanced optimization when specific storage behavior is needed.\n",
    "  - Syntax: _dataframe.persist(StorageLevel.MEMORY_ONLY)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edba4986-afe2-4e51-ad6b-3f9c1c1703b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Interview questions**\n",
    "- Print duplicate records(entries)/columns values, present in a dataset\n",
    "- Remove duplicate entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4c6877-d943-40ca-9689-f780ecfe9168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n| ID|            EmailId|\n+---+-------------------+\n|  1|      abc@gmail.com|\n|  2|      xyz@gmail.com|\n|  3|srishti@outlook.com|\n|  2|      xyz@gmail.com|\n|  4|srishti@outlook.com|\n+---+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emails = [(1,'abc@gmail.com'),\n",
    "            (2,'xyz@gmail.com'),\n",
    "            (3,'srishti@outlook.com'),\n",
    "            (2,'xyz@gmail.com'),\n",
    "            (4,'srishti@outlook.com')]\n",
    "\n",
    "col = ['ID','EmailId']\n",
    "emaildf = spark.createDataFrame(emails, col)\n",
    "emaildf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed0217ad-94ca-432d-b47f-0c609d18b76a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Print Duplicates for a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a53ea89-99c9-4590-a9e8-2399e69bc7ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySaprk"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|            EmailId|count|\n+-------------------+-----+\n|      xyz@gmail.com|    2|\n|srishti@outlook.com|    2|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "emaildf.groupBy(\"EmailId\").count().filter(col(\"count\")>1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2314a9-88ff-4f7e-a312-ba59502c4070",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SparkSQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th><th>EmailId</th></tr></thead><tbody><tr><td>2</td><td>xyz@gmail.com</td></tr><tr><td>2</td><td>srishti@outlook.com</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "xyz@gmail.com"
        ],
        [
         2,
         "srishti@outlook.com"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "EmailId",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*), EmailId \n",
    "FROM Duplicates\n",
    "GROUP BY EmailId\n",
    "HAVING COUNT(*) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3866694-56db-420c-ad0e-5f27d6594b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANT: Printing Duplicates for Records in PySpark and SparkSQL**\n",
    "- In standard SQL, you must specify the columns to check for duplicates; there is no built-in way to automatically check all columns without listing them. SQL does not support GROUP BY *."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18417a8a-8dc5-4979-8fbe-6ac027c0e80a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+\n| ID|      EmailId|count|\n+---+-------------+-----+\n|  2|xyz@gmail.com|    2|\n+---+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emaildf.groupBy(emaildf.columns).count().filter(col(\"count\") > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c901d19-e163-4be8-8ebf-c4e0b5aefcde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emaildf.createOrReplaceTempView(\"Duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6aaef85-0eb5-4468-aba2-63fe8f2f03bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SELECT COUNT(*),*\n",
    "-- FROM Duplicates\n",
    "-- GROUP BY *                    # AnalysisException: Invalid usage of '*' in Aggregate.;\n",
    "-- HAVING COUNT(*) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e1d1022-4b22-4558-97a2-fd84fe496b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Since using '*' directly in the GROUP BY clause is invalid, generate the list of columns dynamically in Python as shown below.\n",
    "- By using the below method, you can efficiently create complex SQL queries for tables with many columns without manual effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6b0c93-1f2f-4ddb-9d23-3fd7b82c255c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---+\n| ID|      EmailId|cnt|\n+---+-------------+---+\n|  2|xyz@gmail.com|  2|\n+---+-------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "columns = [col for col in spark.table(\"Duplicates\").columns]\n",
    "cols_str = \", \".join(columns)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT {cols_str}, COUNT(*) as cnt\n",
    "FROM Duplicates\n",
    "GROUP BY {cols_str}\n",
    "HAVING COUNT(*) > 1\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771b62ef-de64-432e-9706-1a8b948144ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = [col for col in spark.table(\"Duplicates\").columns]\n",
    "cols_str = \", \".join(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906d62da-3614-4e86-b637-c446b0f2b028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- The %sql magic command in Jupyter does not support Python string interpolation: '{col_str}' directly inside the SQL statement, so it will throw parse error shown below\n",
    "-- SELECT {cols_str}, COUNT(*) as cnt   -- ParseException: [PARSE_SYNTAX_ERROR] Syntax error at or near '}'.\n",
    "-- FROM Duplicates\n",
    "-- GROUP BY {cols_str}\n",
    "-- HAVING COUNT(*) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca0972f9-f8bc-4c92-b912-b222c2a7398f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Important:**\n",
    "- Remove Duplicates in PySpark and SparkSQL for a single column and all columns combined(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c90ee1-f41e-443f-bb90-67cc2c545813",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n| ID|            EmailId|\n+---+-------------------+\n|  1|      abc@gmail.com|\n|  2|      xyz@gmail.com|\n|  3|srishti@outlook.com|\n+---+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emaildf.dropDuplicates(['EmailId']).show() # For a Single Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036c6497-24e0-4e9a-aba7-e2cebeaa2e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---+\n| ID|            EmailId| rn|\n+---+-------------------+---+\n|  1|      abc@gmail.com|  1|\n|  2|      xyz@gmail.com|  1|\n|  3|srishti@outlook.com|  1|\n+---+-------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * \n",
    "            FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER (PARTITION BY EmailId ORDER BY ID) as rn  -- For a single column\n",
    "            FROM Duplicates\n",
    "            )\n",
    "            WHERE rn=1\n",
    "            ORDER BY ID\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d17d573-9aa8-4da6-af37-2cc6202cf204",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n| ID|            EmailId|\n+---+-------------------+\n|  1|      abc@gmail.com|\n|  2|      xyz@gmail.com|\n|  3|srishti@outlook.com|\n|  4|srishti@outlook.com|\n+---+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emaildf.dropDuplicates().show() # For all columns combined (records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b54e551-8c6a-4c19-855b-8282372794c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n| ID|            EmailId|\n+---+-------------------+\n|  1|      abc@gmail.com|\n|  2|      xyz@gmail.com|\n|  3|srishti@outlook.com|\n|  4|srishti@outlook.com|\n+---+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT DISTINCT * \n",
    "          FROM Duplicates\"\"\").show() # For all columns combined (records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9096a51a-8dbb-415c-a7ec-e724712a41c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Interview question:\n",
    "- Find Customers who haven't ordered anything\n",
    "- Find Customers who have ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82981c02-6e38-409c-9e38-1e6d71b624d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers = [\n",
    "    (1, 'Alice'),\n",
    "    (2, 'Bob'),\n",
    "    (3, 'Charlie'),\n",
    "    (4,'Taylor')\n",
    "]\n",
    "col_customers = ['cust_id', 'cust_name']\n",
    "\n",
    "\n",
    "orders = [\n",
    "    (101, 1),\n",
    "    (102, 2),\n",
    "    (103, 2)\n",
    "]\n",
    "col_orders = ['orderid', 'cust_id']\n",
    "\n",
    "custdf = spark.createDataFrame(customers,col_customers)\n",
    "orderdf = spark.createDataFrame(orders, col_orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb75e4b-76fd-40d3-ba81-6e01082269ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n|cust_id|cust_name|\n+-------+---------+\n|      1|    Alice|\n|      2|      Bob|\n|      3|  Charlie|\n|      4|   Taylor|\n+-------+---------+\n\n+-------+-------+\n|orderid|cust_id|\n+-------+-------+\n|    101|      1|\n|    102|      2|\n|    103|      2|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "custdf.show()\n",
    "orderdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37ea624-676f-4817-9df0-6ae1c09c3643",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Customers who haven't ordered anything"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+\n|cust_id|cust_name|orderid|\n+-------+---------+-------+\n|      3|  Charlie|   null|\n|      4|   Taylor|   null|\n+-------+---------+-------+\n\n+-------+---------+\n|cust_id|cust_name|\n+-------+---------+\n|      3|  Charlie|\n|      4|   Taylor|\n+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "custdf.join(orderdf, \"cust_id\", \"left\").filter(col(\"orderid\").isNull()).show()\n",
    "custdf.join(orderdf, \"cust_id\", \"left\").filter(orderdf.orderid.isNull()).select(custdf.cust_id,custdf.cust_name).show()  # Charlie and Taylor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49deaadd-770b-495e-bf83-70a6e32e7ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANT:**\n",
    "- For selecting all columns in a DataFrame: `*df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b395f2c-1b6e-4f68-9409-daa923ecbb95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Customers who have ordered something"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n|orderid|cust_id|cust_name|\n+-------+-------+---------+\n|    101|      1|    Alice|\n|    102|      2|      Bob|\n|    103|      2|      Bob|\n+-------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "custdf.join(orderdf, \"cust_id\", \"inner\").select(orderdf.orderid, *custdf.columns).distinct().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513c2229-44cc-4867-bfaf-812a159317f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n|orderid|cust_id|cust_name|\n+-------+-------+---------+\n|    101|      1|    Alice|\n|    102|      2|      Bob|\n|    103|      2|      Bob|\n+-------+-------+---------+\n\n+-------+---------+\n|cust_id|cust_name|\n+-------+---------+\n|      1|    Alice|\n|      2|      Bob|\n+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "custdf.join(orderdf, \"cust_id\", \"left\").filter(orderdf.orderid.isNotNull()).select(orderdf.orderid, *custdf.columns).distinct().show() \n",
    "custdf.join(orderdf, \"cust_id\", \"left\").filter(orderdf.orderid.isNotNull()).select(custdf.cust_id,custdf.cust_name).distinct().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d724ab1-d4de-4ec0-ab69-acea39b2c3e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941b4b52-c0c0-4d1c-a0b5-b33b22249c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n|orderid|cust_id|cust_name|\n+-------+-------+---------+\n|    101|      1|    Alice|\n|    102|      2|      Bob|\n|    103|      2|      Bob|\n+-------+-------+---------+\n\n+-------+-------+---------+\n|orderid|cust_id|cust_name|\n+-------+-------+---------+\n|    101|      1|    Alice|\n|    102|      2|      Bob|\n|    103|      2|      Bob|\n+-------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "custdf.join(orderdf, \"cust_id\", \"right\").filter(custdf.cust_id.isNotNull()).select(orderdf.orderid, *custdf.columns).distinct().show()  \n",
    "custdf.join(orderdf, \"cust_id\", \"right\").filter(orderdf.orderid.isNotNull()).select(orderdf.orderid, *custdf.columns).show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a300df-2d0e-4b2a-8b36-7eb15c8bbc20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interview questions:\n",
    "- Find highest salary based on each Department\n",
    "- Fine Employee with highest salary based on each Department\n",
    "- Find Lowest Salary based on each Department\n",
    "- Fine Employee with highest salary based on each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5e9e94-c292-44a1-909d-0ce501d40021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emps = [\n",
    "    (1, 'John Doe', 50000, 1),\n",
    "    (2, 'Jane Smith', 60000, 2),\n",
    "    (3, 'Mike Johnson', 55000, 3),\n",
    "    (4, 'Emily Davis', 62000, 4),\n",
    "    (5, 'Chris Wilson', 58000, 2),\n",
    "    (6, 'Sarah Brown', 52000, 1),\n",
    "    (7, 'David Lee', 61000, 4),\n",
    "    (8, 'Anna Kim', 54000, 2),\n",
    "    (9, 'Peter Parker', 47000, 3),\n",
    "    (10, 'Linda Green', 63000, 4)\n",
    "]\n",
    "col_emp = ['empid', 'emp_name', 'salary', 'deptid']\n",
    "\n",
    "depts = [\n",
    "    (1, 'HR'),\n",
    "    (2, 'Sales'),\n",
    "    (3, 'Data Engineer'),\n",
    "    (4, 'Marketing')\n",
    "]\n",
    "col_dept = ['deptid', 'dept_name']\n",
    "\n",
    "employeedf=spark.createDataFrame(emps,col_emp)\n",
    "departmentdf=spark.createDataFrame(depts,col_dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd872b4e-ed7e-4fb9-b4a2-1f0a9ac669bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+------+\n|empid|    emp_name|salary|deptid|\n+-----+------------+------+------+\n|    1|    John Doe| 50000|     1|\n|    6| Sarah Brown| 52000|     1|\n|    8|    Anna Kim| 54000|     2|\n|    5|Chris Wilson| 58000|     2|\n|    2|  Jane Smith| 60000|     2|\n|    9|Peter Parker| 47000|     3|\n|    3|Mike Johnson| 55000|     3|\n|   10| Linda Green| 63000|     4|\n|    4| Emily Davis| 62000|     4|\n|    7|   David Lee| 61000|     4|\n+-----+------------+------+------+\n\n+------+-------------+\n|deptid|    dept_name|\n+------+-------------+\n|     1|           HR|\n|     2|        Sales|\n|     3|Data Engineer|\n|     4|    Marketing|\n+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "employeedf.orderBy(\"deptid\").show()\n",
    "departmentdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df166817-9bb5-4e72-8e35-a8bee764faef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Highest salary based on each Department**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f329cf-8308-40cb-8eac-c9e4a99698f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|deptid|max(salary)|\n+------+-----------+\n|     1|      52000|\n|     2|      60000|\n|     3|      55000|\n|     4|      63000|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "employeedf.join(departmentdf, \"deptid\", \"inner\").groupBy(employeedf.deptid).max(\"salary\").show()\n",
    "# AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `empid` cannot be resolved. Did you mean one of the following? [`deptid`, `max(salary)`].;\n",
    "# employeedf.join(departmentdf, \"deptid\", \"inner\").groupBy(employeedf.deptid).max(\"salary\").select(*employeedf.columns).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb002a1-12bc-46a5-9de0-4cfc0147caad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The error occurs because after you use groupBy and max(\"salary\"), the resulting DataFrame only contains the columns used in groupBy (here, deptid) and the aggregated column (max(salary)).\n",
    "Columns like empid, emp_name, and salary are not available in this result, so you cannot select them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49ab710-1860-4081-9f08-18fc03f47df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|deptid|Max Salary|\n+------+----------+\n|     1|     52000|\n|     2|     60000|\n|     3|     55000|\n|     4|     63000|\n+------+----------+\n\n+-------------+----------+\n|    dept_name|Max Salary|\n+-------------+----------+\n|    Marketing|     63000|\n|        Sales|     60000|\n|Data Engineer|     55000|\n|           HR|     52000|\n+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "employeedf.join(departmentdf, \"deptid\", \"inner\").groupBy(employeedf.deptid).agg(max(\"salary\").alias('Max Salary')).show()employeedf.join(departmentdf, \"deptid\", \"inner\").groupBy(departmentdf.dept_name).agg(max(\"salary\").alias('Max Salary')).orderBy(col(\"Max Salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1abc5a2d-38a0-4369-980a-9571288397e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Employee with highest salary based on each Department**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa80abd5-43e1-400a-9e99-faa0b9f98214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|deptid|max(salary)|\n+------+-----------+\n|     1|      52000|\n|     2|      60000|\n|     3|      55000|\n|     4|      63000|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "maxsaldf = employeedf.join(departmentdf, \"deptid\", \"inner\").groupBy(employeedf.deptid).max(\"salary\")\n",
    "maxsaldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9817aa0-6267-4cdb-a2d1-08c65ad235e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can't just write \"deptid\" in the join condition when you want to join on multiple columns or with additional conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be09e16-431f-432a-9219-a98e4e26a2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# result = employeedf.join(maxsaldf, ((\"deptid\") & employeedf.salary==maxsaldf[\"max(salary)\"]), \"inner\")\\   # py4j.Py4JException: Method and([class java.lang.String]) does not exist\n",
    "#         .join(departmentdf, \"deptid\", \"inner\")\\\n",
    "#         .select(*employeedf.columns, departmentdf.dept_name)\n",
    "# result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd23506e-3cb8-4fdb-b41c-0c8a16713a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# result = emp.join(maxsal, (employeedf.deptid==maxsaldf.deptid) & (employeedf.salary==maxsaldf[\"max(salary)\"]), \"inner\")\\\n",
    "#   .join(dept, employeedf.deptid==departmentdf.deptid, \"inner\")\\\n",
    "#     .select(employeedf.empid, employeedf.emp_name, departmentdf.dept_name)\n",
    "# result.show()\n",
    "\n",
    "\n",
    "# AnalysisException: Column deptid#3087L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4c633d-4179-4985-9297-bdc599945fc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using Joins"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+------+-------------+\n|empid|    emp_name|salary|deptid|    dept_name|\n+-----+------------+------+------+-------------+\n|    6| Sarah Brown| 52000|     1|           HR|\n|    2|  Jane Smith| 60000|     2|        Sales|\n|    3|Mike Johnson| 55000|     3|Data Engineer|\n|   10| Linda Green| 63000|     4|    Marketing|\n+-----+------------+------+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "emp = employeedf.alias(\"emp\")\n",
    "maxsal = maxsaldf.alias(\"maxsal\")\n",
    "dept = departmentdf.alias(\"dept\")\n",
    "\n",
    "result = emp.join(maxsal,\n",
    "    (col(\"emp.deptid\") == col(\"maxsal.deptid\")) & (col(\"emp.salary\") == col(\"maxsal.max(salary)\")),\"inner\"\n",
    ").join(dept,\n",
    "    col(\"emp.deptid\") == col(\"dept.deptid\"),\"inner\"\n",
    ").select(\n",
    "    col(\"emp.empid\"),\n",
    "    col(\"emp.emp_name\"),\n",
    "    col(\"emp.salary\"),\n",
    "    col(\"emp.deptid\"),\n",
    "    col(\"dept.dept_name\")\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d5e4e4-92c5-47bc-8f78-bf2e56f1e1df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using Window Fucntion"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+------+-------------+\n|empid|    emp_name|salary|deptid|    dept_name|\n+-----+------------+------+------+-------------+\n|    6| Sarah Brown| 52000|     1|           HR|\n|    2|  Jane Smith| 60000|     2|        Sales|\n|    3|Mike Johnson| 55000|     3|Data Engineer|\n|   10| Linda Green| 63000|     4|    Marketing|\n+-----+------------+------+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window = Window.partitionBy(\"deptid\").orderBy(col(\"salary\").desc())\n",
    "ranked = employeedf.withColumn(\"rank\", rank().over(window))\n",
    "ranked.filter(col(\"rank\")==1)\\\n",
    "    .join(departmentdf, \"deptid\", \"inner\")\\\n",
    "        .select(\"empid\",\"emp_name\", \"salary\",\"deptid\", \"dept_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d38802-5fb6-407e-a198-e51a0ab59412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interview question:\n",
    "- Flatten a DataFrame\n",
    "- Find out the device pinged or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f15d47-12b5-463a-b119-f05a228a894f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flattening"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n|customer_id|  purchases_products|\n+-----------+--------------------+\n|          1|[Mobile, PC, Tablet]|\n|          2|        [Mobile, PC]|\n|          3|       [Tablet, Pen]|\n+-----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "pdts = [\n",
    "    (1,['Mobile','PC','Tablet']),\n",
    "    (2,['Mobile','PC']),\n",
    "    (3,['Tablet','Pen'])\n",
    "]\n",
    "\n",
    "pdt_schema = ['customer_id','purchases_products']\n",
    "\n",
    "flattendf=spark.createDataFrame(pdts,pdt_schema)\n",
    "flattendf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e48795f-022c-4645-9b2e-24d87fc3f2d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Result"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+\n|customer_id|  purchases_products|   col|\n+-----------+--------------------+------+\n|          1|[Mobile, PC, Tablet]|Mobile|\n|          1|[Mobile, PC, Tablet]|    PC|\n|          1|[Mobile, PC, Tablet]|Tablet|\n|          2|        [Mobile, PC]|Mobile|\n|          2|        [Mobile, PC]|    PC|\n|          3|       [Tablet, Pen]|Tablet|\n|          3|       [Tablet, Pen]|   Pen|\n+-----------+--------------------+------+\n\n+-----------+--------------------+--------+\n|customer_id|  purchases_products|products|\n+-----------+--------------------+--------+\n|          1|[Mobile, PC, Tablet]|  Mobile|\n|          1|[Mobile, PC, Tablet]|      PC|\n|          1|[Mobile, PC, Tablet]|  Tablet|\n|          2|        [Mobile, PC]|  Mobile|\n|          2|        [Mobile, PC]|      PC|\n|          3|       [Tablet, Pen]|  Tablet|\n|          3|       [Tablet, Pen]|     Pen|\n+-----------+--------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "flattendf.select(*flattendf.columns, explode(\"purchases_products\")).show()  # Without explicity naming the exploded column\n",
    "flattendf.withColumn(\"products\", explode(\"purchases_products\")).show() # Explicity naming the exploded column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa92a765-d0c4-4195-8da4-da8b26d1b8ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "First Not NULL value in the columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+-------------+\n|customer_id|device_using1|device_using2|device_using3|\n+-----------+-------------+-------------+-------------+\n|          1|          Yes|         null|         null|\n|          2|         null|          Yes|         null|\n|          3|           No|         null|          Yes|\n+-----------+-------------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "devices = [\n",
    "    (1,'Yes',None,None),\n",
    "    (2,None,'Yes',None),\n",
    "    (3,'No', None, 'Yes')\n",
    "] \n",
    "devicesschema = ['customer_id','device_using1','device_using2','device_using3']\n",
    "\n",
    "devicedf= spark.createDataFrame(devices,devicesschema)\n",
    "devicedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a88c53-1287-4e94-a8fa-70b115f311c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+-------------+---+\n|customer_id|device_using1|device_using2|device_using3|New|\n+-----------+-------------+-------------+-------------+---+\n|          1|          Yes|         null|         null|Yes|\n|          2|         null|          Yes|         null|Yes|\n|          3|           No|         null|          Yes| No|\n+-----------+-------------+-------------+-------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "devicedf.withColumn(\"New\", coalesce(col(\"device_using1\"),col(\"device_using2\"),col(\"device_using3\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75cc3c0-1f21-481a-bd85-7299be72fdca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interview Questions:\n",
    "- Parse and Convert string JSON into a proper JSON Format\n",
    "- After converting, separate the columns from the JSON body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f23565b-0988-426d-a968-43a3c306d532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------+\n|Name  |Address                                       |\n+------+----------------------------------------------+\n|Hania |{\"Street\": \"456 Elm St.\", \"City\": \"New York\"} |\n|Sophie|{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}|\n|Winona|{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}|\n|Emma  |{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}|\n+------+----------------------------------------------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Address: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "strdata = [\n",
    "    ('Hania', '{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}'),\n",
    "    ('Sophie', '{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}'),\n",
    "    ('Winona', '{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}'),\n",
    "    ('Emma', '{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}')\n",
    "]\n",
    "strschema = ['Name', 'Address']\n",
    "strdf = spark.createDataFrame(strdata, strschema)\n",
    "strdf.show(truncate=False)   # This DataFrame is in String format\n",
    "strdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c6fdc5e-30d4-441e-8029-cbde4205876b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parse and Convert string JSON into a proper JSON Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e1f088-39ff-4dda-a29e-0ad115206c71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [],
   "source": [
    "strdf.createOrReplaceTempView(\"jsonparsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fdb0d3-2b48-44df-881f-0f05d08b30ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Address</th><th>Address_Parsed</th></tr></thead><tbody><tr><td>Hania</td><td>{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}</td><td>List(456 Elm St., New York)</td></tr><tr><td>Sophie</td><td>{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}</td><td>List(789 Maple Ave., London)</td></tr><tr><td>Winona</td><td>{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}</td><td>List(1010 Oak Blvd., Sydney)</td></tr><tr><td>Emma</td><td>{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}</td><td>List(2020 Pine Rd., Toronto)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hania",
         "{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}",
         [
          "456 Elm St.",
          "New York"
         ]
        ],
        [
         "Sophie",
         "{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}",
         [
          "789 Maple Ave.",
          "London"
         ]
        ],
        [
         "Winona",
         "{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}",
         [
          "1010 Oak Blvd.",
          "Sydney"
         ]
        ],
        [
         "Emma",
         "{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}",
         [
          "2020 Pine Rd.",
          "Toronto"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address_Parsed",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"Street\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *, from_json(Address, 'Street String, City string') AS Address_Parsed\n",
    "FROM jsonparsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85640c50-7510-4bfd-8e84-e4b93043ee37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Address</th><th>Address_Parsed</th></tr></thead><tbody><tr><td>Hania</td><td>{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}</td><td>List(456 Elm St., New York)</td></tr><tr><td>Sophie</td><td>{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}</td><td>List(789 Maple Ave., London)</td></tr><tr><td>Winona</td><td>{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}</td><td>List(1010 Oak Blvd., Sydney)</td></tr><tr><td>Emma</td><td>{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}</td><td>List(2020 Pine Rd., Toronto)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hania",
         "{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}",
         [
          "456 Elm St.",
          "New York"
         ]
        ],
        [
         "Sophie",
         "{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}",
         [
          "789 Maple Ave.",
          "London"
         ]
        ],
        [
         "Winona",
         "{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}",
         [
          "1010 Oak Blvd.",
          "Sydney"
         ]
        ],
        [
         "Emma",
         "{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}",
         [
          "2020 Pine Rd.",
          "Toronto"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address_Parsed",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"Street\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json,to_json\n",
    "\n",
    "res= strdf.withColumn(\"Address_Parsed\", from_json(col(\"Address\"),('Street String, City string')))\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7a0c5b-ce0a-428d-b80c-3fe87c44b592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After converting, separate the columns from the JSON body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7bdee1-c599-4a2a-b884-1c6b7a302e77",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Address</th><th>Address_Parsed</th><th>Street</th><th>City</th></tr></thead><tbody><tr><td>Hania</td><td>{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}</td><td>List(456 Elm St., New York)</td><td>456 Elm St.</td><td>New York</td></tr><tr><td>Sophie</td><td>{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}</td><td>List(789 Maple Ave., London)</td><td>789 Maple Ave.</td><td>London</td></tr><tr><td>Winona</td><td>{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}</td><td>List(1010 Oak Blvd., Sydney)</td><td>1010 Oak Blvd.</td><td>Sydney</td></tr><tr><td>Emma</td><td>{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}</td><td>List(2020 Pine Rd., Toronto)</td><td>2020 Pine Rd.</td><td>Toronto</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hania",
         "{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}",
         [
          "456 Elm St.",
          "New York"
         ],
         "456 Elm St.",
         "New York"
        ],
        [
         "Sophie",
         "{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}",
         [
          "789 Maple Ave.",
          "London"
         ],
         "789 Maple Ave.",
         "London"
        ],
        [
         "Winona",
         "{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}",
         [
          "1010 Oak Blvd.",
          "Sydney"
         ],
         "1010 Oak Blvd.",
         "Sydney"
        ],
        [
         "Emma",
         "{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}",
         [
          "2020 Pine Rd.",
          "Toronto"
         ],
         "2020 Pine Rd.",
         "Toronto"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address_Parsed",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"Street\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "Street",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "WITH CTE AS (\n",
    "    SELECT *, from_json(Address, 'Street String, City string') AS Address_Parsed\n",
    "FROM jsonparsing\n",
    ")\n",
    "\n",
    "SELECT *, Address_Parsed.Street, Address_Parsed.City\n",
    "FROM CTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "effa0b2a-6c32-4e18-ba6f-85f1d5f567da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "After converting, separate the columns from the JSON body"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Address</th><th>Address_Parsed</th><th>Street</th><th>City</th></tr></thead><tbody><tr><td>Hania</td><td>{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}</td><td>List(456 Elm St., New York)</td><td>456 Elm St.</td><td>New York</td></tr><tr><td>Sophie</td><td>{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}</td><td>List(789 Maple Ave., London)</td><td>789 Maple Ave.</td><td>London</td></tr><tr><td>Winona</td><td>{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}</td><td>List(1010 Oak Blvd., Sydney)</td><td>1010 Oak Blvd.</td><td>Sydney</td></tr><tr><td>Emma</td><td>{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}</td><td>List(2020 Pine Rd., Toronto)</td><td>2020 Pine Rd.</td><td>Toronto</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hania",
         "{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}",
         [
          "456 Elm St.",
          "New York"
         ],
         "456 Elm St.",
         "New York"
        ],
        [
         "Sophie",
         "{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}",
         [
          "789 Maple Ave.",
          "London"
         ],
         "789 Maple Ave.",
         "London"
        ],
        [
         "Winona",
         "{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}",
         [
          "1010 Oak Blvd.",
          "Sydney"
         ],
         "1010 Oak Blvd.",
         "Sydney"
        ],
        [
         "Emma",
         "{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}",
         [
          "2020 Pine Rd.",
          "Toronto"
         ],
         "2020 Pine Rd.",
         "Toronto"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address_Parsed",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"Street\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "Street",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = res.select(*res.columns,col(\"Address_Parsed\").Street.alias(\"Street\"), col(\"Address_Parsed\").City.alias(\"City\"))\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bd31988-6d99-421d-930f-ea03392d21c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How `to_json` works in SparkSQL and PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea98497-cc3f-4064-954f-d1ab0e4160e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Address</th><th>Address_Parsed</th><th>Address_Parsed_back_to_String_using_to_json</th></tr></thead><tbody><tr><td>Hania</td><td>{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}</td><td>List(456 Elm St., New York)</td><td>{\"Street\":\"456 Elm St.\",\"City\":\"New York\"}</td></tr><tr><td>Sophie</td><td>{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}</td><td>List(789 Maple Ave., London)</td><td>{\"Street\":\"789 Maple Ave.\",\"City\":\"London\"}</td></tr><tr><td>Winona</td><td>{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}</td><td>List(1010 Oak Blvd., Sydney)</td><td>{\"Street\":\"1010 Oak Blvd.\",\"City\":\"Sydney\"}</td></tr><tr><td>Emma</td><td>{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}</td><td>List(2020 Pine Rd., Toronto)</td><td>{\"Street\":\"2020 Pine Rd.\",\"City\":\"Toronto\"}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hania",
         "{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}",
         [
          "456 Elm St.",
          "New York"
         ],
         "{\"Street\":\"456 Elm St.\",\"City\":\"New York\"}"
        ],
        [
         "Sophie",
         "{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}",
         [
          "789 Maple Ave.",
          "London"
         ],
         "{\"Street\":\"789 Maple Ave.\",\"City\":\"London\"}"
        ],
        [
         "Winona",
         "{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}",
         [
          "1010 Oak Blvd.",
          "Sydney"
         ],
         "{\"Street\":\"1010 Oak Blvd.\",\"City\":\"Sydney\"}"
        ],
        [
         "Emma",
         "{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}",
         [
          "2020 Pine Rd.",
          "Toronto"
         ],
         "{\"Street\":\"2020 Pine Rd.\",\"City\":\"Toronto\"}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address_Parsed",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"Street\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "Address_Parsed_back_to_String_using_to_json",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "WITH CTE AS (\n",
    "    SELECT *, from_json(Address, 'Street String, City String') AS Address_Parsed\n",
    "FROM jsonparsing\n",
    ")\n",
    "\n",
    "SELECT *, to_json(Address_Parsed) AS Address_Parsed_back_to_String_using_to_json\n",
    "FROM CTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef0ec35-148d-4758-927c-ba77a2f6c62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Address</th><th>Address_Parsed</th><th>Address Parsed back to String Using to_json</th></tr></thead><tbody><tr><td>Hania</td><td>{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}</td><td>List(456 Elm St., New York)</td><td>{\"Street\":\"456 Elm St.\",\"City\":\"New York\"}</td></tr><tr><td>Sophie</td><td>{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}</td><td>List(789 Maple Ave., London)</td><td>{\"Street\":\"789 Maple Ave.\",\"City\":\"London\"}</td></tr><tr><td>Winona</td><td>{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}</td><td>List(1010 Oak Blvd., Sydney)</td><td>{\"Street\":\"1010 Oak Blvd.\",\"City\":\"Sydney\"}</td></tr><tr><td>Emma</td><td>{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}</td><td>List(2020 Pine Rd., Toronto)</td><td>{\"Street\":\"2020 Pine Rd.\",\"City\":\"Toronto\"}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hania",
         "{\"Street\": \"456 Elm St.\", \"City\": \"New York\"}",
         [
          "456 Elm St.",
          "New York"
         ],
         "{\"Street\":\"456 Elm St.\",\"City\":\"New York\"}"
        ],
        [
         "Sophie",
         "{\"Street\": \"789 Maple Ave.\", \"City\": \"London\"}",
         [
          "789 Maple Ave.",
          "London"
         ],
         "{\"Street\":\"789 Maple Ave.\",\"City\":\"London\"}"
        ],
        [
         "Winona",
         "{\"Street\": \"1010 Oak Blvd.\", \"City\": \"Sydney\"}",
         [
          "1010 Oak Blvd.",
          "Sydney"
         ],
         "{\"Street\":\"1010 Oak Blvd.\",\"City\":\"Sydney\"}"
        ],
        [
         "Emma",
         "{\"Street\": \"2020 Pine Rd.\", \"City\": \"Toronto\"}",
         [
          "2020 Pine Rd.",
          "Toronto"
         ],
         "{\"Street\":\"2020 Pine Rd.\",\"City\":\"Toronto\"}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Address_Parsed",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"Street\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "Address Parsed back to String Using to_json",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result2 = res.withColumn(\"Address Parsed back to String Using to_json\", to_json(col(\"Address_Parsed\")))\n",
    "display(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad08b76-9ad0-4ce6-856a-aea88df628a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The error occurs because to_json is a Spark SQL function, not a DataFrame method.\n",
    "# You should use it inside a withColumn or select statement.\n",
    "\n",
    "# result2 = res.to_json(col(\"Address_parsed\"))   # - AttributeError: 'DataFrame' object has no attribute 'to_json'. \n",
    "# display(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99fa1e54-d345-4905-9557-afee0af24712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NOTE\n",
    "- from_json(Address, ...) parses the JSON string into a struct.\n",
    "- to_json(Address_Parsed) converts the struct back into a JSON string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1524f105-75c9-494d-baa3-9c8d828a9a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interview Questions\n",
    "- Find out the cumulative sales or total running sales\n",
    "- Find previous sales\n",
    "- Find next sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8758d2-fcd9-4363-9d1b-2613cde27828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n|      Date|Sales|\n+----------+-----+\n|2024-01-01|25000|\n|2024-01-02| 3700|\n|2024-01-03|45600|\n|2024-01-04| 7800|\n|2024-01-05| 2000|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "sales_data = [\n",
    "    ('2024-01-01', 25000),\n",
    "    ('2024-01-02', 3700),\n",
    "    ('2024-01-03', 45600),\n",
    "    ('2024-01-04', 7800),\n",
    "    ('2024-01-05', 2000)\n",
    "]\n",
    "sales_schema = ['Date', 'Sales']\n",
    "\n",
    "salesdf = spark.createDataFrame(sales_data, sales_schema)\n",
    "salesdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb0753dd-43fa-4526-9d73-e753d3ea4032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Find out the cumulative sales or total running sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b93e5ba-e333-418e-9dc7-d68d0846939e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SparkSQL"
    }
   },
   "outputs": [],
   "source": [
    "salesdf.createOrReplaceTempView(\"SalesInfo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29370494-fd7c-47ae-9a7a-6a98f26968f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "INCORRECT - With Quotes"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Cumulative_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>84100</td></tr><tr><td>2024-01-02</td><td>3700</td><td>84100</td></tr><tr><td>2024-01-03</td><td>45600</td><td>84100</td></tr><tr><td>2024-01-04</td><td>7800</td><td>84100</td></tr><tr><td>2024-01-05</td><td>2000</td><td>84100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         84100
        ],
        [
         "2024-01-02",
         3700,
         84100
        ],
        [
         "2024-01-03",
         45600,
         84100
        ],
        [
         "2024-01-04",
         7800,
         84100
        ],
        [
         "2024-01-05",
         2000,
         84100
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Cumulative_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *, SUM(Sales) OVER(ORDER BY 'Date') Cumulative_Sales\n",
    "FROM SalesInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c0f236-e2c2-4f6d-9b6e-847e6fc7ef96",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CORRECT"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Cumulative_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>25000</td></tr><tr><td>2024-01-02</td><td>3700</td><td>28700</td></tr><tr><td>2024-01-03</td><td>45600</td><td>74300</td></tr><tr><td>2024-01-04</td><td>7800</td><td>82100</td></tr><tr><td>2024-01-05</td><td>2000</td><td>84100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         25000
        ],
        [
         "2024-01-02",
         3700,
         28700
        ],
        [
         "2024-01-03",
         45600,
         74300
        ],
        [
         "2024-01-04",
         7800,
         82100
        ],
        [
         "2024-01-05",
         2000,
         84100
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Cumulative_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *, SUM(Sales) OVER(ORDER BY Date) Cumulative_Sales\n",
    "FROM SalesInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135e2a27-93de-4712-8154-8658902ead38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Cumulative_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>25000</td></tr><tr><td>2024-01-02</td><td>3700</td><td>28700</td></tr><tr><td>2024-01-03</td><td>45600</td><td>74300</td></tr><tr><td>2024-01-04</td><td>7800</td><td>82100</td></tr><tr><td>2024-01-05</td><td>2000</td><td>84100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         25000
        ],
        [
         "2024-01-02",
         3700,
         28700
        ],
        [
         "2024-01-03",
         45600,
         74300
        ],
        [
         "2024-01-04",
         7800,
         82100
        ],
        [
         "2024-01-05",
         2000,
         84100
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Cumulative_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "w = Window.orderBy('Date')\n",
    "# display(salesdf.withColumn(\"Cumulative_Sales\", sum(col(\"Sales\")).over(w)))  # This works as wll\n",
    "display(salesdf.select(*salesdf.columns, sum(col(\"Sales\")).over(w).alias(\"Cumulative_Sales\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "994fb990-b870-41e3-91c7-696f1af7322b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### IMPORTANT (For Spark SQL)\n",
    "- ORDER BY Date (without quotes) refers to the column named Date.\n",
    "- ORDER BY 'Date' (with single quotes) is treated as a constant string (the literal value 'Date'), not a column.\n",
    "\n",
    "### What happens:\n",
    "SUM(Sales) OVER(ORDER BY Date)\n",
    "- This computes a running total (cumulative sum) of Sales ordered by the actual Date column, so you get a true cumulative sum.\n",
    "\n",
    "SUM(Sales) OVER(ORDER BY 'Date')\n",
    "- Here, 'Date' is a constant string for every row, so there is no real ordering. The window function treats all rows as having the same value for ordering, so it sums all Sales for every row, resulting in the same total for each row.\n",
    "\n",
    "Summary:\n",
    "- Use column names without quotes in ORDER BY for correct window function behavior.\n",
    "- Using quotes makes it a constant, not a column reference, which breaks the intended logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d963c3e2-4c62-4633-9ea0-774c386d4efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Find previous sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38578721-ab48-4723-8b75-eda70122a0ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SparkSQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Previous_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>null</td></tr><tr><td>2024-01-02</td><td>3700</td><td>25000</td></tr><tr><td>2024-01-03</td><td>45600</td><td>3700</td></tr><tr><td>2024-01-04</td><td>7800</td><td>45600</td></tr><tr><td>2024-01-05</td><td>2000</td><td>7800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         null
        ],
        [
         "2024-01-02",
         3700,
         25000
        ],
        [
         "2024-01-03",
         45600,
         3700
        ],
        [
         "2024-01-04",
         7800,
         45600
        ],
        [
         "2024-01-05",
         2000,
         7800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Previous_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *, LAG(Sales) OVER(ORDER BY Date) Previous_Sales -- Default OFFSET is 1\n",
    "FROM SalesInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d485f9-b930-4ab4-a1e9-120328bbc881",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Previous_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>null</td></tr><tr><td>2024-01-02</td><td>3700</td><td>25000</td></tr><tr><td>2024-01-03</td><td>45600</td><td>3700</td></tr><tr><td>2024-01-04</td><td>7800</td><td>45600</td></tr><tr><td>2024-01-05</td><td>2000</td><td>7800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         null
        ],
        [
         "2024-01-02",
         3700,
         25000
        ],
        [
         "2024-01-03",
         45600,
         3700
        ],
        [
         "2024-01-04",
         7800,
         45600
        ],
        [
         "2024-01-05",
         2000,
         7800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Previous_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "w = Window.orderBy('Date')\n",
    "display(salesdf.withColumn(\"Previous_Sales\", lag(col(\"Sales\")).over(w)))  \n",
    "# display(salesdf.select(*salesdf.columns, lag(col(\"Sales\")).over(w).alias(\"Previous_Sales\"))) # This works as wll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9837022d-f19a-439f-9a8e-970db1e8b833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Find Next Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6643f854-8047-45fa-a57e-8a5331f56c93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Next_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>3700</td></tr><tr><td>2024-01-02</td><td>3700</td><td>45600</td></tr><tr><td>2024-01-03</td><td>45600</td><td>7800</td></tr><tr><td>2024-01-04</td><td>7800</td><td>2000</td></tr><tr><td>2024-01-05</td><td>2000</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         3700
        ],
        [
         "2024-01-02",
         3700,
         45600
        ],
        [
         "2024-01-03",
         45600,
         7800
        ],
        [
         "2024-01-04",
         7800,
         2000
        ],
        [
         "2024-01-05",
         2000,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Next_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *, LEAD(Sales,1) OVER(ORDER BY Date) Next_Sales  -- Default OFFSET is 1, so specifiying 1 is optional\n",
    "FROM SalesInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82deceaf-fabe-4a72-8451-381cee8db7ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Next_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>3700</td></tr><tr><td>2024-01-02</td><td>3700</td><td>45600</td></tr><tr><td>2024-01-03</td><td>45600</td><td>7800</td></tr><tr><td>2024-01-04</td><td>7800</td><td>2000</td></tr><tr><td>2024-01-05</td><td>2000</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         3700
        ],
        [
         "2024-01-02",
         3700,
         45600
        ],
        [
         "2024-01-03",
         45600,
         7800
        ],
        [
         "2024-01-04",
         7800,
         2000
        ],
        [
         "2024-01-05",
         2000,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Next_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "w = Window.orderBy('Date')\n",
    "display(salesdf.select(*salesdf.columns, lead(col(\"Sales\"), 1).over(w).alias(\"Next_Sales\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76f4b79-59b4-4efe-aab8-9fc0b3395aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Sales</th><th>Next_Next_Sales</th></tr></thead><tbody><tr><td>2024-01-01</td><td>25000</td><td>45600</td></tr><tr><td>2024-01-02</td><td>3700</td><td>7800</td></tr><tr><td>2024-01-03</td><td>45600</td><td>2000</td></tr><tr><td>2024-01-04</td><td>7800</td><td>null</td></tr><tr><td>2024-01-05</td><td>2000</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-01",
         25000,
         45600
        ],
        [
         "2024-01-02",
         3700,
         7800
        ],
        [
         "2024-01-03",
         45600,
         2000
        ],
        [
         "2024-01-04",
         7800,
         null
        ],
        [
         "2024-01-05",
         2000,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Next_Next_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "w = Window.orderBy('Date')\n",
    "# display(salesdf.withColumn(\"Next_Next_Sales\", lead(col(\"Sales\"), 2).over(w)))  # This works as well -- Default OFFSET is 1\n",
    "display(salesdf.select(*salesdf.columns, lead(col(\"Sales\"), 2).over(w).alias(\"Next_Next_Sales\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e35e2b2d-fe77-46f3-ad45-44556a9f6a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interview Questions\n",
    "- Find Missing Numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46792d93-dd66-4ec3-9ae8-d62308deeec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| ID|\n+---+\n|  1|\n|  2|\n|  3|\n|  5|\n|  8|\n|  9|\n| 10|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "ids = [\n",
    "    (1,),(2,),(3,),(5,),(8,),(9,),(10,) \n",
    "    ]\n",
    "id_cols = ['ID']\n",
    "idsdf = spark.createDataFrame(ids,id_cols)\n",
    "idsdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c560a2e-ba44-4eff-981c-a2975ca8d8d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Solution using  range and subtract"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  4|\n|  6|\n|  7|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Find min and max ID\n",
    "min_id = idsdf.agg({\"ID\": \"min\"}).collect()[0][0]\n",
    "max_id = idsdf.agg({\"ID\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Create a DataFrame with all possible IDs in the range\n",
    "all_ids = spark.range(min_id, max_id + 1)\n",
    "\n",
    "# Find missing IDs using subtract\n",
    "# missing = all_ids.join(idsdf, \"ID\", \"left_anti\")  # Recommended as it is more efficient for larger datasets as well\n",
    "missing = all_ids.subtract(idsdf) \n",
    "missing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31cdf09a-1cec-4572-a090-919c9150abdf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Advanced Solution using sequence, explode and join"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| ID|\n+---+\n|  7|\n|  6|\n|  4|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min, max, sequence, explode\n",
    "\n",
    "# Find min and max\n",
    "min_max = idsdf.agg(min(\"ID\").alias(\"min_id\"), max(\"ID\").alias(\"max_id\")).collect()[0]\n",
    "min_id, max_id = min_max[\"min_id\"], min_max[\"max_id\"]\n",
    "\n",
    "# Create full range DataFrame\n",
    "full_range = spark.createDataFrame([(min_id, max_id)], [\"start\", \"end\"]) \\\n",
    "    .select(explode(sequence(col(\"start\"), col(\"end\"))).alias(\"ID\"))\n",
    "\n",
    "# Find missing numbers by anti-join\n",
    "missing = full_range.join(idsdf, \"ID\", \"left_anti\")  \n",
    "missing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d802d28-9ea3-46aa-a380-d10981a257b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NOTE\n",
    "Spark DataFrames are unordered by default unless you explicitly use .orderBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2144602-931f-4b42-a0e9-42286a99fa96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interview Question\n",
    "- Group Multiple rows into single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6844aaa-14d5-415b-a106-5c75a17068af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+\n|Customer_id|Customer_Name|       Purchase|\n+-----------+-------------+---------------+\n|        101|Alice Johnson|       Smart TV|\n|        102|  Carol White|         Laptop|\n|        103| Henry Wilson|Air Conditioner|\n|        104|  David Brown|Washing Machine|\n|        105| Frank Miller|     Smartphone|\n|        105| Frank Miller| Microwave Oven|\n|        104|  David Brown|Air Conditioner|\n|        101|Alice Johnson|   Refrigerator|\n|        104|  David Brown| Vacuum Cleaner|\n+-----------+-------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "pdtdata = [\n",
    "    (101, 'Alice Johnson', 'Smart TV'),\n",
    "    (102, 'Carol White', 'Laptop'),\n",
    "    (103, 'Henry Wilson', 'Air Conditioner'),\n",
    "    (104, 'David Brown', 'Washing Machine'),\n",
    "    (105, 'Frank Miller', 'Smartphone'),\n",
    "    (105, 'Frank Miller', 'Microwave Oven'),\n",
    "    (104, 'David Brown', 'Air Conditioner'),\n",
    "    (101, 'Alice Johnson', 'Refrigerator'),\n",
    "    (104, 'David Brown', 'Vacuum Cleaner')\n",
    "]\n",
    "\n",
    "pdtschema = ['Customer_id','Customer_Name','Purchase']\n",
    "\n",
    "pdtdf= spark.createDataFrame(pdtdata, pdtschema)\n",
    "pdtdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4321d020-04e4-4491-8ed8-26b56a62a281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdtdf.createOrReplaceTempView(\"PurchasedPdts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992432a4-7e5c-4eb4-819c-53be8e394f0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Customer_id</th><th>Customer_name</th><th>Purchases</th></tr></thead><tbody><tr><td>101</td><td>Alice Johnson</td><td>List(Refrigerator, Smart TV)</td></tr><tr><td>102</td><td>Carol White</td><td>List(Laptop)</td></tr><tr><td>103</td><td>Henry Wilson</td><td>List(Air Conditioner)</td></tr><tr><td>104</td><td>David Brown</td><td>List(Vacuum Cleaner, Washing Machine, Air Conditioner)</td></tr><tr><td>105</td><td>Frank Miller</td><td>List(Smartphone, Microwave Oven)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "Alice Johnson",
         [
          "Refrigerator",
          "Smart TV"
         ]
        ],
        [
         102,
         "Carol White",
         [
          "Laptop"
         ]
        ],
        [
         103,
         "Henry Wilson",
         [
          "Air Conditioner"
         ]
        ],
        [
         104,
         "David Brown",
         [
          "Vacuum Cleaner",
          "Washing Machine",
          "Air Conditioner"
         ]
        ],
        [
         105,
         "Frank Miller",
         [
          "Smartphone",
          "Microwave Oven"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Purchases",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT Customer_id, Customer_name, \n",
    "collect_set(Purchase) Purchases   -- Will convert Purchase into Array\n",
    "FROM PurchasedPdts\n",
    "GROUP BY 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac973d9-9439-4c65-a068-8c996a850544",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Customer_id</th><th>Customer_name</th><th>collect_set(Purchase)</th></tr></thead><tbody><tr><td>101</td><td>Alice Johnson</td><td>List(Refrigerator, Smart TV)</td></tr><tr><td>102</td><td>Carol White</td><td>List(Laptop)</td></tr><tr><td>103</td><td>Henry Wilson</td><td>List(Air Conditioner)</td></tr><tr><td>104</td><td>David Brown</td><td>List(Vacuum Cleaner, Washing Machine, Air Conditioner)</td></tr><tr><td>105</td><td>Frank Miller</td><td>List(Smartphone, Microwave Oven)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "Alice Johnson",
         [
          "Refrigerator",
          "Smart TV"
         ]
        ],
        [
         102,
         "Carol White",
         [
          "Laptop"
         ]
        ],
        [
         103,
         "Henry Wilson",
         [
          "Air Conditioner"
         ]
        ],
        [
         104,
         "David Brown",
         [
          "Vacuum Cleaner",
          "Washing Machine",
          "Air Conditioner"
         ]
        ],
        [
         105,
         "Frank Miller",
         [
          "Smartphone",
          "Microwave Oven"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "collect_set(Purchase)",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "display(pdtdf.groupBy('Customer_id','Customer_name').agg(collect_set(\"Purchase\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a4ce2da-150f-4ffc-8a35-2487c32fbb24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interview Question\n",
    "- Combine many lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3717dc45-2ae2-49ac-9d11-6cbbf8248788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n|Column1|Column2|\n+-------+-------+\n|      a|      1|\n|      b|      2|\n|      c|      3|\n|      d|      4|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"a\",\"b\",\"c\",\"d\"]\n",
    "list2 = [1,2,3,4]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(list(zip(list1,list2)))\n",
    "\n",
    "listdf = rdd.toDF(['Column1','Column2'])\n",
    "listdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f41bcf-1df6-44a1-90ca-304a9bbb6b9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column1</th><th>Column2</th></tr></thead><tbody><tr><td>a</td><td>1</td></tr><tr><td>b</td><td>2</td></tr><tr><td>c</td><td>3</td></tr><tr><td>d</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "a",
         1
        ],
        [
         "b",
         2
        ],
        [
         "c",
         3
        ],
        [
         "d",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Column2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "zippedlist = list(zip(list1,list2))\n",
    "listdf = spark.createDataFrame(zippedlist, [\"Column1\",\"Column2\"])\n",
    "display(listdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216c4022-89e6-4661-b070-72c97aa5ce1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [],
   "source": [
    "listdf.createOrReplaceTempView(\"Combined_Lists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12773e04-ec44-4089-97ad-f16c97a6370a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column1</th><th>Column2</th></tr></thead><tbody><tr><td>a</td><td>1</td></tr><tr><td>b</td><td>2</td></tr><tr><td>c</td><td>3</td></tr><tr><td>d</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "a",
         1
        ],
        [
         "b",
         2
        ],
        [
         "c",
         3
        ],
        [
         "d",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Column2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM Combined_Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1b5dca-de44-4a06-b7b8-388265f4a05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Streaming Data in PySpark\n",
    "- Lets set up a basic streaming job that reads data from a socket in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b21cc9-6065-40c2-b938-8a3d4e168fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# Read streaming data from a socket\n",
    "lines = spark.readStream\\\n",
    "    .format(\"socket\")\\\n",
    "    .option(\"host\", \"localhost\")\\\n",
    "    .option(\"port\", 9999)\\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "    explode(split(lines.value, \" \")).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordcount = words.groupBy(\"word\").count()\n",
    "\n",
    "\n",
    "# wordcount.show()  -- Unexpected exception formatting exception. Falling back to standard exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b8b0a36-e1f8-4516-9e24-63c26185af44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANT**: \n",
    "- *'wordcount.show()'* will throw an error since you cannot call show() on a streaming Dataframe, wordcount here!\n",
    "- In PySpark, DataFrame.show() is meant only for static Dataframes, not streaming ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19eeab28-6cbb-47c5-91b8-82a03990c3d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Running the query that prints the running counts to the console"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2501329755666365>:6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m wordcount\u001B[38;5;241m.\u001B[39mwriteStream\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      3\u001B[0m         \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      4\u001B[0m             \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[0;32m----> 6\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:198\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n",
       "\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = c947387d-f9ac-4d4c-83bc-28cc461ee1a4, runId = 8e4990da-8a8c-42de-8b74-f64d062984af] terminated with exception: Connection refused (Connection refused)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)\nFile \u001B[0;32m<command-2501329755666365>:6\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m wordcount\u001B[38;5;241m.\u001B[39mwriteStream\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      4\u001B[0m             \u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m----> 6\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:198\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = c947387d-f9ac-4d4c-83bc-28cc461ee1a4, runId = 8e4990da-8a8c-42de-8b74-f64d062984af] terminated with exception: Connection refused (Connection refused)",
       "errorSummary": "<span class='ansi-red-fg'>StreamingQueryException</span>: [STREAM_FAILED] Query [id = c947387d-f9ac-4d4c-83bc-28cc461ee1a4, runId = 8e4990da-8a8c-42de-8b74-f64d062984af] terminated with exception: Connection refused (Connection refused)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = wordcount.writeStream\\\n",
    "    .outputMode(\"complete\")\\\n",
    "        .format(\"console\")\\\n",
    "            .start()\n",
    "\n",
    "query.awaitTermination()  # Keeps the streaming job running!\n",
    "\n",
    "query.stop() # To stop the streaming job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcdb1f9b-0528-438c-b643-22bb3b8f717e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "StreamingQueryException -- The \"Connection refused\" error means Spark can't connect to the socket server given above. It might not be up and running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2af33dd-dad8-4801-a871-7828eb76411b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Project Scenario: Predicting Customer Churn\n",
    "\n",
    "The goal of this project is to:\n",
    "\n",
    "- Load the data into PySpark.\n",
    "- Perform data cleaning and preprocessing.\n",
    "- Build a machine learning model to predict customer churn.\n",
    "- Evaluate the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcad7c22-7107-4d49-b657-21a3314d291c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2110792787414632,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "basic_to_intermediate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55955fec-d106-468b-b1fb-e9d886b2c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=501407609248767#setting/sparkui/0710-162142-9bd147mb/driver-2256041660967793516\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=501407609248767#setting/sparkui/0710-162142-9bd147mb/driver-2256041660967793516\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Optimizing Joins\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.executor.cores\",4)\n",
    "        .config(\"spark.cores.max\",16)\n",
    "        .config(\"spark.executor.memory\", \"512M\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2369cc9d-47cd-4918-8218-74adea67c5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable Adaptive Query Engine(AQE) and Broadcast Join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3ecdac-5721-47a0-b53a-d85f6d1232d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10 million records\n",
    "emp_schema = \"first_name string, last_name string, job_title string, dob date, email string, phone string, salary double, department string, department_id integer\"\n",
    "emp = spark.read.schema(emp_schema).option(\"header\",True).csv(\"/data/input/datasets/employee_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf48e4b6-3ead-4c68-b982-8a6bf6d13b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV file with 10 records\n",
    "dept_schema =\"department_id int, department_name string, description string, city string, state string, country string \"\n",
    "dept = spark.read.schema(dept_schema).option(\"header\",True).csv(\"/data/input/datasets/department_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d7a848-1093-445a-96d9-3a229e59e7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JOINING datasets\n",
    "dfjoined = emp.join(dept, on='department_id', how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d94c2627-ae73-42ce-a608-e4714b9e6372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- '*noop\"* will simulate and read the whole employee and department dataframe.\n",
    "- Simulating the whole dataframe allows us to benchmark the timings and understand the join better\n",
    "- The *\"noop\"* format is a special format that performs no actual I/O operation; it's essentially a no-operation (no-op).\n",
    "- The *\"noop\"* doesn't write anywhere, no data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e00aac9-4c47-42ff-89f7-4a84a2348039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(2) Project [department_id#97, first_name#89, last_name#90, job_title#91, dob#92, email#93, phone#94, salary#95, department#96, department_name#108, description#109, city#110, state#111, country#112]\n+- *(2) BroadcastHashJoin [department_id#97], [department_id#107], LeftOuter, BuildRight, false, false\n   :- FileScan csv [first_name#89,last_name#90,job_title#91,dob#92,email#93,phone#94,salary#95,department#96,department_id#97] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=197]\n      +- *(1) Filter isnotnull(department_id#107)\n         +- FileScan csv [department_id#107,department_name#108,description#109,city#110,state#111,country#112] Batched: false, DataFilters: [isnotnull(department_id#107)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n\n\n"
     ]
    }
   ],
   "source": [
    "dfjoined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "dfjoined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c2ac55-4aac-4adb-b472-10f5dc030166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The reason dfjoined.explain() shows \"BroadCastHashJoin\" above even though we didn't explicitly use a broadcast join is due to Spark's automatic optimization.\n",
    "- Post spark 2.0 by default data less than 10MB is broadcasted and join operation will be sort merge\n",
    "- We can disable automatic broadcast joins: spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1).\n",
    "- Let's see how the explain looks without broadcast below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f658cbe-1395-4ecc-a154-59578843327b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(2) Project [department_id#97, first_name#89, last_name#90, job_title#91, dob#92, email#93, phone#94, salary#95, department#96, department_name#108, description#109, city#110, state#111, country#112]\n+- *(2) SortMergeJoin [department_id#97], [department_id#107], LeftOuter\n   :- Sort [department_id#97 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(department_id#97, 200), ENSURE_REQUIREMENTS, [plan_id=559]\n   :     +- FileScan csv [first_name#89,last_name#90,job_title#91,dob#92,email#93,phone#94,salary#95,department#96,department_id#97] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n   +- Sort [department_id#107 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(department_id#107, 200), ENSURE_REQUIREMENTS, [plan_id=597]\n         +- *(1) Filter isnotnull(department_id#107)\n            +- FileScan csv [department_id#107,department_name#108,description#109,city#110,state#111,country#112] Batched: false, DataFilters: [isnotnull(department_id#107)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n\n\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "dfjoined = emp.join(dept, on='department_id', how=\"left_outer\")\n",
    "dfjoined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "dfjoined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6406e0ca-ebf5-4c8c-9f06-f5917181a431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### USING BROADCAST JOIN (explicitly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "168fe78c-e1c0-4824-85b8-897682678014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- When the data is brodcasted, it is present in all of the excutors which in turn eliminates shuffling leading to performance optimiziation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba0cf02-8660-4787-a260-9d84bf8b5622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(2) Project [department_id#97, first_name#89, last_name#90, job_title#91, dob#92, email#93, phone#94, salary#95, department#96, department_name#108, description#109, city#110, state#111, country#112]\n+- *(2) BroadcastHashJoin [department_id#97], [department_id#107], LeftOuter, BuildRight, false, false\n   :- FileScan csv [first_name#89,last_name#90,job_title#91,dob#92,email#93,phone#94,salary#95,department#96,department_id#97] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/employee_recs.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:date,email:string,phone:string,sal...\n   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=447]\n      +- *(1) Filter isnotnull(department_id#107)\n         +- FileScan csv [department_id#107,department_name#108,description#109,city#110,state#111,country#112] Batched: false, DataFilters: [isnotnull(department_id#107)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/department_recs.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n\n\n"
     ]
    }
   ],
   "source": [
    "# JOINING datasets using broadcast join\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "dfjoined_brodcasted= emp.join(broadcast(dept), on='department_id', how=\"left_outer\")\n",
    "dfjoined_brodcasted.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "dfjoined_brodcasted.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b9d9986-d987-4109-b272-2dffd5d0dccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- What if you do broadcast on a larger dataframe? Let's check below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5ac38e-3d78-43ef-a2b2-3500737be6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Sales CSV file with 1 million records\n",
    "sschema =\"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "sales = spark.read.schema(sschema).option(\"header\",True).csv(\"/data/input/datasets/sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef33e6b7-4f62-4436-a131-cd28a041afd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15905529-ac07-4e8d-95e0-336734790f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Cities CSV file with 5 million records\n",
    "cschema =\"city_id string, city string, state string, state_abv string, country string\"\n",
    "cities = spark.read.schema(cschema).option(\"header\",True).csv(\"/data/input/datasets/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1046ca9-4d61-46e4-afd4-e8da8f1d3590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cities.createOrReplaceTempView(\"Cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a587cd2-7a93-430f-8e6f-a55ef55d2c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# JOIN data\n",
    "dfjoinedsales = sales.join(broadcast(cities), on='city_id', how=\"left_outer\")\n",
    "dfjoinedsales.write.format(\"noop\").mode(\"overwrite\").save()  # Action to check whats happening in the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e0218c-7100-49b0-bf45-19ec12ddb320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The above action will fail for most of the tasks! Since you are broadcasting the join to a larger dataframe which is not allowed! (check Spark UI for more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6ecbbd2-f890-488b-8cef-6cd217007e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Broadcast using SQL Hints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33afb540-d178-4116-aafc-963c7e8bae4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 'false'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.get(\"spark.sql.adaptive.enabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82044256-878f-48fc-8f3a-43cefeaa9cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: '-1'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.get(\"spark.sql.adaptive.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d473ae-4635-4f90-ac6a-bbf0ed7c75a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+-----------+------+-------+-------+-------+--------+---------+-------+\n|transacted_at|       trx_id|retailer_id|description|amount|city_id|city_id|   city|   state|state_abv|country|\n+-------------+-------------+-----------+-----------+------+-------+-------+-------+--------+---------+-------+\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n|   2020-03-19|TRX0001249281|      R0021|  Furniture|3681.2|   C003|   C003|Chicago|Illinois|       IL|    USA|\n+-------------+-------------+-----------+-----------+------+-------+-------+-------+--------+---------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dfsqlopt= spark.sql(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM Sales s\n",
    "                    JOIN Cities c ON s.city_id=c.city_id\n",
    "                    \"\"\")\n",
    "dfsqlopt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec9c8e6-f9c0-4ae8-9abc-8fe87903300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(3) SortMergeJoin [city_id#7], [city_id#14], Inner\n:- Sort [city_id#7 ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(city_id#7, 200), ENSURE_REQUIREMENTS, [plan_id=458]\n:     +- *(1) Filter isnotnull(city_id#7)\n:        +- FileScan csv [transacted_at#2,trx_id#3,retailer_id#4,description#5,amount#6,city_id#7] Batched: false, DataFilters: [isnotnull(city_id#7)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/sales.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit...\n+- Sort [city_id#14 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(city_id#14, 200), ENSURE_REQUIREMENTS, [plan_id=464]\n      +- *(2) Filter isnotnull(city_id#14)\n         +- FileScan csv [city_id#14,city#15,state#16,state_abv#17,country#18] Batched: false, DataFilters: [isnotnull(city_id#14)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/cities.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>\n\n\n"
     ]
    }
   ],
   "source": [
    "dfsqlopt.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf2c780-8c2f-474f-b9ad-53901b386cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+-----------+-------+-------+-------+-----------+----------+---------+-------+\n|transacted_at|       trx_id|retailer_id|description| amount|city_id|city_id|       city|     state|state_abv|country|\n+-------------+-------------+-----------+-----------+-------+-------+-------+-----------+----------+---------+-------+\n|   2022-11-02|TRX0000624636|      R0639|  Groceries| 132.57|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-11-12|TRX0000624632|      R0569|   Clothing| 202.14|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-12-10|TRX0000624631|      R0629|  Furniture| 4429.3|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-06-07|TRX0000624628|      R0888|   Clothing|3699.46|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2021-06-10|TRX0000624617|      R0660|      Books|2326.66|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-05-27|TRX0000624611|      R0001|Electronics|2249.37|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-06-09|TRX0000624601|      R0344|  Furniture|3311.41|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-01-23|TRX0000624597|      R0978|   Clothing|  56.77|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2020-03-14|TRX0000624593|      R0038|Electronics|4482.91|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-05-07|TRX0000624587|      R0765|  Groceries|1412.78|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-07-08|TRX0000624581|      R0569|  Groceries| 516.19|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-12-13|TRX0000624575|      R0129|   Clothing|3782.09|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-01-12|TRX0000624566|      R0499|  Groceries|2227.39|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-05-27|TRX0000624563|      R0728|Electronics|1565.17|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-08-21|TRX0000624562|      R0802|      Books| 184.89|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-11-04|TRX0000624550|      R0964|Electronics| 818.97|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-01-31|TRX0000624546|      R0029|   Clothing|1735.95|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2022-10-16|TRX0000624544|      R0697|Electronics|3600.63|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2021-02-20|TRX0000624537|      R0392|   Clothing|2442.01|   C002|   C002|Los Angeles|California|       CA|    USA|\n|   2023-08-21|TRX0000624535|      R0611|  Furniture|4813.53|   C002|   C002|Los Angeles|California|       CA|    USA|\n+-------------+-------------+-----------+-----------+-------+-------+-------+-----------+----------+---------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dfsqloptbroadcast= spark.sql(\"\"\"\n",
    "                    SELECT /*+ broadcast(s)*/ *\n",
    "                    FROM Sales s\n",
    "                    JOIN Cities c ON s.city_id=c.city_id\n",
    "                    \"\"\")\n",
    "dfsqloptbroadcast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1d3194-6930-45cc-bc83-473cf4f62ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(2) BroadcastHashJoin [city_id#7], [city_id#14], Inner, BuildLeft, false, false\n:- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[5, string, false]),false), [plan_id=971]\n:  +- *(1) Filter isnotnull(city_id#7)\n:     +- FileScan csv [transacted_at#2,trx_id#3,retailer_id#4,description#5,amount#6,city_id#7] Batched: false, DataFilters: [isnotnull(city_id#7)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/sales.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit...\n+- *(2) Filter isnotnull(city_id#14)\n   +- FileScan csv [city_id#14,city#15,state#16,state_abv#17,country#18] Batched: false, DataFilters: [isnotnull(city_id#14)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/cities.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>\n\n\n"
     ]
    }
   ],
   "source": [
    "dfsqloptbroadcast.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a6d8fb4-ba44-48b1-a077-88c2ba7657ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### BUCKETING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76a83e89-aa7a-456c-a09c-857a439d7170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> IMPORTANT\n",
    "- If you want to write the DataFrame to a specific path (not as a table), you cannot use bucketBy directly with .save(path)—bucketBy is only supported with saveAsTable (for Hive tables).\n",
    "- Spark’s `.bucketBy()` method **does not support bucketing for CSV files** when writing data. If you use `.bucketBy()` with `.format(\"csv\")`, Spark will ignore the bucketing and simply write partitioned CSV files. This is documented behavior in Spark: bucketing is only supported for certain formats (like Parquet, ORC, and Hive tables), not CSV.\n",
    "> \n",
    "- **Why you might see performance improvement:**  \n",
    "- When you use `.bucketBy()` with CSV, Spark still writes out multiple files (one per task/partition), which can look similar to bucketing, but these are not true buckets. Any performance gain is likely due to:\n",
    "- - Reduced file size per partition (easier parallelism)\n",
    "- - Partitioning by key, not true bucketing\n",
    "> \n",
    "- **True bucketing** means Spark writes special metadata and organizes files so that Spark can skip shuffling during joins. This only works for supported formats.\n",
    "> \n",
    "- **Summary:**  \n",
    "- - `.bucketBy()` is ignored for CSV writes; no bucket metadata is written.\n",
    "- - Any observed performance gain is due to partitioning, not bucketing.\n",
    "- For true bucketing and shuffle reduction in joins, use Parquet, ORC, or Hive tables.\n",
    "> \n",
    "**Reference:**  \n",
    "[Apache Spark Documentation – Bucketing](https://spark.apache.org/docs/latest/sql-data-sources-bucketization.html)  \n",
    "> \"Currently, bucketed tables are only supported for Hive-compatible file formats such as Parquet and ORC.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8c1ecb-d038-4a46-ac9c-249a61d99b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Sales data in Buckets\n",
    "sales.write.format(\"csv\").bucketBy(4, \"city_id\").option(\"header\",True).option(\"path\", \"/data/input/datasets/bucketed_sales.csv\").mode(\"overwrite\").saveAsTable(\"salesBucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01fa719-0087-4948-92d1-a22631dbb38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Cities data in Buckets\n",
    "cities.write.format(\"csv\").bucketBy(4, \"city_id\").option(\"header\",True).option(\"path\", \"/data/input/datasets/bucketed_cities.csv\").mode(\"overwrite\").saveAsTable(\"citiesBucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2745540f-b75a-42b6-999d-1c12c228bf87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ sales table\n",
    "salesBucket = spark.read.table(\"salesBucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4404f8d4-6787-4675-bdd2-3d64b308f3be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ cities table\n",
    "citiesBucket = spark.read.table(\"citiesBucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2c61c3-35ed-4e74-b664-36812eddd5a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JOIN the above BUCKETED Tables \n",
    "joined = salesBucket.join(citiesBucket, on=salesBucket.city_id==citiesBucket.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceb0bf9-c875-4777-97bf-05681a9efdae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [city_id#854], [city_id#861], LeftOuter\n   :- Sort [city_id#854 ASC NULLS FIRST], false, 0\n   :  +- FileScan csv spark_catalog.default.salesbucket[transacted_at#849,trx_id#850,retailer_id#851,description#852,amount#853,city_id#854] Batched: false, Bucketed: true, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/bucketed_sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit..., SelectedBucketsCount: 4 out of 4\n   +- Sort [city_id#861 ASC NULLS FIRST], false, 0\n      +- Filter isnotnull(city_id#861)\n         +- FileScan csv spark_catalog.default.citiesbucket[city_id#861,city#862,state#863,state_abv#864,country#865] Batched: false, Bucketed: true, DataFilters: [isnotnull(city_id#861)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/data/input/datasets/bucketed_cities.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>, SelectedBucketsCount: 4 out of 4\n\n\n"
     ]
    }
   ],
   "source": [
    "joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8f9d86-1223-478c-a89b-53a93883b09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the above explain plan:\n",
    "- FileScan on both tables shows Bucketed: true, with SelectedBucketsCount: 4 out of 4, meaning all buckets are being utilized.\n",
    "- The plan shows Sort operations followed by a SortMergeJoin, which is expected for bucketed tables.\n",
    "\n",
    "> Key points:\n",
    "- Spark is leveraging bucketing for the join, which explains why the join itself is very fast (<1s).\n",
    "- The join is not shuffling data across the cluster, just reading from bucketed files and merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f869b272-6a90-4544-a002-1ea9d4e8af9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Points To Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6f76ea7-aea8-4ed8-9cc0-49f31706ce71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Joining columns different from Bucket column, same Bucket size - shuffle on both tabless\n",
    "- Joining columns same, One table in bucket - Shuffle on non Bucketed table\n",
    "- Joining columns same, Different bucket size - Shuffle on smaller Bucketed table\n",
    "- Joining columns same, Same bucket size - No Shuffle (Faster Join)\n",
    "> \n",
    "- So, its very important to choose Bucket column and Bucket size.\n",
    "- Decide effectively on number of Buckets, as too many Buckets with not enough data can lead to small size issue\n",
    "- Prefer Shuffle Hash Join if datasets are small."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "18_optimizing_joins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}